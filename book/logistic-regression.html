<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>C Logistic Regression | An R Companion for Introduction to Data Mining</title>
<meta name="author" content="Michael Hahsler">
<meta name="description" content="This chapter introduces the popular classification method logistic regression more in detail. Logistic regression is introduced as an alternative classification method in Chapter 4 of Introduction...">
<meta name="generator" content="bookdown 0.42 with bs4_book()">
<meta property="og:title" content="C Logistic Regression | An R Companion for Introduction to Data Mining">
<meta property="og:type" content="book">
<meta property="og:image" content="/images/cover.png">
<meta property="og:description" content="This chapter introduces the popular classification method logistic regression more in detail. Logistic regression is introduced as an alternative classification method in Chapter 4 of Introduction...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="C Logistic Regression | An R Companion for Introduction to Data Mining">
<meta name="twitter:description" content="This chapter introduces the popular classification method logistic regression more in detail. Logistic regression is introduced as an alternative classification method in Chapter 4 of Introduction...">
<meta name="twitter:image" content="/images/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap/bootstrap.bundle.min.js"></script><script src="libs/bs3compat/transition.js"></script><script src="libs/bs3compat/tabs.js"></script><script src="libs/bs3compat/bs3compat.js"></script><link href="libs/bs4_book/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book/bs4_book.js"></script><link href="libs/htmltools-fill/fill.css" rel="stylesheet">
<script src="libs/htmlwidgets/htmlwidgets.js"></script><script src="libs/plotly-binding/plotly.js"></script><script src="libs/typedarray/typedarray.min.js"></script><link href="libs/crosstalk/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main/plotly-latest.min.js"></script><link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet">
<script src="libs/datatables-binding/datatables.js"></script><link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script><link href="libs/nouislider/jquery.nouislider.min.css" rel="stylesheet">
<script src="libs/nouislider/jquery.nouislider.min.js"></script><link href="libs/selectize/selectize.bootstrap3.css" rel="stylesheet">
<script src="libs/selectize/selectize.min.js"></script><link href="libs/vis/vis-network.min.css" rel="stylesheet">
<script src="libs/vis/vis-network.min.js"></script><script src="libs/visNetwork-binding/visNetwork.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">An R Companion for Introduction to Data Mining</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="data.html"><span class="header-section-number">2</span> Data</a></li>
<li><a class="" href="classification-basic-concepts.html"><span class="header-section-number">3</span> Classification: Basic Concepts</a></li>
<li><a class="" href="classification-alternative-techniques.html"><span class="header-section-number">4</span> Classification: Alternative Techniques</a></li>
<li><a class="" href="association-analysis-basic-concepts.html"><span class="header-section-number">5</span> Association Analysis: Basic Concepts</a></li>
<li><a class="" href="association-analysis-advanced-concepts.html"><span class="header-section-number">6</span> Association Analysis: Advanced Concepts</a></li>
<li><a class="" href="cluster-analysis.html"><span class="header-section-number">7</span> Cluster Analysis</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="data-exploration-and-visualization.html"><span class="header-section-number">A</span> Data Exploration and Visualization</a></li>
<li><a class="" href="regression.html"><span class="header-section-number">B</span> Regression</a></li>
<li><a class="active" href="logistic-regression.html"><span class="header-section-number">C</span> Logistic Regression</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mhahsler/Introduction_to_Data_Mining_R_Examples">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="logistic-regression" class="section level1" number="10">
<h1>
<span class="header-section-number">C</span> Logistic Regression<a class="anchor" aria-label="anchor" href="#logistic-regression"><i class="fas fa-link"></i></a>
</h1>
<p>This chapter introduces the popular classification method
logistic regression more in detail. Logistic regression is introduced
as an alternative classification method in Chapter 4 of Introduction to Data Mining.</p>
<div class="sourceCode" id="cb485"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pkgs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"glmnet"</span>, <span class="st">"caret"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pkgs_install</span> <span class="op">&lt;-</span> <span class="va">pkgs</span><span class="op">[</span><span class="op">!</span><span class="op">(</span><span class="va">pkgs</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/utils/installed.packages.html">installed.packages</a></span><span class="op">(</span><span class="op">)</span><span class="op">[</span>,<span class="st">"Package"</span><span class="op">]</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">pkgs_install</span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="va">pkgs_install</span><span class="op">)</span></span></code></pre></div>
<p>The packages used for this chapter are:</p>
<ul>
<li>
<em>caret</em> <span class="citation">(<a href="references.html#ref-R-caret">Kuhn 2024</a>)</span>
</li>
<li>
<em>glmnet</em> <span class="citation">(<a href="references.html#ref-R-glmnet">Friedman et al. 2023</a>)</span>
</li>
</ul>
<div id="introduction-2" class="section level2" number="10.1">
<h2>
<span class="header-section-number">C.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-2"><i class="fas fa-link"></i></a>
</h2>
<p>Logistic regression contains the word regression, but it is actually a
statistical classification
model to predict the probability <span class="math inline">\(p\)</span> of a binary outcome given a set of features.
It is a very powerful classification model that can be fit very quickly. It is one of the
first classification models you should try on new data.</p>
<p>Logistic regression is a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear model</a>
with the logit as the link function and a binomial error distribution.
It can
be thought of as a linear regression with the
log odds ratio (logit)
of the binary outcome as the dependent variable:</p>
<p><span class="math display">\[logit(p) = ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...\]</span></p>
<p>The logit function links the probability p to the linear regression by converting
a number in the probability range <span class="math inline">\([0,1]\)</span> to the range <span class="math inline">\([-\infty,+\infty]\)</span>.</p>
<div class="sourceCode" id="cb486"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">logit</span>  <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">p</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu">logit</span><span class="op">(</span><span class="va">p</span><span class="op">)</span>, <span class="va">p</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0.5</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v <span class="op">=</span> <span class="fl">0</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="R-Companion-Data-Mining_files/figure-html/unnamed-chunk-433-1.png" width="672"></div>
<p>The figure above shows actually the inverse of the logit function.
The inverse of the logit function is called <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic (or sigmoid) function</a> (<span class="math inline">\(\sigma(\cdot)\)</span>) which is
often used in ML, and especially for artificial neural networks,
to squash the set of real numbers to the <span class="math inline">\([0,1]\)</span> interval.
Using the inverse function, we see that the probability of the outcome <span class="math inline">\(p\)</span>
is modeled by the logistic function of the linear regression:</p>
<p><span class="math display">\[ p = \frac{e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...}}{1 +  e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...}} = \frac{1}{1+e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...)}} = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...)\]</span></p>
<p>After the <span class="math inline">\(\boldsymbol{\beta} = (\beta_0, \beta_1,...)\)</span> parameter vector is fitted using training data by
minimizing the log loss (i.e., cross-entropy loss),
the equation above can be used to predict
the probability <span class="math inline">\(p\)</span> given a new data point <span class="math inline">\(\mathbf{x} = (x_1, x_2, ...)\)</span>. If the predicted <span class="math inline">\(p &gt; .5\)</span>
then we predict that the event happens, otherwise we predict that it does not happen.</p>
<p>The outcome itself is binary and therefore has a Bernoulli distribution. Since we have
multiple examples in our data we draw several times from this distribution resulting
in a Binomial distribution for the number of successful events drawn. Logistic
regression therefore uses a logit link function to link the probability of the event
to the linear regression and the distribution family is Binomial.</p>
</div>
<div id="data-preparation-1" class="section level2" number="10.2">
<h2>
<span class="header-section-number">C.2</span> Data Preparation<a class="anchor" aria-label="anchor" href="#data-preparation-1"><i class="fas fa-link"></i></a>
</h2>
<p>We load and shuffle the data. We also add a useless variable to see if the logistic regression removes it.</p>
<div class="sourceCode" id="cb487"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span> <span class="co"># for reproducability</span></span>
<span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/pkg/arules/man/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">x</span>, useless <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>We create a binary classification problem by
asking if a flower is of species Virginica or not.
We create new logical variable called <code>virginica</code> and remove the
<code>Species</code> column.</p>
<div class="sourceCode" id="cb488"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span><span class="op">$</span><span class="va">virginica</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"virginica"</span></span>
<span><span class="va">x</span><span class="op">$</span><span class="va">Species</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span></code></pre></div>
<p>We can visualize the data using a scatter plot matrix and use the color red for
<code>virginica == TRUE</code> and black for the other flowers.</p>
<div class="sourceCode" id="cb489"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>, col<span class="op">=</span><span class="va">x</span><span class="op">$</span><span class="va">virginica</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="R-Companion-Data-Mining_files/figure-html/unnamed-chunk-436-1.png" width="672"></div>
</div>
<div id="a-first-logistic-regression-model" class="section level2" number="10.3">
<h2>
<span class="header-section-number">C.3</span> A first Logistic Regression Model<a class="anchor" aria-label="anchor" href="#a-first-logistic-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>Logistic regression is a generalized linear model (GLM) with logit as the
link function and a binomial distribution. The <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function is provided by
the R core package <code>stats</code> which is installed with R and automatically loads
when R is started.</p>
<div class="sourceCode" id="cb490"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">virginica</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>  family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span><span class="va">logit</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span></code></pre></div>
<p><em>About the warning:</em> glm.fit: fitted probabilities numerically 0 or 1 occurred means that the data is possibly linearly separable.</p>
<div class="sourceCode" id="cb491"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Call:  glm(formula = virginica ~ ., family = binomial(logit), data = x)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##  (Intercept)  Sepal.Length   Sepal.Width  Petal.Length  </span></span>
<span><span class="co">##      -41.649        -2.531        -6.448         9.376  </span></span>
<span><span class="co">##  Petal.Width       useless  </span></span>
<span><span class="co">##       17.696         0.098  </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Degrees of Freedom: 149 Total (i.e. Null);  144 Residual</span></span>
<span><span class="co">## Null Deviance:       191 </span></span>
<span><span class="co">## Residual Deviance: 11.9  AIC: 23.9</span></span></code></pre></div>
<p>Check which features are significant?</p>
<div class="sourceCode" id="cb492"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## glm(formula = virginica ~ ., family = binomial(logit), data = x)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##              Estimate Std. Error z value Pr(&gt;|z|)  </span></span>
<span><span class="co">## (Intercept)   -41.649     26.556   -1.57    0.117  </span></span>
<span><span class="co">## Sepal.Length   -2.531      2.458   -1.03    0.303  </span></span>
<span><span class="co">## Sepal.Width    -6.448      4.794   -1.34    0.179  </span></span>
<span><span class="co">## Petal.Length    9.376      4.763    1.97    0.049 *</span></span>
<span><span class="co">## Petal.Width    17.696     10.632    1.66    0.096 .</span></span>
<span><span class="co">## useless         0.098      0.807    0.12    0.903  </span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  </span></span>
<span><span class="co">## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##     Null deviance: 190.954  on 149  degrees of freedom</span></span>
<span><span class="co">## Residual deviance:  11.884  on 144  degrees of freedom</span></span>
<span><span class="co">## AIC: 23.88</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Number of Fisher Scoring iterations: 12</span></span></code></pre></div>
<p>AIC (<a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">Akaike information criterion</a>)
is a measure of how good the model is. Smaller is better. It can be used for model selection.</p>
<p>The parameter estimates in the coefficients table are log odds. The <code>*</code> and <code>.</code>
indicate if the effect of the parameter is significantly different from 0.
Positive numbers
mean that increasing the variable increases the predicted probability
and negative numbers mean that the probability decreases. For example,
observing a larger Petal.Length increases the predicted probability for the flower to
be of class Virginica. This effect is significant and you can
verify it in the scatter plot above. For Petal.Length, the red dots have
larger values than
the black dots.</p>
</div>
<div id="stepwise-variable-selection-1" class="section level2" number="10.4">
<h2>
<span class="header-section-number">C.4</span> Stepwise Variable Selection<a class="anchor" aria-label="anchor" href="#stepwise-variable-selection-1"><i class="fas fa-link"></i></a>
</h2>
<p>Only two variables were flagged as significant. We can remove insignificant
variables by trying to remove one variable at a time
as long as the model does not significantly deteriorate (according to the AIC).
This variable selection process is done automatically by the <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function.</p>
<div class="sourceCode" id="cb493"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span><span class="va">model</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="co">## Start:  AIC=23.88</span></span>
<span><span class="co">## virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + </span></span>
<span><span class="co">##     useless</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">##                Df Deviance  AIC</span></span>
<span><span class="co">## - useless       1     11.9 21.9</span></span>
<span><span class="co">## - Sepal.Length  1     13.2 23.2</span></span>
<span><span class="co">## &lt;none&gt;                11.9 23.9</span></span>
<span><span class="co">## - Sepal.Width   1     14.8 24.8</span></span>
<span><span class="co">## - Petal.Width   1     22.4 32.4</span></span>
<span><span class="co">## - Petal.Length  1     25.9 35.9</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Step:  AIC=21.9</span></span>
<span><span class="co">## virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">##                Df Deviance  AIC</span></span>
<span><span class="co">## - Sepal.Length  1     13.3 21.3</span></span>
<span><span class="co">## &lt;none&gt;                11.9 21.9</span></span>
<span><span class="co">## - Sepal.Width   1     15.5 23.5</span></span>
<span><span class="co">## - Petal.Width   1     23.8 31.8</span></span>
<span><span class="co">## - Petal.Length  1     25.9 33.9</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Step:  AIC=21.27</span></span>
<span><span class="co">## virginica ~ Sepal.Width + Petal.Length + Petal.Width</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">## Warning: glm.fit: fitted probabilities numerically 0 or 1</span></span>
<span><span class="co">## occurred</span></span>
<span><span class="co">##                Df Deviance  AIC</span></span>
<span><span class="co">## &lt;none&gt;                13.3 21.3</span></span>
<span><span class="co">## - Sepal.Width   1     20.6 26.6</span></span>
<span><span class="co">## - Petal.Length  1     27.4 33.4</span></span>
<span><span class="co">## - Petal.Width   1     31.5 37.5</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model2</span><span class="op">)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## glm(formula = virginica ~ Sepal.Width + Petal.Length + Petal.Width, </span></span>
<span><span class="co">##     family = binomial(logit), data = x)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##              Estimate Std. Error z value Pr(&gt;|z|)  </span></span>
<span><span class="co">## (Intercept)    -50.53      23.99   -2.11    0.035 *</span></span>
<span><span class="co">## Sepal.Width     -8.38       4.76   -1.76    0.079 .</span></span>
<span><span class="co">## Petal.Length     7.87       3.84    2.05    0.040 *</span></span>
<span><span class="co">## Petal.Width     21.43      10.71    2.00    0.045 *</span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  </span></span>
<span><span class="co">## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##     Null deviance: 190.954  on 149  degrees of freedom</span></span>
<span><span class="co">## Residual deviance:  13.266  on 146  degrees of freedom</span></span>
<span><span class="co">## AIC: 21.27</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Number of Fisher Scoring iterations: 12</span></span></code></pre></div>
<p>The estimates (<span class="math inline">\(\beta_0, \beta_1,...\)</span> ) are
log-odds and can be converted into odds using <span class="math inline">\(exp(\beta)\)</span>.
A negative log-odds ratio means that the odds go down with an increase in
the value of the predictor. A predictor with a
positive log-odds ratio increases the odds. In this case, the odds of
looking at a Virginica iris goes down with Sepal.Width and increases with the
other two predictors.</p>
</div>
<div id="calculate-the-response" class="section level2" number="10.5">
<h2>
<span class="header-section-number">C.5</span> Calculate the Response<a class="anchor" aria-label="anchor" href="#calculate-the-response"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Note:</strong> we do here in-sample testing on the data we learned the data
from. To get a generalization error estimate you should use a test set or
cross-validation!</p>
<div class="sourceCode" id="cb494"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">model2</span>, <span class="va">x</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">pr</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">##  102  112    4   55   70   98  135    7   43  140 </span></span>
<span><span class="co">## 1.00 1.00 0.00 0.00 0.00 0.00 0.86 0.00 0.00 1.00</span></span></code></pre></div>
<p>The response is the predicted probability of the flower being of species
Virginica. The probabilities of the first 10 flowers are shown. Below is
a histogram of predicted probabilities. The color is used to show the
examples that have the true class Virginica.</p>
<div class="sourceCode" id="cb495"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">pr</span>, breaks <span class="op">=</span> <span class="fl">20</span>, main <span class="op">=</span> <span class="st">"Predicted Probability vs. True Class"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">pr</span><span class="op">[</span><span class="va">x</span><span class="op">$</span><span class="va">virginica</span> <span class="op">==</span> <span class="cn">TRUE</span><span class="op">]</span>, col <span class="op">=</span> <span class="st">"red"</span>, breaks <span class="op">=</span> <span class="fl">20</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="R-Companion-Data-Mining_files/figure-html/unnamed-chunk-442-1.png" width="672"></div>
</div>
<div id="check-classification-performance" class="section level2" number="10.6">
<h2>
<span class="header-section-number">C.6</span> Check Classification Performance<a class="anchor" aria-label="anchor" href="#check-classification-performance"><i class="fas fa-link"></i></a>
</h2>
<p>Here we perform in-sample evaluation on the training set. To get an estimate for
generalization error, we should calculate the performance on a held out test set.</p>
<p>The predicted class is calculated by checking if the predicted probability
is larger than .5.</p>
<div class="sourceCode" id="cb496"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pred</span> <span class="op">&lt;-</span> <span class="va">pr</span> <span class="op">&gt;</span> <span class="fl">.5</span></span></code></pre></div>
<p>Now we can create a confusion table and calculate the accuracy.</p>
<div class="sourceCode" id="cb497"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tbl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>predicted <span class="op">=</span> <span class="va">pred</span>, actual <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">virginica</span><span class="op">)</span></span>
<span><span class="va">tbl</span></span>
<span><span class="co">##          actual</span></span>
<span><span class="co">## predicted FALSE TRUE</span></span>
<span><span class="co">##     FALSE    98    1</span></span>
<span><span class="co">##     TRUE      2   49</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">tbl</span><span class="op">)</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">tbl</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.98</span></span></code></pre></div>
<p>We can also use caretâ€™s more advanced function <code><a href="https://rdrr.io/pkg/caret/man/confusionMatrix.html">caret::confusionMatrix()</a></code>. Our code
above uses <code>logical</code> vectors. For caret, we need to make sure that both,
the reference and the predictions are coded as <code>factor</code>.</p>
<div class="sourceCode" id="cb498"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">caret</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/confusionMatrix.html">confusionMatrix</a></span><span class="op">(</span></span>
<span>  reference <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">virginica</span>, labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Yes"</span>, <span class="st">"No"</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span>, </span>
<span>  data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">pred</span>, labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Yes"</span>, <span class="st">"No"</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## Confusion Matrix and Statistics</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##           Reference</span></span>
<span><span class="co">## Prediction Yes No</span></span>
<span><span class="co">##        Yes  49  2</span></span>
<span><span class="co">##        No    1 98</span></span>
<span><span class="co">##                                         </span></span>
<span><span class="co">##                Accuracy : 0.98          </span></span>
<span><span class="co">##                  95% CI : (0.943, 0.996)</span></span>
<span><span class="co">##     No Information Rate : 0.667         </span></span>
<span><span class="co">##     P-Value [Acc &gt; NIR] : &lt;2e-16        </span></span>
<span><span class="co">##                                         </span></span>
<span><span class="co">##                   Kappa : 0.955         </span></span>
<span><span class="co">##                                         </span></span>
<span><span class="co">##  Mcnemar's Test P-Value : 1             </span></span>
<span><span class="co">##                                         </span></span>
<span><span class="co">##             Sensitivity : 0.980         </span></span>
<span><span class="co">##             Specificity : 0.980         </span></span>
<span><span class="co">##          Pos Pred Value : 0.961         </span></span>
<span><span class="co">##          Neg Pred Value : 0.990         </span></span>
<span><span class="co">##              Prevalence : 0.333         </span></span>
<span><span class="co">##          Detection Rate : 0.327         </span></span>
<span><span class="co">##    Detection Prevalence : 0.340         </span></span>
<span><span class="co">##       Balanced Accuracy : 0.980         </span></span>
<span><span class="co">##                                         </span></span>
<span><span class="co">##        'Positive' Class : Yes           </span></span>
<span><span class="co">## </span></span></code></pre></div>
<p>We see that the model performs well with a very high accuracy and kappa value.</p>
</div>
<div id="regularized-logistic-regression" class="section level2" number="10.7">
<h2>
<span class="header-section-number">C.7</span> Regularized Logistic Regression<a class="anchor" aria-label="anchor" href="#regularized-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p><code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet::glmnet()</a></code> fits generalized linear models (including logistic regression)
using regularization via penalized maximum likelihood.
The regularization parameter <span class="math inline">\(\lambda\)</span> is a hyperparameter and
glmnet can use cross-validation to find an appropriate
value. glmnet does not have a function interface, so we have
to supply a matrix for <code>X</code> and a vector of responses for <code>y</code>.</p>
<div class="sourceCode" id="cb499"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://glmnet.stanford.edu">glmnet</a></span><span class="op">)</span></span>
<span><span class="co">## Loaded glmnet 4.1-8</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/proxy/man/dist.html">as.matrix</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">$</span><span class="va">virginica</span></span>
<span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span>, family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span></span>
<span><span class="va">fit</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Call:  cv.glmnet(x = X, y = y, family = "binomial") </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Measure: Binomial Deviance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">##      Lambda Index Measure     SE Nonzero</span></span>
<span><span class="co">## min 0.00164    59   0.126 0.0456       5</span></span>
<span><span class="co">## 1se 0.00664    44   0.167 0.0422       3</span></span></code></pre></div>
<p>There are several selection rules for lambda, we look at the
coefficients of the logistic regression using the
lambda that gives the most regularized model such that the cross-validated error is within one standard error of the minimum cross-validated error.</p>
<div class="sourceCode" id="cb500"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fit</span>, s <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">lambda.1se</span><span class="op">)</span></span>
<span><span class="co">## 6 x 1 sparse Matrix of class "dgCMatrix"</span></span>
<span><span class="co">##                   s1</span></span>
<span><span class="co">## (Intercept)  -16.961</span></span>
<span><span class="co">## Sepal.Length   .    </span></span>
<span><span class="co">## Sepal.Width   -1.766</span></span>
<span><span class="co">## Petal.Length   2.197</span></span>
<span><span class="co">## Petal.Width    6.820</span></span>
<span><span class="co">## useless        .</span></span></code></pre></div>
<p>A dot means 0. We see that the predictors Sepal.Length and
useless are not used in the prediction giving a models similar to
stepwise variable selection above.</p>
<p>A predict function is provided. We need to specify
what regularization to use and that we want to predict a class
label.</p>
<div class="sourceCode" id="cb501"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">fit</span>, newx <span class="op">=</span> <span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>,<span class="op">]</span>, s <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">lambda.1se</span>, type <span class="op">=</span> <span class="st">"class"</span><span class="op">)</span></span>
<span><span class="co">##     s1     </span></span>
<span><span class="co">## 102 "TRUE" </span></span>
<span><span class="co">## 112 "TRUE" </span></span>
<span><span class="co">## 4   "FALSE"</span></span>
<span><span class="co">## 55  "FALSE"</span></span>
<span><span class="co">## 70  "FALSE"</span></span></code></pre></div>
<p>Glmnet provides supports many types of
generalized linear models. Examples can be found in the
article <a href="https://glmnet.stanford.edu/articles/glmnet.html">An Introduction to glmnet</a>.</p>
</div>
<div id="multinomial-logistic-regression-2" class="section level2" number="10.8">
<h2>
<span class="header-section-number">C.8</span> Multinomial Logistic Regression<a class="anchor" aria-label="anchor" href="#multinomial-logistic-regression-2"><i class="fas fa-link"></i></a>
</h2>
<p>Regular logistic regression predicts only one outcome of a binary event represented
by two classes. Extending this model to data with more than two classes
is called <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">multinomial logistic regression</a>,
(or log-linear model).
A popular implementation uses simple artificial neural networks.
Regular logistic regression is equivalent to a single neuron with a
sigmoid (i.e., logistic) activation function optimized with cross-entropy loss.
For multinomial logistic regression, one neuron is used for each class and the
probability distribution is calculated with the softmax activation.
This extension is implemented in <code><a href="https://rdrr.io/pkg/nnet/man/multinom.html">nnet::multinom()</a></code>.</p>
<div class="sourceCode" id="cb502"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/pkg/arules/man/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">nnet</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/nnet/man/multinom.html">multinom</a></span><span class="op">(</span><span class="va">Species</span> <span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="co">## # weights:  18 (10 variable)</span></span>
<span><span class="co">## initial  value 164.791843 </span></span>
<span><span class="co">## iter  10 value 16.177348</span></span>
<span><span class="co">## iter  20 value 7.111438</span></span>
<span><span class="co">## iter  30 value 6.182999</span></span>
<span><span class="co">## iter  40 value 5.984028</span></span>
<span><span class="co">## iter  50 value 5.961278</span></span>
<span><span class="co">## iter  60 value 5.954900</span></span>
<span><span class="co">## iter  70 value 5.951851</span></span>
<span><span class="co">## iter  80 value 5.950343</span></span>
<span><span class="co">## iter  90 value 5.949904</span></span>
<span><span class="co">## iter 100 value 5.949867</span></span>
<span><span class="co">## final  value 5.949867 </span></span>
<span><span class="co">## stopped after 100 iterations</span></span>
<span><span class="va">model</span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## nnet::multinom(formula = Species ~ ., data = x)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##            (Intercept) Sepal.Length Sepal.Width</span></span>
<span><span class="co">## versicolor       18.69       -5.458      -8.707</span></span>
<span><span class="co">## virginica       -23.84       -7.924     -15.371</span></span>
<span><span class="co">##            Petal.Length Petal.Width</span></span>
<span><span class="co">## versicolor        14.24      -3.098</span></span>
<span><span class="co">## virginica         23.66      15.135</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual Deviance: 11.9 </span></span>
<span><span class="co">## AIC: 31.9</span></span></code></pre></div>
<p>We get a <span class="math inline">\(\boldsymbol{\beta}\)</span> vector with weights for two of the three classes.
The third class is used as the default class with all weights set to 0. This can
be interpreted as comparing the log odds of each of the two classes with the
default class. A positive number means that increasing the variable makes the
class more likely and a negative number means the opposite.</p>
<p>Predict the class for the first 5 flowers in the training data.</p>
<div class="sourceCode" id="cb503"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="op">]</span></span>
<span><span class="co">##     Sepal.Length Sepal.Width Petal.Length Petal.Width</span></span>
<span><span class="co">## 102          5.8         2.7          5.1         1.9</span></span>
<span><span class="co">## 112          6.4         2.7          5.3         1.9</span></span>
<span><span class="co">## 4            4.6         3.1          1.5         0.2</span></span>
<span><span class="co">## 55           6.5         2.8          4.6         1.5</span></span>
<span><span class="co">## 70           5.6         2.5          3.9         1.1</span></span>
<span><span class="co">##        Species</span></span>
<span><span class="co">## 102  virginica</span></span>
<span><span class="co">## 112  virginica</span></span>
<span><span class="co">## 4       setosa</span></span>
<span><span class="co">## 55  versicolor</span></span>
<span><span class="co">## 70  versicolor</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">model</span>, <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>,<span class="op">]</span><span class="op">)</span></span>
<span><span class="co">## [1] virginica  virginica  setosa     versicolor versicolor</span></span>
<span><span class="co">## Levels: setosa versicolor virginica</span></span></code></pre></div>
<p>The package glmnet implements also multinomial logistic regression using
<code>glmnet(..., family = "multinomial")</code>.</p>
</div>
<div id="exercises-7" class="section level2" number="10.9">
<h2>
<span class="header-section-number">C.9</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-7"><i class="fas fa-link"></i></a>
</h2>
<p>We will again use the Palmer penguin data for the exercises.</p>
<div class="sourceCode" id="cb504"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://allisonhorst.github.io/palmerpenguins/">palmerpenguins</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">penguins</span><span class="op">)</span></span>
<span><span class="co">## # A tibble: 6 Ã— 8</span></span>
<span><span class="co">##   species island    bill_length_mm bill_depth_mm</span></span>
<span><span class="co">##   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;</span></span>
<span><span class="co">## 1 Adelie  Torgersen           39.1          18.7</span></span>
<span><span class="co">## 2 Adelie  Torgersen           39.5          17.4</span></span>
<span><span class="co">## 3 Adelie  Torgersen           40.3          18  </span></span>
<span><span class="co">## 4 Adelie  Torgersen           NA            NA  </span></span>
<span><span class="co">## 5 Adelie  Torgersen           36.7          19.3</span></span>
<span><span class="co">## 6 Adelie  Torgersen           39.3          20.6</span></span>
<span><span class="co">## # â„¹ 4 more variables: flipper_length_mm &lt;dbl&gt;,</span></span>
<span><span class="co">## #   body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;</span></span></code></pre></div>
<p>Create an R markdown document that performs the following:</p>
<ol style="list-style-type: decimal">
<li>Create a test and a training data set (see section <a href="classification-basic-concepts.html#holdout-method">Holdout Method</a> in Chapter 3).</li>
<li>Create a logistic regression using the training set to predict the variable sex.</li>
<li>Use stepwise variable selection. What variables are selected?</li>
<li>What do the parameters for for each of the selected features tell you?</li>
<li>Predict the sex of the penguins in the test set. Create a
confusion table and calculate the accuracy and discuss how well the model works.</li>
</ol>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="regression.html"><span class="header-section-number">B</span> Regression</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#logistic-regression"><span class="header-section-number">C</span> Logistic Regression</a></li>
<li><a class="nav-link" href="#introduction-2"><span class="header-section-number">C.1</span> Introduction</a></li>
<li><a class="nav-link" href="#data-preparation-1"><span class="header-section-number">C.2</span> Data Preparation</a></li>
<li><a class="nav-link" href="#a-first-logistic-regression-model"><span class="header-section-number">C.3</span> A first Logistic Regression Model</a></li>
<li><a class="nav-link" href="#stepwise-variable-selection-1"><span class="header-section-number">C.4</span> Stepwise Variable Selection</a></li>
<li><a class="nav-link" href="#calculate-the-response"><span class="header-section-number">C.5</span> Calculate the Response</a></li>
<li><a class="nav-link" href="#check-classification-performance"><span class="header-section-number">C.6</span> Check Classification Performance</a></li>
<li><a class="nav-link" href="#regularized-logistic-regression"><span class="header-section-number">C.7</span> Regularized Logistic Regression</a></li>
<li><a class="nav-link" href="#multinomial-logistic-regression-2"><span class="header-section-number">C.8</span> Multinomial Logistic Regression</a></li>
<li><a class="nav-link" href="#exercises-7"><span class="header-section-number">C.9</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mhahsler/Introduction_to_Data_Mining_R_Examples/blob/master/book_src/92_logistic_regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mhahsler/Introduction_to_Data_Mining_R_Examples/edit/master/book_src/92_logistic_regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>An R Companion for Introduction to Data Mining</strong>" was written by Michael Hahsler. It was last built on 2025-02-15.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
