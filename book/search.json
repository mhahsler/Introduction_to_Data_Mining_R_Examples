[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"book contains documented R examples accompany several chapters\npopular data mining textbook Introduction Data\nMining Pang-Ning\nTan, Michael Steinbach, Anuj Karpatne Vipin Kumar. companion\nbook can used either edition: 1st edition (Tan, Steinbach, Kumar 2005) 2nd\nedition (Tan et al. 2017).code examples collected book developed course\nCS 7331 - Data Mining taught\nSMU since Spring 2013 regularly\nupdated improved.\nlearning method used book learning--.\ncode examples throughout book \nwritten self-contained manner can copy--paste portion code,\ntry provided dataset apply directly \ndata.latest update includes use popular\npackages meta-package tidyverse (Wickham 2023b) including\nggplot2 (Wickham, Chang, et al. 2023) data wrangling visualization along \ncaret (M. Kuhn 2023) model building evaluation.\nPresentation slides instructor resources available \nbook’s GitHub page.\nPlease use edit function within book visit book’s\nGitHub project\npage\nsubmit corrections suggest improvements. cite book use:Michael Hahsler (2021). R Companion Introduction Data\nMining. Online Book.\nhttps://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/hope book helps learn use R efficiently \ndata mining projects.Michael Hahsler","code":""},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":" online version \nbook licensed Creative Commons\nAttribution-NonCommercial-ShareAlike 4.0 International\nLicense.cover art based \n“rocks” \nstebulus licensed CC\n\n2.0.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Data mining goal finding patterns large data sets.\nchapter, talk data characteristics prepared \ndata mining.book organized following main data mining tasks:Data preparation exploratory data analysis (Chapter 2)Classification (Chapters 3 4)Association analysis (Chapters 5 6)Clustering (Chapter 7)First, need talk needed software.","code":""},{"path":"introduction.html","id":"used-software","chapter":"1 Introduction","heading":"1.1 Used Software","text":"companion book assumes \nR RStudio\nDesktop installed \nfamiliar basics R, run R code, install\npackages.new R, working official R manual \nIntroduction \nR\n(Venables, Smith, R Core Team 2021) get started. many introduction videos\nRStudio available, basic video shows run code \ninstall packages suffice.book chapter use set packages must installed.\ninstallation code can found beginning chapter.\ncode install packages used chapter:packages used chapter : ggplot2 (Wickham, Chang, et al. 2023), tidyverse (Wickham 2023b)code book uses tidyverse manipulate data ggplot2\nvisualization. great introduction useful tools can\nfound freely available web book R Data\nScience Wickham Grolemund (2017).","code":"\npkgs <- sort(c('tidyverse', 'ggplot2'))\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"introduction.html","id":"tidyverse","chapter":"1 Introduction","heading":"1.2 Tidyverse","text":"tidyverse (Wickham 2023b) collection many useful packages\nwork well together sharing design principles data\nstructures. tidyverse also includes ggplot2 (Wickham, Chang, et al. 2023) \nvisualization.book, useoften tidyverse tibbles replace R’s built-data.frames,pipe operator |> chain functions together, anddata transformation functions like filter(), arrange(),\nselect(), group_by(), mutate() provided tidyverse\npackage dplyr.good introduction can found Section Data\nWrangling (Wickham Grolemund 2017),\nuseful reference resource RStudio Data Transformation\nCheat\nSheet.short example get familiar basic syntax.\ncreate tibble price dollars\nper pound vitamin C content milligrams (mg) per pound \nthree fruit.Now add column vitamin C (mg) dollar buys ,\nfilter fruit provides 20 mg, order (arrange)\ndata vitamin C per dollar largest smallest.pipes operator |> lets compose sequence function calls\nreadably passing value left first\nargument function right.","code":"\nlibrary(tidyverse)## ── Attaching core tidyverse packages ────────────────────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.2     ✔ readr     2.1.4\n## ✔ forcats   1.0.0     ✔ stringr   1.5.0\n## ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n## ✔ purrr     1.0.1     \n## ── Conflicts ──────────────────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nfruit <- tibble(\n  name = c(\"apple\", \"banana\", \"orange\"), \n  price = c(2.5, 2.0, 3.5), \n  vitamin_c = c(20, 45, 250))\nfruit## # A tibble: 3 × 3\n##   name   price vitamin_c\n##   <chr>  <dbl>     <dbl>\n## 1 apple    2.5        20\n## 2 banana   2          45\n## 3 orange   3.5       250\naffordable_vitamin_c_sources <- fruit |>\n  mutate(vitamin_c_per_dollar = vitamin_c / price) |> \n  filter(vitamin_c_per_dollar > 20) |>\n  arrange(desc(vitamin_c_per_dollar))\n\naffordable_vitamin_c_sources ## # A tibble: 2 × 4\n##   name   price vitamin_c vitamin_c_per_dollar\n##   <chr>  <dbl>     <dbl>                <dbl>\n## 1 orange   3.5       250                 71.4\n## 2 banana   2          45                 22.5"},{"path":"introduction.html","id":"ggplot2","chapter":"1 Introduction","heading":"1.3 ggplot2","text":"visualization, use mainly ggplot2. gg ggplot2\nstands Grammar Graphics introduced Wilkinson (2005). \nmain idea every graph built basic components:data,coordinate system, andvisual marks representing data (geoms).ggplot2, components combined using + operator.ggplot(data, mapping = aes(x = ..., y = ..., color = ...)) +\ngeom_point()Since typically use Cartesian coordinate system, ggplot uses \ndefault. geom_ function uses stat_ function calculate\nvisualizes. example, geom_bar uses stat_count create\nbar chart counting often value appears data (see\n? geom_bar). geom_point just uses stat \"identity\" display\npoints using coordinates . great introduction can\nfound Chapter Data\nVisualization\n(Wickham Grolemund 2017), useful RStudio’s Data Visualization Cheat\nSheet.can visualize fruit data scatter plot.\neasy add geoms. example, can add regression line\nusing geom_smooth method \"lm\" (linear model). suppress \nconfidence interval since 3 data points.Alternatively, can visualize fruit’s vitamin C content per dollar\nusing bar chart.Note geom_bar default uses stat_count statistics \naggregate data counting, \njust want visualize value already available tibble, \nspecify identity statistic instead.","code":"\nggplot(fruit, aes(x = price, y = vitamin_c)) + \n  geom_point()\nggplot(fruit, aes(x = price, y = vitamin_c)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)## `geom_smooth()` using formula = 'y ~ x'\nggplot(fruit, aes(x = name, y = vitamin_c)) + \n  geom_bar(stat = \"identity\")"},{"path":"data.html","id":"data","chapter":"2 Data","heading":"2 Data","text":"chapter provides examples cleaning preparing data data\nmining.Install packages used chapter:packages used chapter : arules (Hahsler et al. 2023), caret (M. Kuhn 2023), factoextra (Kassambara Mundt 2020), GGally (Schloerke et al. 2021), ggcorrplot (Kassambara 2022), plotly (Sievert et al. 2023), proxy (Meyer Buchta 2022), sampling (Tillé Matei 2021), seriation (Hahsler, Buchta, Hornik 2023), tidyverse (Wickham 2023b)","code":"\npkgs <- sort(c('tidyverse', 'GGally', 'ggcorrplot', \n    'plotly', 'factoextra', 'arules', 'seriation', \n    'sampling', 'caret', 'proxy'))\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"data.html","id":"introduction-1","chapter":"2 Data","heading":"2.1 Introduction","text":"Data data mining typically organized tabular form, rows containing\nobjects interest columns representing features describing objects.\ndiscuss topics like data quality, sampling, feature selection,\nmeasure similarities objects features.\nsecond part chapter deals data exploration visualization.","code":""},{"path":"data.html","id":"scale-of-measurement","chapter":"2 Data","heading":"2.2 Scale of Measurement","text":"values features can measured several scales\nranging \nsimple labels way numbers. scales come four levels.scales build meaning ordinal variable also characteristics \nnominal variable added order information.\noften differentiate interval ratio scale rarely\nneed calculate percentages statistics require meaningful zero value.\nfollowing code gives example vectors nominal, ordinal (levels defines order)\ninterval scale.","code":"\nfactor(c(\"red\", \"green\", \"green\", \"blue\"))## [1] red   green green blue \n## Levels: blue green red\nfactor(\"S\", \"L\", \"M\", \"S\", levels = c(\"S\", \"M\", \"L\"), ordered = TRUE)## [1] L1\n## Levels: L1 < L2\nc(1, 2, 3, 4, 3)## [1] 1 2 3 4 3"},{"path":"data.html","id":"the-iris-dataset","chapter":"2 Data","heading":"2.3 The Iris Dataset","text":"use toy dataset comes R. Fisher’s iris\ndataset gives \nmeasurements centimeters variables sepal length, sepal width\npetal length, petal width representing features 150 flowers (objects).\ndataset contains 50\nflowers 3 species iris. species Iris Setosa,\nIris Versicolor, Iris Virginica. details see ? iris.load iris data set. Datasets come R R packages can\nloaded data(). standard format data R \ndata.frame. convert data.frame tidyverse tibble.see data contains 150 rows (flowers) 5 features. tibbles\nshow first rows show features, \nfit screen width. can call print define many rows\nshow using parameter n force print show features \nchanging width infinity.","code":"\nlibrary(tidyverse)\ndata(iris)\niris <- as_tibble(iris)\niris## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1          5.1         3.5          1.4         0.2\n##  2          4.9         3            1.4         0.2\n##  3          4.7         3.2          1.3         0.2\n##  4          4.6         3.1          1.5         0.2\n##  5          5           3.6          1.4         0.2\n##  6          5.4         3.9          1.7         0.4\n##  7          4.6         3.4          1.4         0.3\n##  8          5           3.4          1.5         0.2\n##  9          4.4         2.9          1.4         0.2\n## 10          4.9         3.1          1.5         0.1\n## # ℹ 140 more rows\n## # ℹ 1 more variable: Species <fct>\nprint(iris, n = 3, width = Inf)## # A tibble: 150 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1          5.1         3.5          1.4         0.2\n## 2          4.9         3            1.4         0.2\n## 3          4.7         3.2          1.3         0.2\n##   Species\n##   <fct>  \n## 1 setosa \n## 2 setosa \n## 3 setosa \n## # ℹ 147 more rows"},{"path":"data.html","id":"data-quality","chapter":"2 Data","heading":"2.4 Data Quality","text":"Assessing quality available data crucial start\nusing data. Start summary statistics column \nidentify outliers missing values.can also summarize numeric columns using statistic function like\nmean().find outliers data problems, need look small\nvalues (often suspicious large number zeros) using min \nextremely large values using max. Comparing median mean tells us \ndistribution symmetric.visual method inspect data use scatterplot matrix (\nuse ggpairs() package GGally). plot, can\nvisually identify noise data points outliers (points far\nmajority points).See can spot one red dot far away others.Many data mining methods require complete data, data \ncontain missing values (NA). remove missing values duplicates\n(identical data points might mistake data), often\n:Note one non-unique case gone leaving 149 flowers. data\ncontain missing values, , also \ndropped. Typically, spend lot time data cleaning.","code":"\nsummary(iris)##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.35  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width        Species  \n##  Min.   :0.1   setosa    :50  \n##  1st Qu.:0.3   versicolor:50  \n##  Median :1.3   virginica :50  \n##  Mean   :1.2                  \n##  3rd Qu.:1.8                  \n##  Max.   :2.5\niris |> \n  summarize(across(where(is.numeric), mean))## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         5.84        3.06         3.76        1.20\nlibrary(GGally)## Registered S3 method overwritten by 'GGally':\n##   method from   \n##   +.gg   ggplot2\nggpairs(iris, aes(color = Species), progress = FALSE)## `stat_bin()` using `bins = 30`. Pick better value with\n## `binwidth`.## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nclean.data <- iris |> \n  drop_na() |> \n  unique()\n\nsummary(clean.data)##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.30  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.75  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width        Species  \n##  Min.   :0.1   setosa    :50  \n##  1st Qu.:0.3   versicolor:50  \n##  Median :1.3   virginica :49  \n##  Mean   :1.2                  \n##  3rd Qu.:1.8                  \n##  Max.   :2.5"},{"path":"data.html","id":"aggregation","chapter":"2 Data","heading":"2.5 Aggregation","text":"Data often contains groups want compare groups. group\niris dataset species calculate summary statistic \ngroup.Using information, can compare features differ \ngroups.","code":"\niris |> \n  group_by(Species) |> \n  summarize(across(everything(), mean))## # A tibble: 3 × 5\n##   Species    Sepal.Length Sepal.Width Petal.Length\n##   <fct>             <dbl>       <dbl>        <dbl>\n## 1 setosa             5.01        3.43         1.46\n## 2 versicolor         5.94        2.77         4.26\n## 3 virginica          6.59        2.97         5.55\n## # ℹ 1 more variable: Petal.Width <dbl>\niris |> \n  group_by(Species) |> \n  summarize(across(everything(), median))## # A tibble: 3 × 5\n##   Species    Sepal.Length Sepal.Width Petal.Length\n##   <fct>             <dbl>       <dbl>        <dbl>\n## 1 setosa              5           3.4         1.5 \n## 2 versicolor          5.9         2.8         4.35\n## 3 virginica           6.5         3           5.55\n## # ℹ 1 more variable: Petal.Width <dbl>"},{"path":"data.html","id":"sampling","chapter":"2 Data","heading":"2.6 Sampling","text":"Sampling often\nused data mining reduce dataset size modeling \nvisualization.","code":""},{"path":"data.html","id":"random-sampling","chapter":"2 Data","heading":"2.6.1 Random Sampling","text":"built-sample function can sample vector. sample\nreplacement.often want sample rows dataset. can done \nsampling without replacement vector row indices (using \nfunctions seq() nrow()). sample vector used \nsubset rows dataset.dplyr tidyverse lets us sample rows tibbles directly using\nslice_sample(). set random number generator seed make \nresults reproducible.","code":"\nsample(c(\"A\", \"B\", \"C\"), size = 10, replace = TRUE)##  [1] \"B\" \"B\" \"B\" \"A\" \"B\" \"A\" \"A\" \"C\" \"A\" \"B\"\ntake <- sample(seq(nrow(iris)), size = 15)\ntake##  [1] 118  78 114  94  14   9  54   7 127  91 134  27\n## [13]  29 124  82\niris[take, ]## # A tibble: 15 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1          7.7         3.8          6.7         2.2\n##  2          6.7         3            5           1.7\n##  3          5.7         2.5          5           2  \n##  4          5           2.3          3.3         1  \n##  5          4.3         3            1.1         0.1\n##  6          4.4         2.9          1.4         0.2\n##  7          5.5         2.3          4           1.3\n##  8          4.6         3.4          1.4         0.3\n##  9          6.2         2.8          4.8         1.8\n## 10          5.5         2.6          4.4         1.2\n## 11          6.3         2.8          5.1         1.5\n## 12          5           3.4          1.6         0.4\n## 13          5.2         3.4          1.4         0.2\n## 14          6.3         2.7          4.9         1.8\n## 15          5.5         2.4          3.7         1  \n## # ℹ 1 more variable: Species <fct>\nset.seed(1000)\n\ns <- iris |> \n  slice_sample(n = 15)\nggpairs(s, aes(color = Species), progress = FALSE)## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"data.html","id":"stratified-sampling","chapter":"2 Data","heading":"2.6.2 Stratified Sampling","text":"Stratified sampling\nmethod sampling population can partitioned \nsubpopulations, controlling proportions subpopulation\nresulting sample.following, subpopulations different types species\nwant make sure sample number (5) flowers \n. library sampling provides function stratified\nsampling. column ID_unit resulting data.frame contains \nrow numbers sampled rows. can use slice() dplyr \nselect sampled rows.","code":"\nlibrary(sampling)\nid2 <- strata(iris, stratanames = \"Species\", \n              size = c(5,5,5), method = \"srswor\")\nid2##        Species ID_unit Prob Stratum\n## 7       setosa       7  0.1       1\n## 9       setosa       9  0.1       1\n## 10      setosa      10  0.1       1\n## 24      setosa      24  0.1       1\n## 48      setosa      48  0.1       1\n## 58  versicolor      58  0.1       2\n## 62  versicolor      62  0.1       2\n## 74  versicolor      74  0.1       2\n## 78  versicolor      78  0.1       2\n## 99  versicolor      99  0.1       2\n## 106  virginica     106  0.1       3\n## 107  virginica     107  0.1       3\n## 127  virginica     127  0.1       3\n## 135  virginica     135  0.1       3\n## 145  virginica     145  0.1       3\ns2 <- iris |> \n  slice(id2$ID_unit)\n\nggpairs(s2, aes(color = Species), progress = FALSE)## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"data.html","id":"features","chapter":"2 Data","heading":"2.7 Features","text":"number features often called dimensional data following \nidea feature (least numeric features) can seen axis data.Common feature preprocessing includes\ndimensionality reduction\ntries represent high-dimensional data low-dimensional space \nlow-dimensional representation retains meaningful properties (e.g., information \nsimilarity distances) original data. special case feature selection.important feature preprocessing includes discretization feature scaling.","code":""},{"path":"data.html","id":"principal-components-analysis-pca","chapter":"2 Data","heading":"2.7.0.1 Principal Components Analysis (PCA)","text":"PCA\ncalculates principal components (set new orthonormal basis vectors\ndata space) data points first principal\ncomponent explains variability data, second next\n. data analysis, PCA used project\nhigh-dimensional data points onto first (typically two)\nprincipal components visualization scatter plot \npreprocessing modeling (e.g., k-means clustering). Points\ncloser together high-dimensional original space, tend\nalso closer together projected lower-dimensional space,can use interactive 3-d plot (package plotly) look \nthree four dimensions iris dataset. Note hard\nvisualize 3 dimensions.principal components can calculated matrix using \nfunction prcomp(). select numeric columns (unselecting \nspecies column) convert tibble matrix \ncalculation.important principal component can also seen using \nscree plot. plot\nfunction result prcomp function visualizes much\nvariability data explained additional principal\ncomponent.Note first principal component (PC1) explains \nvariability iris dataset.find information stored object pc, can\ninspect raw object (display structure).object pc (like objects R) list class\nattribute. list element x contains data points projected \nprincipal components. can convert matrix tibble \nadd species column original dataset back (since rows\norder), display data projected first\ntwo principal components.Flowers displayed close together projection also\nclose together original 4-dimensional space. Since first\nprincipal component represents variability, can also show\ndata projected PC1.see can perfectly separate species Setosa using just \nfirst principal component. two species harder separate.plot projected data original axes added arrows \ncalled biplot. arrows\n(original axes) align roughly axes projection, \ncorrelated (linearly dependent).can also display old new axes.see Petal.Width Petal.Length point direction \nindicates highly correlated. also roughly aligned\nPC1 (called Dim1 plot) means PC1 represents \nvariability two variables. Sepal.Width almost aligned\ny-axis therefore represented PC2 (Dim2).\nPetal.Width/Petal.Length Sepal.Width almost 90 degrees,\nindicating close uncorrelated. Sepal.Length \ncorrelated variables represented , PC1 PC2\nprojection.exist methods embed data higher dimensions \nlower-dimensional space. popular method project data lower\ndimensions visualization t-distributed stochastic neighbor\nembedding (t-SNE) available package Rtsne.","code":"\n# library(plotly) # I don't load the package because it's namespace clashes with select in dplyr.\nplotly::plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, z = ~Sepal.Width, \n      color = ~Species, size = 1) |> \n  plotly::add_markers()\npc <- iris |> \n  select(-Species) |> \n  as.matrix() |> \n  prcomp()\nsummary(pc)## Importance of components:\n##                          PC1    PC2    PC3     PC4\n## Standard deviation     2.056 0.4926 0.2797 0.15439\n## Proportion of Variance 0.925 0.0531 0.0171 0.00521\n## Cumulative Proportion  0.925 0.9777 0.9948 1.00000\nplot(pc, type = \"line\")\nstr(pc)## List of 5\n##  $ sdev    : num [1:4] 2.056 0.493 0.28 0.154\n##  $ rotation: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 -0.6566 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n##   .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\"\n##  $ center  : Named num [1:4] 5.84 3.06 3.76 1.2\n##   ..- attr(*, \"names\")= chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n##  $ scale   : logi FALSE\n##  $ x       : num [1:150, 1:4] -2.68 -2.71 -2.89 -2.75 -2.73 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\"\n##  - attr(*, \"class\")= chr \"prcomp\"\niris_projected <- as_tibble(pc$x) |> \n  add_column(Species = iris$Species)\n\nggplot(iris_projected, aes(x = PC1, y = PC2, color = Species)) + \n  geom_point()\nggplot(iris_projected, \n  aes(x = PC1, y = 0, color = Species)) + \n  geom_point() +\n  scale_y_continuous(expand=c(0,0)) +\n  theme(axis.text.y = element_blank(),\n      axis.title.y = element_blank()\n  )\nlibrary(factoextra)## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_pca(pc)\nfviz_pca_var(pc)"},{"path":"data.html","id":"multi-dimensional-scaling-mds","chapter":"2 Data","heading":"2.7.0.2 Multi-Dimensional Scaling (MDS)","text":"MDS similar\nPCA. Instead data points, starts pairwise distances (.e.,\ndistance matrix) produces space points placed \nrepresent distances well possible. axes space\ncalled components similar principal components \nPCA.First, calculate distance matrix (Euclidean distances) 4-d\nspace iris dataset.Metric (classic) MDS tries construct space points lower\ndistances placed closer together. project data represented \ndistance matrix k = 2 dimensions.resulting projection similar (except rotation reflection)\nresult projection using PCA.","code":"\nd <- iris |> \n  select(-Species) |> \n  dist()\nfit <- cmdscale(d, k = 2)\ncolnames(fit) <- c(\"comp1\", \"comp2\")\nfit <- as_tibble(fit) |> \n  add_column(Species = iris$Species)\n\nggplot(fit, aes(x = comp1, y = comp2, color = Species)) + geom_point()"},{"path":"data.html","id":"non-parametric-multidimensional-scaling","chapter":"2 Data","heading":"2.7.0.3 Non-Parametric Multidimensional Scaling","text":"Non-parametric multidimensional scaling performs MDS relaxing \nneed linear relationships. Methods available package MASS \nfunctions isoMDS() sammon().","code":""},{"path":"data.html","id":"feature-selection","chapter":"2 Data","heading":"2.7.1 Feature Selection","text":"Feature selection process identifying features \nused create model. talk feature selection \ndiscuss classification models Chapter 3 Feature Selection \nFeature Preparation.","code":""},{"path":"data.html","id":"discretize-features","chapter":"2 Data","heading":"2.7.2 Discretize Features","text":"data mining methods require discrete data. Discretization converts\ncontinuous features discrete features. example, \ndiscretize continuous feature Petal.Width. perform\ndiscretization, look distribution see gives\nus idea group continuous values set \ndiscrete values. histogram visualizes distribution single\ncontinuous feature.bins histogram represent discretization using fixed bin\nwidth. R function cut() performs equal interval width\ndiscretization.discretization methods include equal frequency discretization \nusing k-means clustering. methods implemented several R\npackages. use implementation package arules \nvisualize results histograms blue lines separate\nintervals assigned discrete value.user needs decide number intervals used method.","code":"\nggplot(iris, aes(x = Petal.Width)) + geom_histogram(binwidth = .2)\niris |> \n  pull(Sepal.Width) |> \n  cut(breaks = 3)##   [1] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##   [6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [11] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (3.6,4.4]\n##  [16] (3.6,4.4] (3.6,4.4] (2.8,3.6] (3.6,4.4] (3.6,4.4]\n##  [21] (2.8,3.6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [26] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [31] (2.8,3.6] (2.8,3.6] (3.6,4.4] (3.6,4.4] (2.8,3.6]\n##  [36] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [41] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (3.6,4.4]\n##  [46] (2.8,3.6] (3.6,4.4] (2.8,3.6] (3.6,4.4] (2.8,3.6]\n##  [51] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]  \n##  [56] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [61] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6]\n##  [66] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]  \n##  [71] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n##  [76] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2,2.8]  \n##  [81] (2,2.8]   (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n##  [86] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [91] (2,2.8]   (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]  \n##  [96] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]  \n## [101] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## [106] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6]\n## [111] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2,2.8]  \n## [116] (2.8,3.6] (2.8,3.6] (3.6,4.4] (2,2.8]   (2,2.8]  \n## [121] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n## [126] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6]\n## [131] (2,2.8]   (3.6,4.4] (2,2.8]   (2,2.8]   (2,2.8]  \n## [136] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## [141] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6]\n## [146] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## Levels: (2,2.8] (2.8,3.6] (3.6,4.4]\nlibrary(arules)## Loading required package: Matrix## \n## Attaching package: 'Matrix'## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack## \n## Attaching package: 'arules'## The following object is masked from 'package:dplyr':\n## \n##     recode## The following objects are masked from 'package:base':\n## \n##     abbreviate, write\niris |> pull(Petal.Width) |> \n  discretize(method = \"interval\", breaks = 3)##   [1] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##   [6] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [11] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [16] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [21] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [26] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [31] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [36] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [41] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [46] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [51] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [56] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [61] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [66] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [71] [1.7,2.5] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [76] [0.9,1.7) [0.9,1.7) [1.7,2.5] [0.9,1.7) [0.9,1.7)\n##  [81] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [86] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [91] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [96] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n## [101] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [106] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [111] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [116] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7)\n## [121] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [126] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7)\n## [131] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) [0.9,1.7)\n## [136] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [141] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [146] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## attr(,\"discretized:breaks\")\n## [1] 0.1 0.9 1.7 2.5\n## attr(,\"discretized:method\")\n## [1] interval\n## Levels: [0.1,0.9) [0.9,1.7) [1.7,2.5]\niris |> pull(Petal.Width) |> \n  discretize(method = \"frequency\", breaks = 3)##   [1] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##   [5] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##   [9] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [13] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [17] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [21] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [25] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [29] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [33] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [37] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [41] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [45] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [49] [0.1,0.867) [0.1,0.867) [0.867,1.6) [0.867,1.6)\n##  [53] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [57] [1.6,2.5]   [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [61] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [65] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [69] [0.867,1.6) [0.867,1.6) [1.6,2.5]   [0.867,1.6)\n##  [73] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [77] [0.867,1.6) [1.6,2.5]   [0.867,1.6) [0.867,1.6)\n##  [81] [0.867,1.6) [0.867,1.6) [0.867,1.6) [1.6,2.5]  \n##  [85] [0.867,1.6) [1.6,2.5]   [0.867,1.6) [0.867,1.6)\n##  [89] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [93] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [97] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n## [101] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [105] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [109] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [113] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [117] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [0.867,1.6)\n## [121] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [125] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [129] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [133] [1.6,2.5]   [0.867,1.6) [0.867,1.6) [1.6,2.5]  \n## [137] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [141] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [145] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [149] [1.6,2.5]   [1.6,2.5]  \n## attr(,\"discretized:breaks\")\n## [1] 0.100 0.867 1.600 2.500\n## attr(,\"discretized:method\")\n## [1] frequency\n## Levels: [0.1,0.867) [0.867,1.6) [1.6,2.5]\niris |> pull(Petal.Width) |> \n  discretize(method = \"cluster\", breaks = 3)##   [1] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##   [4] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##   [7] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [10] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [13] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [16] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [19] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [22] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [25] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [28] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [31] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [34] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [37] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [40] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [43] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [46] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [49] [0.1,0.792)  [0.1,0.792)  [0.792,1.71)\n##  [52] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [55] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [58] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [61] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [64] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [67] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [70] [0.792,1.71) [1.71,2.5]   [0.792,1.71)\n##  [73] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [76] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [79] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [82] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [85] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [88] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [91] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [94] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [97] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n## [100] [0.792,1.71) [1.71,2.5]   [1.71,2.5]  \n## [103] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [106] [1.71,2.5]   [0.792,1.71) [1.71,2.5]  \n## [109] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [112] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [115] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [118] [1.71,2.5]   [1.71,2.5]   [0.792,1.71)\n## [121] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [124] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [127] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [130] [0.792,1.71) [1.71,2.5]   [1.71,2.5]  \n## [133] [1.71,2.5]   [0.792,1.71) [0.792,1.71)\n## [136] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [139] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [142] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [145] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [148] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## attr(,\"discretized:breaks\")\n## [1] 0.100 0.792 1.705 2.500\n## attr(,\"discretized:method\")\n## [1] cluster\n## Levels: [0.1,0.792) [0.792,1.71) [1.71,2.5]\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept = iris |> pull(Petal.Width) |> \n        discretize(method = \"interval\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: interval\", \n       subtitle = \"Blue lines are boundaries\")\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept = iris |> pull(Petal.Width) |> \n        discretize(method = \"frequency\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: frequency\", \n       subtitle = \"Blue lines are boundaries\")\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept = iris |> pull(Petal.Width) |> \n               discretize(method = \"cluster\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: cluster\", \n       subtitle = \"Blue lines are boundaries\")"},{"path":"data.html","id":"standardize-data","chapter":"2 Data","heading":"2.7.3 Standardize Data","text":"Standardizing (scaling, normalizing) range features values \nimportant make comparable. popular method convert\nvalues feature \nz-scores. subtracting\nmean (centering) dividing standard deviation (scaling).\nstandardized feature mean zero measured \nstandard deviations mean. Positive values indicate many\nstandard deviation original feature value average.\nNegative standardized values indicate -average values.Note: tidyverse currently simple scale function, \nmake one provides wrapper standard scale function R:standardized feature mean zero “normal” values\nfall range \\([-3,3]\\) measured standard deviations average.\nNegative values mean smaller average positive values mean larger average.","code":"\nscale_numeric <- function(x) \n  x |> \n  mutate(across(where(is.numeric), ~as.vector(scale(.))))\n\niris.scaled <- iris |> \n  scale_numeric()\niris.scaled## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1       -0.898      1.02          -1.34       -1.31\n##  2       -1.14      -0.132         -1.34       -1.31\n##  3       -1.38       0.327         -1.39       -1.31\n##  4       -1.50       0.0979        -1.28       -1.31\n##  5       -1.02       1.25          -1.34       -1.31\n##  6       -0.535      1.93          -1.17       -1.05\n##  7       -1.50       0.786         -1.34       -1.18\n##  8       -1.02       0.786         -1.28       -1.31\n##  9       -1.74      -0.361         -1.34       -1.31\n## 10       -1.14       0.0979        -1.28       -1.44\n## # ℹ 140 more rows\n## # ℹ 1 more variable: Species <fct>\nsummary(iris.scaled)##   Sepal.Length     Sepal.Width      Petal.Length   \n##  Min.   :-1.864   Min.   :-2.426   Min.   :-1.562  \n##  1st Qu.:-0.898   1st Qu.:-0.590   1st Qu.:-1.222  \n##  Median :-0.052   Median :-0.132   Median : 0.335  \n##  Mean   : 0.000   Mean   : 0.000   Mean   : 0.000  \n##  3rd Qu.: 0.672   3rd Qu.: 0.557   3rd Qu.: 0.760  \n##  Max.   : 2.484   Max.   : 3.080   Max.   : 1.780  \n##   Petal.Width           Species  \n##  Min.   :-1.442   setosa    :50  \n##  1st Qu.:-1.180   versicolor:50  \n##  Median : 0.132   virginica :50  \n##  Mean   : 0.000                  \n##  3rd Qu.: 0.788                  \n##  Max.   : 1.706"},{"path":"data.html","id":"proximities-similarities-and-distances","chapter":"2 Data","heading":"2.8 Proximities: Similarities and Distances","text":"Proximities help quantifying similar two objects .\nSimilariy concept geometry.\nbest-known\nway define similarity Euclidean distance, proximities can measured \ndifferent ways depending information objects.R stores proximity dissimilarities/distances matrices. Similarities\nfirst converted dissimilarities. Distances symmetric, .e.,\ndistance B distance B . R\ntherefore stores triangle (typically lower triangle) \ndistance matrix.","code":""},{"path":"data.html","id":"minkowsky-distances","chapter":"2 Data","heading":"2.8.1 Minkowsky Distances","text":"Minkowsky\ndistance family\nmetric distances including Euclidean Manhattan distance. avoid\none feature dominate distance calculation, scaled data \ntypically used. select first 5 flowers example.Different types Minkowsky distance matrices first 5\nflowers can calculated using dist().see lower triangle distance matrices stored\n(note rows start row 2).","code":"\niris_sample <- iris.scaled |> \n  select(-Species) |> \n  slice(1:5)\niris_sample## # A tibble: 5 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1       -0.898      1.02          -1.34       -1.31\n## 2       -1.14      -0.132         -1.34       -1.31\n## 3       -1.38       0.327         -1.39       -1.31\n## 4       -1.50       0.0979        -1.28       -1.31\n## 5       -1.02       1.25          -1.34       -1.31\ndist(iris_sample, method = \"euclidean\")##       1     2     3     4\n## 2 1.172                  \n## 3 0.843 0.522            \n## 4 1.100 0.433 0.283      \n## 5 0.259 1.382 0.988 1.246\ndist(iris_sample, method = \"manhattan\")##       1     2     3     4\n## 2 1.389                  \n## 3 1.228 0.757            \n## 4 1.578 0.648 0.463      \n## 5 0.350 1.497 1.337 1.687\ndist(iris_sample, method = \"maximum\")##       1     2     3     4\n## 2 1.147                  \n## 3 0.688 0.459            \n## 4 0.918 0.362 0.229      \n## 5 0.229 1.377 0.918 1.147"},{"path":"data.html","id":"distances-for-binary-data","chapter":"2 Data","heading":"2.8.2 Distances for Binary Data","text":"Binary data can encodes 0 1 (numeric) TRUE \nFALSE (logical).","code":"\nb <- rbind(\n  c(0,0,0,1,1,1,1,0,0,1),\n  c(0,0,1,1,1,0,0,1,0,0)\n  )\nb##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n## [1,]    0    0    0    1    1    1    1    0    0\n## [2,]    0    0    1    1    1    0    0    1    0\n##      [,10]\n## [1,]     1\n## [2,]     0\nb_logical <- apply(b, MARGIN = 2, as.logical)\nb_logical##       [,1]  [,2]  [,3] [,4] [,5]  [,6]  [,7]  [,8]\n## [1,] FALSE FALSE FALSE TRUE TRUE  TRUE  TRUE FALSE\n## [2,] FALSE FALSE  TRUE TRUE TRUE FALSE FALSE  TRUE\n##       [,9] [,10]\n## [1,] FALSE  TRUE\n## [2,] FALSE FALSE"},{"path":"data.html","id":"hamming-distance","chapter":"2 Data","heading":"2.8.2.1 Hamming Distance","text":"Hamming distance\nnumber mismatches two binary vectors. 0-1 data\nequivalent Manhattan distance also squared\nEuclidean distance.","code":"\ndist(b, method = \"manhattan\")##   1\n## 2 5\ndist(b, method = \"euclidean\")^2##   1\n## 2 5"},{"path":"data.html","id":"jaccard-index","chapter":"2 Data","heading":"2.8.2.2 Jaccard Index","text":"Jaccard index \nsimilarity measure focuses matching 1s. R converts \nsimilarity dissimilarity using \\(d_{J} = 1 - s_{J}\\).","code":"\ndist(b, method = \"binary\")##       1\n## 2 0.714"},{"path":"data.html","id":"distances-for-mixed-data","chapter":"2 Data","heading":"2.8.3 Distances for Mixed Data","text":"distance measures work numeric data. Often, \nmixture numbers nominal ordinal features like data:important nominal features stored factors \ncharacter (<chr>).","code":"\npeople <- tibble(\n  height = c(      160,    185,    170),\n  weight = c(       52,     90,     75),\n  sex    = c( \"female\", \"male\", \"male\")\n)\npeople## # A tibble: 3 × 3\n##   height weight sex   \n##    <dbl>  <dbl> <chr> \n## 1    160     52 female\n## 2    185     90 male  \n## 3    170     75 male\npeople <- people |> \n  mutate(across(where(is.character), factor))\npeople## # A tibble: 3 × 3\n##   height weight sex   \n##    <dbl>  <dbl> <fct> \n## 1    160     52 female\n## 2    185     90 male  \n## 3    170     75 male"},{"path":"data.html","id":"gowers-coefficient","chapter":"2 Data","heading":"2.8.3.1 Gower’s Coefficient","text":"Gower’s coefficient similarity works mixed data \ncalculating appropriate similarity feature \naggregating single measure. package proxy implements\nGower’s coefficient converted distance.Gower’s coefficient calculation implicitly scales data \ncalculates distances feature individually, need\nscale data first.","code":"\nlibrary(proxy)## \n## Attaching package: 'proxy'## The following object is masked from 'package:Matrix':\n## \n##     as.matrix## The following objects are masked from 'package:stats':\n## \n##     as.dist, dist## The following object is masked from 'package:base':\n## \n##     as.matrix\nd_Gower <- dist(people, method = \"Gower\")\nd_Gower##       1     2\n## 2 1.000      \n## 3 0.668 0.332"},{"path":"data.html","id":"using-euclidean-distance-with-mixed-data","chapter":"2 Data","heading":"2.8.3.2 Using Euclidean Distance with Mixed Data","text":"Sometimes methods (e.g., k-means) can use Euclidean distance. \ncase, nominal features can converted 0-1 dummy variables.\nscaling, Euclidean distance result usable distance\nmeasure.use package caret create dummy variables.Note feature sex now two columns. want height,\nweight sex influence distance measure, \nneed weight sex columns 1/2 scaling.distance using dummy variables consistent Gower’s distance.\nHowever, note Gower’s distance scaled 0 1 \nEuclidean distance .","code":"\nlibrary(caret)## Loading required package: lattice## \n## Attaching package: 'caret'## The following object is masked from 'package:sampling':\n## \n##     cluster## The following object is masked from 'package:purrr':\n## \n##     lift\ndata_dummy <- dummyVars(~., people) |> \n  predict(people)\ndata_dummy##   height weight sex.female sex.male\n## 1    160     52          1        0\n## 2    185     90          0        1\n## 3    170     75          0        1\nweight_matrix <- matrix(c(1, 1, 1/2, 1/2), ncol = 4, nrow = nrow(data_dummy), byrow = TRUE)\ndata_dummy_scaled <- scale(data_dummy) * weight_matrix\n\nd_dummy <- dist(data_dummy_scaled)\nd_dummy##      1    2\n## 2 3.06     \n## 3 1.89 1.43\nggplot(tibble(d_dummy, d_Gower), aes(x = d_dummy, y = d_Gower)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)## Don't know how to automatically pick scale for object of type <dist>. Defaulting to\n## continuous.\n## Don't know how to automatically pick scale for object of type <dist>. Defaulting to\n## continuous.\n## `geom_smooth()` using formula = 'y ~ x'"},{"path":"data.html","id":"additional-proximity-measures-available-in-package-proxy","chapter":"2 Data","heading":"2.8.4 Additional proximity Measures Available in Package proxy","text":"package proxy implements wide array distances.Note loading package proxy replaces dist function R.\ncan specify dist function use specifying package \ncall. example stats::dist() calls default function R\n(package stats part R) proxy::dist() calls \nversion package proxy.","code":"\nlibrary(proxy)\npr_DB$get_entry_names()##  [1] \"Jaccard\"         \"Kulczynski1\"    \n##  [3] \"Kulczynski2\"     \"Mountford\"      \n##  [5] \"Fager\"           \"Russel\"         \n##  [7] \"simple matching\" \"Hamman\"         \n##  [9] \"Faith\"           \"Tanimoto\"       \n## [11] \"Dice\"            \"Phi\"            \n## [13] \"Stiles\"          \"Michael\"        \n## [15] \"Mozley\"          \"Yule\"           \n## [17] \"Yule2\"           \"Ochiai\"         \n## [19] \"Simpson\"         \"Braun-Blanquet\" \n## [21] \"cosine\"          \"angular\"        \n## [23] \"eJaccard\"        \"eDice\"          \n## [25] \"correlation\"     \"Chi-squared\"    \n## [27] \"Phi-squared\"     \"Tschuprow\"      \n## [29] \"Cramer\"          \"Pearson\"        \n## [31] \"Gower\"           \"Euclidean\"      \n## [33] \"Mahalanobis\"     \"Bhjattacharyya\" \n## [35] \"Manhattan\"       \"supremum\"       \n## [37] \"Minkowski\"       \"Canberra\"       \n## [39] \"Wave\"            \"divergence\"     \n## [41] \"Kullback\"        \"Bray\"           \n## [43] \"Soergel\"         \"Levenshtein\"    \n## [45] \"Podani\"          \"Chord\"          \n## [47] \"Geodesic\"        \"Whittaker\"      \n## [49] \"Hellinger\"       \"fJaccard\""},{"path":"data.html","id":"relationships-between-features","chapter":"2 Data","heading":"2.9 Relationships Between Features","text":"","code":""},{"path":"data.html","id":"correlation","chapter":"2 Data","heading":"2.9.1 Correlation","text":"Correlation can used ratio/interval scaled features. typically\nthink Pearson correlation\ncoefficient\nfeatures (columns).cor calculates correlation matrix pairwise correlations \nfeatures. Correlation matrices symmetric, different \ndistances, whole matrix stored.correlation Petal.Length Petal.Width can visualized\nusing scatter plot.geom_smooth adds regression line fitting linear model (lm).\npoints close line indicating strong linear dependence\n(.e., high correlation).can calculate individual correlations specifying two vectors.Note: lets use columns using just names \n(iris, cor(Petal.Length, Petal.Width)) \ncor(iris$Petal.Length, iris$Petal.Width).Finally, can test correlation significantly different \nzero.small p-value (less 0.05) indicates observed correlation\nsignificantly different zero. can also seen fact\n95% confidence interval span zero.Sepal.Length Sepal.Width show little correlation:","code":"\ncc <- iris |> \n  select(-Species) |> \n  cor()\ncc##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.118        0.872\n## Sepal.Width        -0.118       1.000       -0.428\n## Petal.Length        0.872      -0.428        1.000\n## Petal.Width         0.818      -0.366        0.963\n##              Petal.Width\n## Sepal.Length       0.818\n## Sepal.Width       -0.366\n## Petal.Length       0.963\n## Petal.Width        1.000\nggplot(iris, aes(Petal.Length, Petal.Width)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")## `geom_smooth()` using formula = 'y ~ x'\nwith(iris, cor(Petal.Length, Petal.Width))## [1] 0.963\nwith(iris, cor.test(Petal.Length, Petal.Width))## \n##  Pearson's product-moment correlation\n## \n## data:  Petal.Length and Petal.Width\n## t = 43, df = 148, p-value <2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.949 0.973\n## sample estimates:\n##   cor \n## 0.963\nggplot(iris, aes(Sepal.Length, Sepal.Width)) + \n  geom_point() +   \n  geom_smooth(method = \"lm\") ## `geom_smooth()` using formula = 'y ~ x'\nwith(iris, cor(Sepal.Length, Sepal.Width)) ## [1] -0.118\nwith(iris, cor.test(Sepal.Length, Sepal.Width))## \n##  Pearson's product-moment correlation\n## \n## data:  Sepal.Length and Sepal.Width\n## t = -1, df = 148, p-value = 0.2\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.2727  0.0435\n## sample estimates:\n##    cor \n## -0.118"},{"path":"data.html","id":"rank-correlation","chapter":"2 Data","heading":"2.9.2 Rank Correlation","text":"Rank correlation used ordinal features correlation \nlinear. show , first convert continuous features \nIris dataset ordered factors (ordinal) three levels using\nfunction cut.Two measures rank correlation Kendall’s Tau Spearman’s Rho.Kendall’s Tau Rank Correlation\nCoefficient\nmeasures agreement two rankings (.e., ordinal features).Note: use xtfrm transform ordered factors \nranks, .e., numbers representing order.Spearman’s\nRho\nequal Pearson correlation rank values two\nfeatures.Spearman’s Rho much faster compute large datasets \nKendall’s Tau.Comparing rank correlation results Pearson correlation \noriginal data shows similar. indicates \ndiscretizing data result loss much information.","code":"\niris_ord <- iris |> \n  mutate(across(where(is.numeric), \n  ~ cut(., 3, labels = c(\"short\", \"medium\", \"long\"), ordered = TRUE)))\n\niris_ord## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##    <ord>        <ord>       <ord>        <ord>      \n##  1 short        medium      short        short      \n##  2 short        medium      short        short      \n##  3 short        medium      short        short      \n##  4 short        medium      short        short      \n##  5 short        medium      short        short      \n##  6 short        long        short        short      \n##  7 short        medium      short        short      \n##  8 short        medium      short        short      \n##  9 short        medium      short        short      \n## 10 short        medium      short        short      \n## # ℹ 140 more rows\n## # ℹ 1 more variable: Species <fct>\nsummary(iris_ord)##  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##  short :59    short :47   short :50    short :50  \n##  medium:71    medium:88   medium:54    medium:54  \n##  long  :20    long  :15   long  :46    long  :46  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50\niris_ord |> \n  pull(Sepal.Length)##   [1] short  short  short  short  short  short  short \n##   [8] short  short  short  short  short  short  short \n##  [15] medium medium short  short  medium short  short \n##  [22] short  short  short  short  short  short  short \n##  [29] short  short  short  short  short  short  short \n##  [36] short  short  short  short  short  short  short \n##  [43] short  short  short  short  short  short  short \n##  [50] short  long   medium long   short  medium medium\n##  [57] medium short  medium short  short  medium medium\n##  [64] medium medium medium medium medium medium medium\n##  [71] medium medium medium medium medium medium long  \n##  [78] medium medium medium short  short  medium medium\n##  [85] short  medium medium medium medium short  short \n##  [92] medium medium short  medium medium medium medium\n##  [99] short  medium medium medium long   medium medium\n## [106] long   short  long   medium long   medium medium\n## [113] long   medium medium medium medium long   long  \n## [120] medium long   medium long   medium medium long  \n## [127] medium medium medium long   long   long   medium\n## [134] medium medium long   medium medium medium long  \n## [141] medium long   medium long   medium medium medium\n## [148] medium medium medium\n## Levels: short < medium < long\niris_ord |> \n  select(-Species) |> \n  sapply(xtfrm) |> \n  cor(method = \"kendall\")##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.144        0.742\n## Sepal.Width        -0.144       1.000       -0.330\n## Petal.Length        0.742      -0.330        1.000\n## Petal.Width         0.730      -0.315        0.920\n##              Petal.Width\n## Sepal.Length       0.730\n## Sepal.Width       -0.315\n## Petal.Length       0.920\n## Petal.Width        1.000\niris_ord |> \n  select(-Species) |> \n  sapply(xtfrm) |> \n  cor(method = \"spearman\")##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.157        0.794\n## Sepal.Width        -0.157       1.000       -0.366\n## Petal.Length        0.794      -0.366        1.000\n## Petal.Width         0.784      -0.352        0.940\n##              Petal.Width\n## Sepal.Length       0.784\n## Sepal.Width       -0.352\n## Petal.Length       0.940\n## Petal.Width        1.000\niris |> \n  select(-Species) |> \n  cor()##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.118        0.872\n## Sepal.Width        -0.118       1.000       -0.428\n## Petal.Length        0.872      -0.428        1.000\n## Petal.Width         0.818      -0.366        0.963\n##              Petal.Width\n## Sepal.Length       0.818\n## Sepal.Width       -0.366\n## Petal.Length       0.963\n## Petal.Width        1.000"},{"path":"data.html","id":"density-estimation","chapter":"2 Data","heading":"2.10 Density Estimation","text":"Density estimation\nconstructions estimate probability density function\n(distribution) continuous variable based observed data.Just plotting data using points helpful single\nfeature.","code":"\nggplot(iris, aes(x = Petal.Length, y = 0)) + geom_point()"},{"path":"data.html","id":"histograms","chapter":"2 Data","heading":"2.10.1 Histograms","text":"histograms shows \ndistribution counting many values fall within bin \nvisualizing counts bar chart. use geom_rug place marks\noriginal data points bottom histogram.Two-dimensional distributions can visualized using 2-d binning \nhexagonal bins.","code":"\nggplot(iris, aes(x = Petal.Length)) +\n  geom_histogram() +\n  geom_rug(alpha = 1/2)## `stat_bin()` using `bins = 30`. Pick better value with\n## `binwidth`.\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_bin2d(bins = 10) +\n  geom_jitter(color = \"red\")\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_hex(bins = 10) +\n  geom_jitter(color = \"red\")## Warning: Computation failed in `stat_binhex()`\n## Caused by error in `compute_group()`:\n## ! The package \"hexbin\" is required for `stat_binhex()`"},{"path":"data.html","id":"kernel-density-estimate-kde","chapter":"2 Data","heading":"2.10.2 Kernel Density Estimate (KDE)","text":"Kernel density\nestimation \nused estimate probability density function (distribution) \nfeature. works replacing value kernel function (often\nGaussian) adding . result estimated\nprobability density function looks like smoothed version \nhistogram. bandwidth (bw) kernel controls amount \nsmoothing.Kernel density estimates can also done two dimensions.","code":"\nggplot(iris, aes(Petal.Length)) +\n  geom_density(bw = .2) +\n  geom_rug(alpha = 1/2)\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_density_2d_filled() +\n  geom_jitter()"},{"path":"data.html","id":"exploring-data","chapter":"2 Data","heading":"2.11 Exploring Data","text":"","code":""},{"path":"data.html","id":"basic-statistics","chapter":"2 Data","heading":"2.11.1 Basic statistics","text":"Get summary statistics (using base R)Get mean standard deviation sepal lengthData missing values result statistics NA. Adding \nparameter na.rm = TRUE can used statistics functions \nignore missing values.Outliers typically smallest largest values feature.\nmake mean robust outliers, can trim 10% \nobservations end distribution.Sepal length outliers, trimmed mean almost\nidentical.calculate summary set features (e.g., numeric\nfeatures), tidyverse provides across((.numeric), fun).median absolute deviation (MAD) another measure dispersion.","code":"\nsummary(iris)##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.35  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width        Species  \n##  Min.   :0.1   setosa    :50  \n##  1st Qu.:0.3   versicolor:50  \n##  Median :1.3   virginica :50  \n##  Mean   :1.2                  \n##  3rd Qu.:1.8                  \n##  Max.   :2.5\niris |> \n  summarize(avg_Sepal.Length = mean(Sepal.Length), sd_Sepal.Length = sd(Sepal.Length))## # A tibble: 1 × 2\n##   avg_Sepal.Length sd_Sepal.Length\n##              <dbl>           <dbl>\n## 1             5.84           0.828\nmean(c(1, 2, NA, 3, 4, 5))## [1] NA\nmean(c(1, 2, NA, 3, 4, 5),  na.rm = TRUE)## [1] 3\niris |>\n  summarize(avg_Sepal.Length = mean(Sepal.Length),\n            trimmed_avg_Sepal.Length = mean(Sepal.Length, trim = .1))## # A tibble: 1 × 2\n##   avg_Sepal.Length trimmed_avg_Sepal.Length\n##              <dbl>                    <dbl>\n## 1             5.84                     5.81\niris |> summarize(across(where(is.numeric), mean))## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         5.84        3.06         3.76        1.20\niris |> summarize(across(where(is.numeric), sd))## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1        0.828       0.436         1.77       0.762\niris |> summarize(across(where(is.numeric), \n            list(min = min, median = median, max = max)))## # A tibble: 1 × 12\n##   Sepal.Length_min Sepal.Length_median Sepal.Length_max\n##              <dbl>               <dbl>            <dbl>\n## 1              4.3                 5.8              7.9\n## # ℹ 9 more variables: Sepal.Width_min <dbl>,\n## #   Sepal.Width_median <dbl>, Sepal.Width_max <dbl>,\n## #   Petal.Length_min <dbl>, Petal.Length_median <dbl>,\n## #   Petal.Length_max <dbl>, Petal.Width_min <dbl>,\n## #   Petal.Width_median <dbl>, Petal.Width_max <dbl>\niris |> summarize(across(where(is.numeric), mad))## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         1.04       0.445         1.85        1.04"},{"path":"data.html","id":"grouping","chapter":"2 Data","heading":"2.11.2 Grouping","text":"can use nominal feature form groups calculate\ngroup-wise statistics continuous features. often use\ngroup-wise averages see differ groups.see species Virginica highest average , \nSepal.Width.statistical difference groups can tested using ANOVA\n(analysis \nvariance).summary shows significant difference \nSepal.Length groups. TukeyHDS evaluates differences\npairs groups. case, significantly different.\ndata contains two groups, t.test can used.","code":"\niris |> \n  group_by(Species) |> \n  summarize(across(Sepal.Length, mean))## # A tibble: 3 × 2\n##   Species    Sepal.Length\n##   <fct>             <dbl>\n## 1 setosa             5.01\n## 2 versicolor         5.94\n## 3 virginica          6.59\niris |> \n  group_by(Species) |> \n  summarize(across(where(is.numeric), mean))## # A tibble: 3 × 5\n##   Species    Sepal.Length Sepal.Width Petal.Length\n##   <fct>             <dbl>       <dbl>        <dbl>\n## 1 setosa             5.01        3.43         1.46\n## 2 versicolor         5.94        2.77         4.26\n## 3 virginica          6.59        2.97         5.55\n## # ℹ 1 more variable: Petal.Width <dbl>\nres.aov <- aov(Sepal.Length ~ Species, data = iris)\nsummary(res.aov)##              Df Sum Sq Mean Sq F value Pr(>F)    \n## Species       2   63.2   31.61     119 <2e-16 ***\n## Residuals   147   39.0    0.27                   \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(res.aov)##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = Sepal.Length ~ Species, data = iris)\n## \n## $Species\n##                       diff   lwr   upr p adj\n## versicolor-setosa    0.930 0.686 1.174     0\n## virginica-setosa     1.582 1.338 1.826     0\n## virginica-versicolor 0.652 0.408 0.896     0"},{"path":"data.html","id":"tabulate-data","chapter":"2 Data","heading":"2.11.3 Tabulate data","text":"can count number flowers species.base R, can also done using count(iris$Species).following examples, discretize data using cut.Cross tabulation used find two discrete features \nrelated.table contains number rows contain combination \nvalues (e.g., number flowers short Sepal.Length \nspecies Setosa 47). cells large counts\nothers low counts, might \nrelationship. iris data, see species Setosa mostly \nshort Sepal.Length, Versicolor Virginica longer sepals.Creating cross table tidyverse little involved uses\npivot operations grouping.can use statistical test determine significant\nrelationship two features. Pearson’s chi-squared\ntest independence\nperformed null hypothesis joint distribution \ncell counts 2-dimensional contingency table product \nrow column marginals. null hypothesis h0 independence \nrows columns.small p-value indicates null hypothesis independence\nneeds rejected. small counts (cells counts <5),\nFisher’s exact\ntest better.","code":"\niris |> \n  group_by(Species) |> \n  summarize(n())## # A tibble: 3 × 2\n##   Species    `n()`\n##   <fct>      <int>\n## 1 setosa        50\n## 2 versicolor    50\n## 3 virginica     50\niris_ord <- iris |> mutate(across(where(is.numeric),  \n  ~ cut(., 3, labels = c(\"short\", \"medium\", \"long\"), ordered = TRUE)))\n\niris_ord## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##    <ord>        <ord>       <ord>        <ord>      \n##  1 short        medium      short        short      \n##  2 short        medium      short        short      \n##  3 short        medium      short        short      \n##  4 short        medium      short        short      \n##  5 short        medium      short        short      \n##  6 short        long        short        short      \n##  7 short        medium      short        short      \n##  8 short        medium      short        short      \n##  9 short        medium      short        short      \n## 10 short        medium      short        short      \n## # ℹ 140 more rows\n## # ℹ 1 more variable: Species <fct>\nsummary(iris_ord)##  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##  short :59    short :47   short :50    short :50  \n##  medium:71    medium:88   medium:54    medium:54  \n##  long  :20    long  :15   long  :46    long  :46  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50\ntbl <- iris_ord |> \n  select(Sepal.Length, Species) |> \n  table()\ntbl##             Species\n## Sepal.Length setosa versicolor virginica\n##       short      47         11         1\n##       medium      3         36        32\n##       long        0          3        17\niris_ord |>\n  select(Species, Sepal.Length) |>\n### Relationship Between Nominal and Ordinal Features\n  pivot_longer(cols = Sepal.Length) |>\n  group_by(Species, value) |> \n  count() |> \n  ungroup() |>\n  pivot_wider(names_from = Species, values_from = n)## # A tibble: 3 × 4\n##   value  setosa versicolor virginica\n##   <ord>   <int>      <int>     <int>\n## 1 short      47         11         1\n## 2 medium      3         36        32\n## 3 long       NA          3        17\ntbl |> \n  chisq.test()## \n##  Pearson's Chi-squared test\n## \n## data:  tbl\n## X-squared = 112, df = 4, p-value <2e-16\nfisher.test(tbl)## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tbl\n## p-value <2e-16\n## alternative hypothesis: two.sided"},{"path":"data.html","id":"percentiles-quantiles","chapter":"2 Data","heading":"2.11.4 Percentiles (Quantiles)","text":"Quantiles cutting points\ndividing range probability distribution continuous\nintervals equal probability. example, median \nempirical 50% quantile dividing observations 50% \nobservations smaller median 50% \nlarger median.default quartiles calculated. 25% typically called Q1, 50% \ncalled Q2 median 75% called Q3.interquartile range measure variability robust\noutliers. defined length Q3 - Q2 covers 50%\ndata middle.","code":"\niris |> \n  pull(Petal.Length) |> \n  quantile()##   0%  25%  50%  75% 100% \n## 1.00 1.60 4.35 5.10 6.90\niris |> \n  summarize(IQR = \n  quantile(Petal.Length, probs = 0.75) - quantile(Petal.Length, probs = 0.25))## # A tibble: 1 × 1\n##     IQR\n##   <dbl>\n## 1   3.5"},{"path":"data.html","id":"visualization","chapter":"2 Data","heading":"2.12 Visualization","text":"","code":""},{"path":"data.html","id":"histogram","chapter":"2 Data","heading":"2.12.1 Histogram","text":"Histograms show distribution single continuous feature.","code":"\nggplot(iris, aes(Petal.Width)) + geom_histogram(bins = 20)"},{"path":"data.html","id":"boxplot","chapter":"2 Data","heading":"2.12.2 Boxplot","text":"Boxplots used compare distribution feature \ndifferent groups. horizontal line middle boxes \ngroup-wise medians, boxes span interquartile range. whiskers\n(vertical lines) span typically 1.4 times interquartile range.\nPoints fall outside range typically outliers shown \ndots.group-wise medians can also calculated directly.compare distribution four features using ggplot boxplot,\nfirst transform data long format (.e., feature\nvalues combined single column).visualization useful features roughly \nrange. data can scaled first compare distributions.","code":"\nggplot(iris, aes(Species, Sepal.Length)) + \n  geom_boxplot()\niris |> group_by(Species) |> \n  summarize(across(where(is.numeric), median))## # A tibble: 3 × 5\n##   Species    Sepal.Length Sepal.Width Petal.Length\n##   <fct>             <dbl>       <dbl>        <dbl>\n## 1 setosa              5           3.4         1.5 \n## 2 versicolor          5.9         2.8         4.35\n## 3 virginica           6.5         3           5.55\n## # ℹ 1 more variable: Petal.Width <dbl>\nlibrary(tidyr)\niris_long <- iris |> \n  mutate(id = row_number()) |> \n  pivot_longer(1:4)\n\nggplot(iris_long, aes(name, value)) + \n  geom_boxplot() +\n  labs(y = \"Original value\")\nlibrary(tidyr)\niris_long_scaled <- iris |> \n  scale_numeric() |> \n  mutate(id = row_number()) |> pivot_longer(1:4)\n\nggplot(iris_long_scaled, aes(name, value)) + \n  geom_boxplot() +\n  labs(y = \"Scaled value\")"},{"path":"data.html","id":"scatter-plot","chapter":"2 Data","heading":"2.12.3 Scatter plot","text":"Scatter plots show relationship two continuous features.","code":"\nggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) + \n  geom_point()"},{"path":"data.html","id":"scatter-plot-matrix","chapter":"2 Data","heading":"2.12.4 Scatter Plot Matrix","text":"scatter plot matrix show relationship several featuresThe implementation package GGally also shows additional plots\n(histograms, density estimates box plots) correlation\ncoefficients.","code":"\nlibrary(\"GGally\")\nggpairs(iris,  aes(color = Species), progress = FALSE)## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"data.html","id":"data-matrix-visualization","chapter":"2 Data","heading":"2.12.5 Data Matrix Visualization","text":"Matrix visualization shows values matrix using color scale.need long format tidyverse.Smaller values darker. Package seriation provides simpler\nplotting function.can scale features z-scores make better comparable.reveals red blue blocks. row flower flowers\nIris dataset sorted species. blue blocks top\n50 flowers show flowers smaller average \nSepal.Width red blocks show bottom 50 flowers \nlarger features.Often, reordering data matrices help visualization. reordering\ntechnique called seriation. Ir reorders rows columns place\nsimilar points closer together.see rows (flowers) organized blue red\nfeatures reordered move Sepal.Width way \nright different features.","code":"\niris_matrix <- iris |> select(-Species) |> as.matrix()\niris_long <- as_tibble(iris_matrix) |> \n  mutate(id = row_number()) |> \n  pivot_longer(1:4)\n\nhead(iris_long)## # A tibble: 6 × 3\n##      id name         value\n##   <int> <chr>        <dbl>\n## 1     1 Sepal.Length   5.1\n## 2     1 Sepal.Width    3.5\n## 3     1 Petal.Length   1.4\n## 4     1 Petal.Width    0.2\n## 5     2 Sepal.Length   4.9\n## 6     2 Sepal.Width    3\nggplot(iris_long,\n  aes(x = name, y = id, fill = value)) + geom_tile()\nlibrary(seriation)## Registered S3 methods overwritten by 'registry':\n##   method               from \n##   print.registry_field proxy\n##   print.registry_entry proxy## \n## Attaching package: 'seriation'## The following object is masked from 'package:lattice':\n## \n##     panel.lines\nggpimage(iris_matrix, prop = FALSE)\niris_scaled <- scale(iris_matrix)\nggpimage(iris_scaled, prop = FALSE)\nggpimage(iris_scaled, order = seriate(iris_scaled), prop = FALSE)"},{"path":"data.html","id":"correlation-matrix","chapter":"2 Data","heading":"2.12.6 Correlation Matrix","text":"correlation matrix contains correlation features.Package ggcorrplot provides visualization correlation matrices.Package seriation provides reordered version plot using \nheatmap.Correlations can also calculates objects transposing \ndata matrix.Object--object correlations can used measure similarity.\ndark red blocks indicate different species.","code":"\ncm1 <- iris |> select(-Species) |> as.matrix() |> cor()\ncm1##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.118        0.872\n## Sepal.Width        -0.118       1.000       -0.428\n## Petal.Length        0.872      -0.428        1.000\n## Petal.Width         0.818      -0.366        0.963\n##              Petal.Width\n## Sepal.Length       0.818\n## Sepal.Width       -0.366\n## Petal.Length       0.963\n## Petal.Width        1.000\nlibrary(ggcorrplot)\nggcorrplot(cm1)\ngghmap(cm1, prop = TRUE)\ncm2 <- iris |> select(-Species) |> as.matrix() |> t() |> cor()\n\nggcorrplot(cm2)"},{"path":"data.html","id":"parallel-coordinates-plot","chapter":"2 Data","heading":"2.12.7 Parallel Coordinates Plot","text":"Parallel coordinate plots can visualize several features single\nplot. Lines connect values object (flower).plot can improved reordering variables place correlated\nfeatures next .","code":"\nlibrary(GGally)\nggparcoord(iris, columns = 1:4, groupColumn = 5)\no <- seriate(as.dist(1-cor(iris[,1:4])), method = \"BBURCG\")\nget_order(o)## Petal.Length  Petal.Width Sepal.Length  Sepal.Width \n##            3            4            1            2\nggparcoord(iris, columns = get_order(o), groupColumn = 5)"},{"path":"data.html","id":"more-visualizations","chapter":"2 Data","heading":"2.12.8 More Visualizations","text":"well organized collection visualizations code can found \nR Graph Gallery.","code":""},{"path":"classification-basic-concepts-and-techniques.html","id":"classification-basic-concepts-and-techniques","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3 Classification: Basic Concepts and Techniques","text":"Install packages used chapter:packages used chapter : caret (M. Kuhn 2023), FSelector (Romanski, Kotthoff, Schratz 2021), lattice (Sarkar 2023), mlbench (Leisch Dimitriadou. 2023), pROC (Robin et al. 2023), rpart (Therneau Atkinson 2022), rpart.plot (Milborrow 2022), sampling (Tillé Matei 2021), tidyverse (Wickham 2023b)","code":"\npkgs <- sort(c('tidyverse', 'rpart', 'rpart.plot', 'caret', \n  'lattice', 'FSelector', 'sampling', 'pROC', 'mlbench'))\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"classification-basic-concepts-and-techniques.html","id":"introduction-2","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.1 Introduction","text":"Classification machine learning task goal learn predictive\nfunction form\\[y = f(\\mathbf{x}),\\]\\(\\mathbf{x}\\) called attribute set \\(y\\) class label. attribute set\nconsists feature describe object. features can measured using scale\n(.e., nominal, interval, …). class label nominal attribute. binary\nattribute, problem called binary classification problem.Classification learns classification model training data features \ncorrect class label available. called supervised learning problem.related supervised learning problem regression,\n\\(y\\) number instead label.\nLinear regression popular supervised learning model, however, talk \nsince taught almost introductory statistics course.chapter introduce decision trees, model evaluation comparison, feature selection,\nexplore methods handle class imbalance problem.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 3. Classification: Basic Concepts \nTechniques","code":""},{"path":"classification-basic-concepts-and-techniques.html","id":"the-zoo-dataset","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.2 The Zoo Dataset","text":"demonstrate classification, use Zoo dataset included R package\nmlbench (may install ). Zoo dataset containing 17\n(mostly logical) variables 101 animals data frame \n17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator,\ntoothed, backbone, breathes, venomous, fins, legs, tail, domestic,\ncatsize, type).\nfirst 16 columns represent feature vector \\(\\mathbf{x}\\) last column\ncalled type class label \\(y\\).\nconvert data frame tidyverse tibble\n(optional).Note: data.frames R can row names. Zoo data set uses \nanimal name row names. tibbles tidyverse support\nrow names. keep animal name can add column animal\nname.remove animal column learning model! \nfollowing use data.frame.translate TRUE/FALSE values factors (nominal). \noften needed building models. Always check summary() make sure\ndata ready model learning.","code":"\ndata(Zoo, package=\"mlbench\")\nhead(Zoo)##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n## bear      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## boar      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## buffalo   TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n##          predator toothed backbone breathes venomous\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE\n## bear         TRUE    TRUE     TRUE     TRUE    FALSE\n## boar         TRUE    TRUE     TRUE     TRUE    FALSE\n## buffalo     FALSE    TRUE     TRUE     TRUE    FALSE\n##           fins legs  tail domestic catsize   type\n## aardvark FALSE    4 FALSE    FALSE    TRUE mammal\n## antelope FALSE    4  TRUE    FALSE    TRUE mammal\n## bass      TRUE    0  TRUE    FALSE   FALSE   fish\n## bear     FALSE    4 FALSE    FALSE    TRUE mammal\n## boar     FALSE    4  TRUE    FALSE    TRUE mammal\n## buffalo  FALSE    4  TRUE    FALSE    TRUE mammal\nlibrary(tidyverse)\nas_tibble(Zoo, rownames = \"animal\")## # A tibble: 101 × 18\n##    animal   hair  feathers eggs  milk  airborne aquatic\n##    <chr>    <lgl> <lgl>    <lgl> <lgl> <lgl>    <lgl>  \n##  1 aardvark TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  2 antelope TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  3 bass     FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n##  4 bear     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  5 boar     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  6 buffalo  TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  7 calf     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  8 carp     FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n##  9 catfish  FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n## 10 cavy     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n## # ℹ 91 more rows\n## # ℹ 11 more variables: predator <lgl>, toothed <lgl>,\n## #   backbone <lgl>, breathes <lgl>, venomous <lgl>,\n## #   fins <lgl>, legs <int>, tail <lgl>,\n## #   domestic <lgl>, catsize <lgl>, type <fct>\nZoo <- Zoo |>\n  mutate(across(where(is.logical), factor, levels = c(TRUE, FALSE))) |>\n  mutate(across(where(is.character), factor))## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `across(where(is.logical), factor, levels = c(TRUE, FALSE))`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\nsummary(Zoo)##     hair     feathers     eggs       milk   \n##  TRUE :43   TRUE :20   TRUE :59   TRUE :41  \n##  FALSE:58   FALSE:81   FALSE:42   FALSE:60  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##   airborne   aquatic    predator   toothed  \n##  TRUE :24   TRUE :36   TRUE :56   TRUE :61  \n##  FALSE:77   FALSE:65   FALSE:45   FALSE:40  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##   backbone   breathes   venomous     fins   \n##  TRUE :83   TRUE :80   TRUE : 8   TRUE :17  \n##  FALSE:18   FALSE:21   FALSE:93   FALSE:84  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##       legs         tail     domestic   catsize  \n##  Min.   :0.00   TRUE :75   TRUE :13   TRUE :44  \n##  1st Qu.:2.00   FALSE:26   FALSE:88   FALSE:57  \n##  Median :4.00                                   \n##  Mean   :2.84                                   \n##  3rd Qu.:4.00                                   \n##  Max.   :8.00                                   \n##                                                 \n##             type   \n##  mammal       :41  \n##  bird         :20  \n##  reptile      : 5  \n##  fish         :13  \n##  amphibian    : 4  \n##  insect       : 8  \n##  mollusc.et.al:10"},{"path":"classification-basic-concepts-and-techniques.html","id":"decision-trees","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3 Decision Trees","text":"Recursive Partitioning (similar CART) uses Gini index make\nsplitting decisions early stopping (pre-pruning).","code":"\nlibrary(rpart)"},{"path":"classification-basic-concepts-and-techniques.html","id":"create-tree-with-default-settings-uses-pre-pruning","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.1 Create Tree With Default Settings (uses pre-pruning)","text":"Notes:|> supplies data rpart. Since data \nfirst argument rpart, syntax data = _ used specify\ndata Zoo goes. call equivalent \ntree_default <- rpart(type ~ ., data = Zoo).|> supplies data rpart. Since data \nfirst argument rpart, syntax data = _ used specify\ndata Zoo goes. call equivalent \ntree_default <- rpart(type ~ ., data = Zoo).formula models \ntype variable features represented ..formula models \ntype variable features represented ..class variable needs factor (nominal) rpart create \nregression tree instead decision tree. Use .factor() \nnecessary.class variable needs factor (nominal) rpart create \nregression tree instead decision tree. Use .factor() \nnecessary.PlottingNote: extra=2 prints leaf node number correctly\nclassified objects data total number objects \ntraining data falling node (correct/total).","code":"\ntree_default <- Zoo |> \n  rpart(type ~ ., data = _)\ntree_default## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##  1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##    2) milk=TRUE 41  0 mammal (1 0 0 0 0 0 0) *\n##    3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##      6) feathers=TRUE 20  0 bird (0 1 0 0 0 0 0) *\n##      7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##       14) fins=TRUE 13  0 fish (0 0 0 1 0 0 0) *\n##       15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##         30) backbone=TRUE 9  4 reptile (0 0 0.56 0 0.44 0 0) *\n##         31) backbone=FALSE 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56) *\nlibrary(rpart.plot)\nrpart.plot(tree_default, extra = 2)"},{"path":"classification-basic-concepts-and-techniques.html","id":"create-a-full-tree","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.2 Create a Full Tree","text":"create full tree, set complexity parameter cp 0 (split\neven improve tree) set minimum number \nobservations node needed split smallest value 2 (see:\n?rpart.control). Note: full trees overfit training data!Training error tree pre-pruningUse function accuracyTraining error full treeGet confusion table statistics (using caret)","code":"\ntree_full <- Zoo |> \n  rpart(type ~ . , data = _, \n        control = rpart.control(minsplit = 2, cp = 0))\nrpart.plot(tree_full, extra = 2, \n           roundint=FALSE,\n            box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \n                               \"Or\", \"Rd\", \"Pu\")) # specify 7 colors\ntree_full## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##     2) milk=TRUE 41  0 mammal (1 0 0 0 0 0 0) *\n##     3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##       6) feathers=TRUE 20  0 bird (0 1 0 0 0 0 0) *\n##       7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##        14) fins=TRUE 13  0 fish (0 0 0 1 0 0 0) *\n##        15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##          30) backbone=TRUE 9  4 reptile (0 0 0.56 0 0.44 0 0)  \n##            60) aquatic=FALSE 4  0 reptile (0 0 1 0 0 0 0) *\n##            61) aquatic=TRUE 5  1 amphibian (0 0 0.2 0 0.8 0 0)  \n##             122) eggs=FALSE 1  0 reptile (0 0 1 0 0 0 0) *\n##             123) eggs=TRUE 4  0 amphibian (0 0 0 0 1 0 0) *\n##          31) backbone=FALSE 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56)  \n##            62) airborne=TRUE 6  0 insect (0 0 0 0 0 1 0) *\n##            63) airborne=FALSE 12  2 mollusc.et.al (0 0 0 0 0 0.17 0.83)  \n##             126) predator=FALSE 4  2 insect (0 0 0 0 0 0.5 0.5)  \n##               252) legs>=3 2  0 insect (0 0 0 0 0 1 0) *\n##               253) legs< 3 2  0 mollusc.et.al (0 0 0 0 0 0 1) *\n##             127) predator=TRUE 8  0 mollusc.et.al (0 0 0 0 0 0 1) *\npredict(tree_default, Zoo) |> head ()##          mammal bird reptile fish amphibian insect\n## aardvark      1    0       0    0         0      0\n## antelope      1    0       0    0         0      0\n## bass          0    0       0    1         0      0\n## bear          1    0       0    0         0      0\n## boar          1    0       0    0         0      0\n## buffalo       1    0       0    0         0      0\n##          mollusc.et.al\n## aardvark             0\n## antelope             0\n## bass                 0\n## bear                 0\n## boar                 0\n## buffalo              0\npred <- predict(tree_default, Zoo, type=\"class\")\nhead(pred)## aardvark antelope     bass     bear     boar  buffalo \n##   mammal   mammal     fish   mammal   mammal   mammal \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al\nconfusion_table <- with(Zoo, table(type, pred))\nconfusion_table##                pred\n## type            mammal bird reptile fish amphibian\n##   mammal            41    0       0    0         0\n##   bird               0   20       0    0         0\n##   reptile            0    0       5    0         0\n##   fish               0    0       0   13         0\n##   amphibian          0    0       4    0         0\n##   insect             0    0       0    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                pred\n## type            insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             0             8\n##   mollusc.et.al      0            10\ncorrect <- confusion_table |> diag() |> sum()\ncorrect## [1] 89\nerror <- confusion_table |> sum() - correct\nerror## [1] 12\naccuracy <- correct / (correct + error)\naccuracy## [1] 0.881\naccuracy <- function(truth, prediction) {\n    tbl <- table(truth, prediction)\n    sum(diag(tbl))/sum(tbl)\n}\n\naccuracy(Zoo |> pull(type), pred)## [1] 0.881\naccuracy(Zoo |> pull(type), \n         predict(tree_full, Zoo, type = \"class\"))## [1] 1\nlibrary(caret)\nconfusionMatrix(data = pred, \n                reference = Zoo |> pull(type))## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian\n##   mammal            41    0       0    0         0\n##   bird               0   20       0    0         0\n##   reptile            0    0       5    0         4\n##   fish               0    0       0   13         0\n##   amphibian          0    0       0    0         0\n##   insect             0    0       0    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                Reference\n## Prediction      insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             0             0\n##   mollusc.et.al      8            10\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.881         \n##                  95% CI : (0.802, 0.937)\n##     No Information Rate : 0.406         \n##     P-Value [Acc > NIR] : <2e-16        \n##                                         \n##                   Kappa : 0.843         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       1.000\n## Pos Pred Value               1.000       1.000\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.406       0.198\n## Detection Rate               0.406       0.198\n## Detection Prevalence         0.406       0.198\n## Balanced Accuracy            1.000       1.000\n##                      Class: reptile Class: fish\n## Sensitivity                  1.0000       1.000\n## Specificity                  0.9583       1.000\n## Pos Pred Value               0.5556       1.000\n## Neg Pred Value               1.0000       1.000\n## Prevalence                   0.0495       0.129\n## Detection Rate               0.0495       0.129\n## Detection Prevalence         0.0891       0.129\n## Balanced Accuracy            0.9792       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                    0.0000        0.0000\n## Specificity                    1.0000        1.0000\n## Pos Pred Value                    NaN           NaN\n## Neg Pred Value                 0.9604        0.9208\n## Prevalence                     0.0396        0.0792\n## Detection Rate                 0.0000        0.0000\n## Detection Prevalence           0.0000        0.0000\n## Balanced Accuracy              0.5000        0.5000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         0.912\n## Pos Pred Value                      0.556\n## Neg Pred Value                      1.000\n## Prevalence                          0.099\n## Detection Rate                      0.099\n## Detection Prevalence                0.178\n## Balanced Accuracy                   0.956"},{"path":"classification-basic-concepts-and-techniques.html","id":"make-predictions-for-new-data","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.3 Make Predictions for New Data","text":"Make animal: lion feathered wingsFix columns factors like training set.Make prediction using default tree","code":"\nmy_animal <- tibble(hair = TRUE, feathers = TRUE, eggs = FALSE,\n  milk = TRUE, airborne = TRUE, aquatic = FALSE, predator = TRUE,\n  toothed = TRUE, backbone = TRUE, breathes = TRUE, venomous = FALSE,\n  fins = FALSE, legs = 4, tail = TRUE, domestic = FALSE,\n  catsize = FALSE, type = NA)\nmy_animal <- my_animal |> \n  mutate(across(where(is.logical), factor, levels = c(TRUE, FALSE)))\nmy_animal## # A tibble: 1 × 17\n##   hair  feathers eggs  milk  airborne aquatic predator\n##   <fct> <fct>    <fct> <fct> <fct>    <fct>   <fct>   \n## 1 TRUE  TRUE     FALSE TRUE  TRUE     FALSE   TRUE    \n## # ℹ 10 more variables: toothed <fct>, backbone <fct>,\n## #   breathes <fct>, venomous <fct>, fins <fct>,\n## #   legs <dbl>, tail <fct>, domestic <fct>,\n## #   catsize <fct>, type <fct>\npredict(tree_default , my_animal, type = \"class\")##      1 \n## mammal \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al"},{"path":"classification-basic-concepts-and-techniques.html","id":"model-evaluation-with-caret","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.4 Model Evaluation with Caret","text":"package caret makes preparing\ntraining sets, building classification (regression) models \nevaluation easier. great cheat sheet can found\n.Cross-validation runs independent can done faster \nparallel. enable multi-core support, caret uses package\nforeach need load backend. Linux, can use\ndoMC 4 cores. Windows needs different backend like doParallel\n(see caret cheat sheet ).Set random number generator seed make results reproducible","code":"\nlibrary(caret)\n## Linux backend\n# library(doMC)\n# registerDoMC(cores = 4)\n# getDoParWorkers()\n\n## Windows backend\n# library(doParallel)\n# cl <- makeCluster(4, type=\"SOCK\")\n# registerDoParallel(cl)\nset.seed(2000)"},{"path":"classification-basic-concepts-and-techniques.html","id":"hold-out-test-data","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.4.1 Hold out Test Data","text":"Test data used model building process set aside purely\ntesting model. , partition data 80% training 20%\ntesting.","code":"\ninTrain <- createDataPartition(y = Zoo$type, p = .8, list = FALSE)\nZoo_train <- Zoo |> slice(inTrain)## Warning: Slicing with a 1-column matrix was deprecated in dplyr 1.1.0.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nZoo_test <- Zoo |> slice(-inTrain)"},{"path":"classification-basic-concepts-and-techniques.html","id":"learn-a-model-and-tune-hyperparameters-on-the-training-data","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.4.2 Learn a Model and Tune Hyperparameters on the Training Data","text":"package caret combines training validation hyperparameter\ntuning single function called train(). internally splits \ndata training validation sets thus provide \nerror estimates different hyperparameter settings. trainControl \nused choose testing performed.rpart, train tries tune cp parameter (tree complexity)\nusing accuracy chose best model. set minsplit 2 since \nmuch data. Note: Parameters used tuning (case\ncp) need set using data.frame argument tuneGrid!\nSetting control ignored.Note: Train built 10 trees using training folds \nvalue cp reported values accuracy Kappa \naverages validation folds.model using best tuning parameters using data supplied\ntrain() available fit$finalModel.caret also computes variable importance. default uses competing\nsplits (splits runners , get chosen \ntree) rpart models (see ? varImp). Toothed runner \nmany splits, never gets chosen!variable importance without competing splits.Note: models provide variable importance function. \ncase caret might calculate variable importance ignore\nmodel (see ? varImp)!","code":"\nfit <- Zoo_train |>\n  train(type ~ .,\n    data = _ ,\n    method = \"rpart\",\n    control = rpart.control(minsplit = 2),\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 5)\n\nfit## CART \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 77, 74, 75, 73, 74, 76, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa\n##   0.00  0.938     0.919\n##   0.08  0.897     0.868\n##   0.16  0.745     0.664\n##   0.22  0.666     0.554\n##   0.32  0.474     0.190\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was cp = 0.\nrpart.plot(fit$finalModel, extra = 2,\n  box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \"Or\", \"Rd\", \"Pu\"))\nvarImp(fit)## rpart variable importance\n## \n##               Overall\n## toothedFALSE   100.00\n## feathersFALSE   69.81\n## backboneFALSE   63.08\n## milkFALSE       55.56\n## eggsFALSE       53.61\n## hairFALSE       50.52\n## finsFALSE       46.98\n## tailFALSE       28.45\n## breathesFALSE   28.13\n## airborneFALSE   26.27\n## legs            25.86\n## aquaticFALSE     5.96\n## predatorFALSE    2.35\n## venomousFALSE    1.39\n## catsizeFALSE     0.00\n## domesticFALSE    0.00\nimp <- varImp(fit, compete = FALSE)\nimp## rpart variable importance\n## \n##               Overall\n## milkFALSE      100.00\n## feathersFALSE   55.69\n## finsFALSE       39.45\n## toothedFALSE    22.96\n## airborneFALSE   22.48\n## aquaticFALSE     9.99\n## eggsFALSE        6.66\n## legs             5.55\n## predatorFALSE    1.85\n## domesticFALSE    0.00\n## breathesFALSE    0.00\n## catsizeFALSE     0.00\n## tailFALSE        0.00\n## hairFALSE        0.00\n## backboneFALSE    0.00\n## venomousFALSE    0.00\nggplot(imp)"},{"path":"classification-basic-concepts-and-techniques.html","id":"testing-confusion-matrix-and-confidence-interval-for-accuracy","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.5 Testing: Confusion Matrix and Confidence Interval for Accuracy","text":"Use best model test dataCaret’s confusionMatrix() function calculates accuracy, confidence\nintervals, kappa many evaluation metrics. need use\nseparate test data create confusion matrix based \ngeneralization error.notesMany classification algorithms train caret deal well\nmissing values. classification model can deal \nmissing values (e.g., rpart) use na.action = na.pass \ncall train predict. Otherwise, need remove\nobservations missing values na.omit use imputation \nreplace missing values train model. Make sure\nstill enough observations left.Make sure nominal variables (includes logical variables)\ncoded factors.class variable train caret level names \nkeywords R (e.g., TRUE FALSE). Rename , \nexample, “yes” “.”Make sure nominal variables (factors) examples \npossible values. methods might problems variable\nvalues without examples. can drop empty levels using\ndroplevels factor.Sampling train might create sample contain\nexamples values nominal (factor) variable. get\nerror message. likely happens variables \none rare value. may remove variable.","code":"\npred <- predict(fit, newdata = Zoo_test)\npred##  [1] mammal        mammal        mollusc.et.al\n##  [4] insect        mammal        mammal       \n##  [7] mammal        bird          mammal       \n## [10] mammal        bird          fish         \n## [13] fish          mammal        mollusc.et.al\n## [16] bird          insect        bird         \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al\nconfusionMatrix(data = pred, \n                ref = Zoo_test |> pull(type))## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian\n##   mammal             8    0       0    0         0\n##   bird               0    4       0    0         0\n##   reptile            0    0       0    0         0\n##   fish               0    0       0    2         0\n##   amphibian          0    0       0    0         0\n##   insect             0    0       1    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                Reference\n## Prediction      insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             1             0\n##   mollusc.et.al      0             2\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.944         \n##                  95% CI : (0.727, 0.999)\n##     No Information Rate : 0.444         \n##     P-Value [Acc > NIR] : 1.08e-05      \n##                                         \n##                   Kappa : 0.923         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       1.000\n## Pos Pred Value               1.000       1.000\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.444       0.222\n## Detection Rate               0.444       0.222\n## Detection Prevalence         0.444       0.222\n## Balanced Accuracy            1.000       1.000\n##                      Class: reptile Class: fish\n## Sensitivity                  0.0000       1.000\n## Specificity                  1.0000       1.000\n## Pos Pred Value                  NaN       1.000\n## Neg Pred Value               0.9444       1.000\n## Prevalence                   0.0556       0.111\n## Detection Rate               0.0000       0.111\n## Detection Prevalence         0.0000       0.111\n## Balanced Accuracy            0.5000       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                        NA        1.0000\n## Specificity                         1        0.9412\n## Pos Pred Value                     NA        0.5000\n## Neg Pred Value                     NA        1.0000\n## Prevalence                          0        0.0556\n## Detection Rate                      0        0.0556\n## Detection Prevalence                0        0.1111\n## Balanced Accuracy                  NA        0.9706\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-basic-concepts-and-techniques.html","id":"model-comparison","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.6 Model Comparison","text":"compare decision trees k-nearest neighbors (kNN)\nclassifier. create fixed sampling scheme (10-folds) \ncompare different models using exactly folds. \nspecified trControl training.Build modelsNote: kNN ask train scale data using\npreProcess = \"scale\". Logicals used 0-1 variables \nEuclidean distance calculation.Compare accuracy folds.caret provides visualizations using package lattice. \nexample, boxplot compare accuracy kappa distribution (\n10 folds).see kNN performing consistently better folds CART\n(except outlier folds).Find one models statistically better (\ndifference accuracy zero).p-values tells probability seeing even extreme value\n(difference accuracy) given null hypothesis (difference\n= 0) true. better classifier, p-value less \n.05 0.01. diff automatically applies Bonferroni correction \nmultiple comparisons. case, kNN seems better classifiers\nperform statistically differently.","code":"\ntrain_index <- createFolds(Zoo_train$type, k = 10)\nrpartFit <- Zoo_train |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        tuneLength = 10,\n        trControl = trainControl(method = \"cv\", indexOut = train_index)\n  )\nknnFit <- Zoo_train |> \n  train(type ~ .,\n        data = _,\n        method = \"knn\",\n        preProcess = \"scale\",\n          tuneLength = 10,\n          trControl = trainControl(method = \"cv\", indexOut = train_index)\n  )\nresamps <- resamples(list(\n        CART = rpartFit,\n        kNearestNeighbors = knnFit\n        ))\n\nsummary(resamps)## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: CART, kNearestNeighbors \n## Number of resamples: 10 \n## \n## Accuracy \n##                    Min. 1st Qu. Median  Mean 3rd Qu.\n## CART              0.667   0.875  0.889 0.872   0.889\n## kNearestNeighbors 0.875   0.917  1.000 0.965   1.000\n##                   Max. NA's\n## CART                 1    0\n## kNearestNeighbors    1    0\n## \n## Kappa \n##                    Min. 1st Qu. Median  Mean 3rd Qu.\n## CART              0.591   0.833  0.847 0.834   0.857\n## kNearestNeighbors 0.833   0.898  1.000 0.955   1.000\n##                   Max. NA's\n## CART                 1    0\n## kNearestNeighbors    1    0\nlibrary(lattice)\nbwplot(resamps, layout = c(3, 1))\ndifs <- diff(resamps)\ndifs## \n## Call:\n## diff.resamples(x = resamps)\n## \n## Models: CART, kNearestNeighbors \n## Metrics: Accuracy, Kappa \n## Number of differences: 1 \n## p-value adjustment: bonferroni\nsummary(difs)## \n## Call:\n## summary.diff.resamples(object = difs)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## Accuracy \n##                   CART   kNearestNeighbors\n## CART                     -0.0931          \n## kNearestNeighbors 0.0115                  \n## \n## Kappa \n##                   CART   kNearestNeighbors\n## CART                     -0.121           \n## kNearestNeighbors 0.0104"},{"path":"classification-basic-concepts-and-techniques.html","id":"feature-selection-and-feature-preparation","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7 Feature Selection and Feature Preparation","text":"Decision trees implicitly select features splitting, can also\nselect features manually.see:\nhttp://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection#The_Feature_Ranking_Approach","code":"\nlibrary(FSelector)"},{"path":"classification-basic-concepts-and-techniques.html","id":"univariate-feature-importance-score","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.1 Univariate Feature Importance Score","text":"scores measure related feature class variable.\ndiscrete features (case), chi-square statistic can \nused derive score.plot importance descending order (using reorder order factor\nlevels used ggplot).Get 5 best featuresUse best 5 features build model (Fselector provides\n.simple.formula)many alternative ways calculate univariate importance\nscores (see package FSelector). (also) work continuous\nfeatures. One example information gain ratio based entropy \nused decision tree induction.","code":"\nweights <- Zoo_train |> \n  chi.squared(type ~ ., data = _) |>\n  as_tibble(rownames = \"feature\") |>\n  arrange(desc(attr_importance))\n\nweights## # A tibble: 16 × 2\n##    feature  attr_importance\n##    <chr>              <dbl>\n##  1 feathers           1    \n##  2 milk               1    \n##  3 backbone           1    \n##  4 toothed            0.975\n##  5 eggs               0.933\n##  6 hair               0.907\n##  7 breathes           0.898\n##  8 airborne           0.848\n##  9 fins               0.845\n## 10 legs               0.828\n## 11 tail               0.779\n## 12 catsize            0.664\n## 13 aquatic            0.655\n## 14 venomous           0.475\n## 15 predator           0.385\n## 16 domestic           0.231\nggplot(weights,\n  aes(x = attr_importance, y = reorder(feature, attr_importance))) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Importance score\") + \n  ylab(\"Feature\")\nsubset <- cutoff.k(weights |> \n                   column_to_rownames(\"feature\"), 5)\nsubset## [1] \"feathers\" \"milk\"     \"backbone\" \"toothed\" \n## [5] \"eggs\"\nf <- as.simple.formula(subset, \"type\")\nf## type ~ feathers + milk + backbone + toothed + eggs\n## <environment: 0x55a5094da288>\nm <- Zoo_train |> rpart(f, data = _)\nrpart.plot(m, extra = 2, roundint = FALSE)\nZoo_train |> \n  gain.ratio(type ~ ., data = _) |>\n  as_tibble(rownames = \"feature\") |>\n  arrange(desc(attr_importance))## # A tibble: 16 × 2\n##    feature  attr_importance\n##    <chr>              <dbl>\n##  1 milk              1     \n##  2 backbone          1     \n##  3 feathers          1     \n##  4 toothed           0.919 \n##  5 eggs              0.827 \n##  6 breathes          0.821 \n##  7 hair              0.782 \n##  8 fins              0.689 \n##  9 legs              0.682 \n## 10 airborne          0.671 \n## 11 tail              0.573 \n## 12 aquatic           0.391 \n## 13 catsize           0.383 \n## 14 venomous          0.351 \n## 15 predator          0.125 \n## 16 domestic          0.0975"},{"path":"classification-basic-concepts-and-techniques.html","id":"feature-subset-selection","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.2 Feature Subset Selection","text":"Often features related calculating importance feature\nindependently optimal. can use greedy search heuristics. \nexample cfs uses correlation/entropy best first search.Black-box feature selection uses evaluator function (black box)\ncalculate score maximized. First, define evaluation\nfunction builds model given subset features calculates \nquality score. use average 5 bootstrap samples\n(method = \"cv\" can also used instead), tuning (faster),\naverage accuracy score.Start features (class variable type)several (greedy) search strategies available. run \n!","code":"\nZoo_train |> \n  cfs(type ~ ., data = _)##  [1] \"hair\"     \"feathers\" \"eggs\"     \"milk\"    \n##  [5] \"toothed\"  \"backbone\" \"breathes\" \"fins\"    \n##  [9] \"legs\"     \"tail\"\nevaluator <- function(subset) {\n  model <- Zoo_train |> \n    train(as.simple.formula(subset, \"type\"),\n          data = _,\n          method = \"rpart\",\n          trControl = trainControl(method = \"boot\", number = 5),\n          tuneLength = 0)\n  results <- model$resample$Accuracy\n  cat(\"Trying features:\", paste(subset, collapse = \" + \"), \"\\n\")\n  m <- mean(results)\n  cat(\"Accuracy:\", round(m, 2), \"\\n\\n\")\n  m\n}\nfeatures <- Zoo_train |> colnames() |> setdiff(\"type\")\n##subset <- backward.search(features, evaluator)\n##subset <- forward.search(features, evaluator)\n##subset <- best.first.search(features, evaluator)\n##subset <- hill.climbing.search(features, evaluator)\n##subset"},{"path":"classification-basic-concepts-and-techniques.html","id":"using-dummy-variables-for-factors","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.3 Using Dummy Variables for Factors","text":"Nominal features (factors) often encoded series 0-1 dummy\nvariables. example, let us try predict animal predator\ngiven type. First use original encoding type factor\nseveral values.Note: splits use multiple values. Building tree become\nextremely slow factor many levels (different values) since \ntree check possible splits two subsets. situation\navoided.Convert type set 0-1 dummy variables using class2ind. See\nalso ? dummyVars package caret.Using caret original factor encoding automatically translates\nfactors (type) 0-1 dummy variables (e.g., typeinsect = 0).\nreason models directly use factors caret\ntries consistently work .Note: use fixed value tuning parameter cp, \ncreate tuning grid contains value.","code":"\ntree_predator <- Zoo_train |> \n  rpart(predator ~ type, data = _)\nrpart.plot(tree_predator, extra = 2, roundint = FALSE)\nZoo_train_dummy <- as_tibble(class2ind(Zoo_train$type)) |> \n  mutate(across(everything(), as.factor)) |>\n  add_column(predator = Zoo_train$predator)\nZoo_train_dummy## # A tibble: 83 × 8\n##    mammal bird  reptile fish  amphibian insect\n##    <fct>  <fct> <fct>   <fct> <fct>     <fct> \n##  1 1      0     0       0     0         0     \n##  2 1      0     0       0     0         0     \n##  3 0      0     0       1     0         0     \n##  4 1      0     0       0     0         0     \n##  5 1      0     0       0     0         0     \n##  6 1      0     0       0     0         0     \n##  7 0      0     0       1     0         0     \n##  8 0      0     0       1     0         0     \n##  9 1      0     0       0     0         0     \n## 10 0      1     0       0     0         0     \n## # ℹ 73 more rows\n## # ℹ 2 more variables: mollusc.et.al <fct>,\n## #   predator <fct>\ntree_predator <- Zoo_train_dummy |> \n  rpart(predator ~ ., \n        data = _,\n        control = rpart.control(minsplit = 2, cp = 0.01))\nrpart.plot(tree_predator, roundint = FALSE)\nfit <- Zoo_train |> \n  train(predator ~ type, \n        data = _, \n        method = \"rpart\",\n        control = rpart.control(minsplit = 2),\n        tuneGrid = data.frame(cp = 0.01))\nfit## CART \n## \n## 83 samples\n##  1 predictor\n##  2 classes: 'TRUE', 'FALSE' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.606     0.203\n## \n## Tuning parameter 'cp' was held constant at a value\n##  of 0.01\nrpart.plot(fit$finalModel, extra = 2)"},{"path":"classification-basic-concepts-and-techniques.html","id":"class-imbalance","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.8 Class Imbalance","text":"Classifiers hard time learn data much \nobservations one class (called majority class). called\nclass imbalance problem.good article problem \nsolutions.Class distributionTo create imbalanced problem, want decide animal \nreptile. First, change class variable make binary\nreptile/reptile classification problem. Note: use \ntraining data testing. use separate testing data set!forget make class variable factor (nominal variable)\nget regression tree instead classification tree.See class imbalance problem.Create test training data. use 50/50 split make sure\ntest set samples rare reptile class.new class variable clearly balanced. problem \nbuilding tree!","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\ndata(Zoo, package=\"mlbench\")\nggplot(Zoo, aes(y = type)) + geom_bar()\nZoo_reptile <- Zoo |> \n  mutate(type = factor(Zoo$type == \"reptile\", \n                       levels = c(FALSE, TRUE),\n                       labels = c(\"nonreptile\", \"reptile\")))\nsummary(Zoo_reptile)##     hair          feathers          eggs        \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:58        FALSE:81        FALSE:42       \n##  TRUE :43        TRUE :20        TRUE :59       \n##                                                 \n##                                                 \n##                                                 \n##     milk          airborne        aquatic       \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:60        FALSE:77        FALSE:65       \n##  TRUE :41        TRUE :24        TRUE :36       \n##                                                 \n##                                                 \n##                                                 \n##   predator        toothed         backbone      \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:45        FALSE:40        FALSE:18       \n##  TRUE :56        TRUE :61        TRUE :83       \n##                                                 \n##                                                 \n##                                                 \n##   breathes        venomous          fins        \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:21        FALSE:93        FALSE:84       \n##  TRUE :80        TRUE :8         TRUE :17       \n##                                                 \n##                                                 \n##                                                 \n##       legs         tail          domestic      \n##  Min.   :0.00   Mode :logical   Mode :logical  \n##  1st Qu.:2.00   FALSE:26        FALSE:88       \n##  Median :4.00   TRUE :75        TRUE :13       \n##  Mean   :2.84                                  \n##  3rd Qu.:4.00                                  \n##  Max.   :8.00                                  \n##   catsize                type   \n##  Mode :logical   nonreptile:96  \n##  FALSE:57        reptile   : 5  \n##  TRUE :44                       \n##                                 \n##                                 \n## \nggplot(Zoo_reptile, aes(y = type)) + geom_bar()\nset.seed(1234)\n\ninTrain <- createDataPartition(y = Zoo_reptile$type, p = .5, list = FALSE)\ntraining_reptile <- Zoo_reptile |> slice(inTrain)\ntesting_reptile <- Zoo_reptile |> slice(-inTrain)"},{"path":"classification-basic-concepts-and-techniques.html","id":"option-1-use-the-data-as-is-and-hope-for-the-best","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.8.1 Option 1: Use the Data As Is and Hope For The Best","text":"Warnings: “missing values resampled performance\nmeasures.” means test folds contain examples \nclasses. likely class imbalance small datasets.tree predicts everything non-reptile. look error \ntest set.Accuracy high, exactly -information rate\nkappa zero. Sensitivity also zero, meaning \nidentify positive (reptile). cost missing positive \nmuch larger cost associated misclassifying negative,\naccuracy good measure! dealing imbalance, \nconcerned accuracy, want increase \nsensitivity, .e., chance identify positive examples.Note: positive class value (one want detect) \nset manually reptile using positive = \"reptile\". Otherwise\nsensitivity/specificity correctly calculated.","code":"\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"))## Warning in nominalTrainWorkflow(x = x, y = y, wts =\n## weights, info = trainInfo, : There were missing values\n## in resampled performance measures.\nfit## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 47, 46, 46, 45, 46, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.947     0    \n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         48       2\n##   reptile             0       0\n##                                         \n##                Accuracy : 0.96          \n##                  95% CI : (0.863, 0.995)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.677         \n##                                         \n##                   Kappa : 0             \n##                                         \n##  Mcnemar's Test P-Value : 0.480         \n##                                         \n##             Sensitivity : 0.00          \n##             Specificity : 1.00          \n##          Pos Pred Value :  NaN          \n##          Neg Pred Value : 0.96          \n##              Prevalence : 0.04          \n##          Detection Rate : 0.00          \n##    Detection Prevalence : 0.00          \n##       Balanced Accuracy : 0.50          \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-basic-concepts-and-techniques.html","id":"option-2-balance-data-with-resampling","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.8.2 Option 2: Balance Data With Resampling","text":"use stratified sampling replacement (oversample \nminority/positive class). also use SMOTE (package DMwR)\nsampling strategies (e.g., package unbalanced). use\n50+50 observations (Note: many samples chosen several\ntimes).Check unbalanced testing data.Note accuracy information rate! However,\nkappa (improvement accuracy randomness) sensitivity (\nability identify reptiles) increased.tradeoff sensitivity specificity (many \nidentified animals really reptiles) tradeoff can controlled\nusing sample proportions. can sample reptiles increase\nsensitivity cost lower specificity (effect seen\ndata since test set reptiles).","code":"\nlibrary(sampling)\nset.seed(1000) # for repeatability\n\nid <- strata(training_reptile, stratanames = \"type\", size = c(50, 50), method = \"srswr\")\ntraining_reptile_balanced <- training_reptile |> \n  slice(id$ID_unit)\ntable(training_reptile_balanced$type)## \n## nonreptile    reptile \n##         50         50\nfit <- training_reptile_balanced |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"),\n        control = rpart.control(minsplit = 5))\n\nfit## CART \n## \n## 100 samples\n##  16 predictor\n##   2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 90, 90, 90, 90, 90, 90, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa\n##   0.18  0.81      0.62 \n##   0.30  0.63      0.26 \n##   0.34  0.53      0.06 \n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was cp = 0.18.\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         19       0\n##   reptile            29       2\n##                                         \n##                Accuracy : 0.42          \n##                  95% CI : (0.282, 0.568)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1             \n##                                         \n##                   Kappa : 0.05          \n##                                         \n##  Mcnemar's Test P-Value : 2e-07         \n##                                         \n##             Sensitivity : 1.0000        \n##             Specificity : 0.3958        \n##          Pos Pred Value : 0.0645        \n##          Neg Pred Value : 1.0000        \n##              Prevalence : 0.0400        \n##          Detection Rate : 0.0400        \n##    Detection Prevalence : 0.6200        \n##       Balanced Accuracy : 0.6979        \n##                                         \n##        'Positive' Class : reptile       \n## \nid <- strata(training_reptile, stratanames = \"type\", size = c(50, 100), method = \"srswr\")\ntraining_reptile_balanced <- training_reptile |> \n  slice(id$ID_unit)\ntable(training_reptile_balanced$type)## \n## nonreptile    reptile \n##         50        100\nfit <- training_reptile_balanced |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"),\n        control = rpart.control(minsplit = 5))\n\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         33       0\n##   reptile            15       2\n##                                         \n##                Accuracy : 0.7           \n##                  95% CI : (0.554, 0.821)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1.000000      \n##                                         \n##                   Kappa : 0.15          \n##                                         \n##  Mcnemar's Test P-Value : 0.000301      \n##                                         \n##             Sensitivity : 1.000         \n##             Specificity : 0.688         \n##          Pos Pred Value : 0.118         \n##          Neg Pred Value : 1.000         \n##              Prevalence : 0.040         \n##          Detection Rate : 0.040         \n##    Detection Prevalence : 0.340         \n##       Balanced Accuracy : 0.844         \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-basic-concepts-and-techniques.html","id":"option-3-build-a-larger-tree-and-use-predicted-probabilities","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.8.3 Option 3: Build A Larger Tree and use Predicted Probabilities","text":"Increase complexity require less data splitting node. \nalso use AUC (area ROC) tuning metric. need \nspecify two class summary function. Note tree still trying\nimprove accuracy data AUC! also enable class\nprobabilities since want predict probabilities later.Note: Accuracy high, close \n-information rate!","code":"\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        tuneLength = 10,\n        trControl = trainControl(method = \"cv\",\n        classProbs = TRUE,  ## necessary for predict with type=\"prob\"\n        summaryFunction=twoClassSummary),  ## necessary for ROC\n        metric = \"ROC\",\n        control = rpart.control(minsplit = 3))## Warning in nominalTrainWorkflow(x = x, y = y, wts =\n## weights, info = trainInfo, : There were missing values\n## in resampled performance measures.\nfit## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 47, 46, 46, 46, 45, ... \n## Resampling results:\n## \n##   ROC    Sens   Spec\n##   0.358  0.975  0   \n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         48       2\n##   reptile             0       0\n##                                         \n##                Accuracy : 0.96          \n##                  95% CI : (0.863, 0.995)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.677         \n##                                         \n##                   Kappa : 0             \n##                                         \n##  Mcnemar's Test P-Value : 0.480         \n##                                         \n##             Sensitivity : 0.00          \n##             Specificity : 1.00          \n##          Pos Pred Value :  NaN          \n##          Neg Pred Value : 0.96          \n##              Prevalence : 0.04          \n##          Detection Rate : 0.00          \n##    Detection Prevalence : 0.00          \n##       Balanced Accuracy : 0.50          \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-basic-concepts-and-techniques.html","id":"create-a-biased-classifier","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.8.3.1 Create A Biased Classifier","text":"can create classifier detect reptiles \nexpense misclassifying non-reptiles. equivalent increasing\ncost misclassifying reptile non-reptile. usual rule \npredict node majority class test data \nnode. binary classification problem means probability \n>50%. following, reduce threshold 1% . \nmeans new observation ends leaf node 1% \nreptiles training observation classified \nreptile. data set small works better data.Note accuracy goes information rate.\nHowever, measures based idea errors \ncost. important now able find \nreptiles.","code":"\nprob <- predict(fit, testing_reptile, type = \"prob\")\ntail(prob)##      nonreptile reptile\n## tuna      1.000  0.0000\n## vole      0.962  0.0385\n## wasp      0.500  0.5000\n## wolf      0.962  0.0385\n## worm      1.000  0.0000\n## wren      0.962  0.0385\npred <- as.factor(ifelse(prob[,\"reptile\"]>=0.01, \"reptile\", \"nonreptile\"))\n\nconfusionMatrix(data = pred,\n                ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         13       0\n##   reptile            35       2\n##                                         \n##                Accuracy : 0.3           \n##                  95% CI : (0.179, 0.446)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1             \n##                                         \n##                   Kappa : 0.029         \n##                                         \n##  Mcnemar's Test P-Value : 9.08e-09      \n##                                         \n##             Sensitivity : 1.0000        \n##             Specificity : 0.2708        \n##          Pos Pred Value : 0.0541        \n##          Neg Pred Value : 1.0000        \n##              Prevalence : 0.0400        \n##          Detection Rate : 0.0400        \n##    Detection Prevalence : 0.7400        \n##       Balanced Accuracy : 0.6354        \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-basic-concepts-and-techniques.html","id":"plot-the-roc-curve","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.8.3.2 Plot the ROC Curve","text":"Since binary classification problem classifier \npredicts probability observation reptile, can also\nuse receiver operating characteristic\n(ROC)\ncurve. ROC curve different cutoff thresholds \nprobability used connected line. area \ncurve represents single number well classifier works (\ncloser one, better).","code":"\nlibrary(\"pROC\")## Type 'citation(\"pROC\")' for a citation.## \n## Attaching package: 'pROC'## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\nr <- roc(testing_reptile$type == \"reptile\", prob[,\"reptile\"])## Setting levels: control = FALSE, case = TRUE## Setting direction: controls < cases\nr## \n## Call:\n## roc.default(response = testing_reptile$type == \"reptile\", predictor = prob[,     \"reptile\"])\n## \n## Data: prob[, \"reptile\"] in 48 controls (testing_reptile$type == \"reptile\" FALSE) < 2 cases (testing_reptile$type == \"reptile\" TRUE).\n## Area under the curve: 0.766\nggroc(r) + geom_abline(intercept = 1, slope = 1, color = \"darkgrey\")"},{"path":"classification-basic-concepts-and-techniques.html","id":"option-4-use-a-cost-sensitive-classifier","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.8.4 Option 4: Use a Cost-Sensitive Classifier","text":"implementation CART rpart can use cost matrix making\nsplitting decisions (parameter loss). matrix formTP FP FN TNTP TN 0. make FN expensive (100).warning “missing values resampled performance\nmeasures” means folds contain reptiles (\nclass imbalance) thus performance measures \ncalculates.high cost false negatives results classifier \nmiss reptile.Note: Using cost-sensitive classifier often best option.\nUnfortunately, classification algorithms (\nimplementation) ability consider misclassification\ncost.","code":"\ncost <- matrix(c(\n  0,   1,\n  100, 0\n), byrow = TRUE, nrow = 2)\ncost##      [,1] [,2]\n## [1,]    0    1\n## [2,]  100    0\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        parms = list(loss = cost),\n        trControl = trainControl(method = \"cv\"))\nfit## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 46, 46, 45, 46, 45, ... \n## Resampling results:\n## \n##   Accuracy  Kappa  \n##   0.477     -0.0304\n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         39       0\n##   reptile             9       2\n##                                         \n##                Accuracy : 0.82          \n##                  95% CI : (0.686, 0.914)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.99998       \n##                                         \n##                   Kappa : 0.257         \n##                                         \n##  Mcnemar's Test P-Value : 0.00766       \n##                                         \n##             Sensitivity : 1.000         \n##             Specificity : 0.812         \n##          Pos Pred Value : 0.182         \n##          Neg Pred Value : 1.000         \n##              Prevalence : 0.040         \n##          Detection Rate : 0.040         \n##    Detection Prevalence : 0.220         \n##       Balanced Accuracy : 0.906         \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"classification-alternative-techniques","chapter":"4 Classification: Alternative Techniques","heading":"4 Classification: Alternative Techniques","text":"Install packages used chapter:packages used chapter : C50 (M. Kuhn Quinlan 2023), caret (M. Kuhn 2023), e1071 (Meyer et al. 2023), keras (Allaire Chollet 2023), lattice (Sarkar 2023), MASS (B. Ripley 2023a), mlbench (Leisch Dimitriadou. 2023), nnet (B. Ripley 2023b), randomForest (Breiman et al. 2022), rpart (Therneau Atkinson 2022), RWeka (Hornik 2023), scales (Wickham Seidel 2022), tidyverse (Wickham 2023b)use tidyverse prepare data.Show fewer digits","code":"\npkgs <- sort(c('tidyverse', 'caret', 'RWeka', 'lattice', 'scales',\n  'e1071', 'MASS', 'nnet', 'rpart', 'C50', 'randomForest', 'keras',\n  'mlbench'\n  ))\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)\nlibrary(tidyverse)\noptions(digits=3)"},{"path":"classification-alternative-techniques.html","id":"introduction-3","chapter":"4 Classification: Alternative Techniques","heading":"4.1 Introduction","text":"Many different classification algorithms\nproposed literature.\nchapter, apply popular methods.","code":""},{"path":"classification-alternative-techniques.html","id":"training-and-test-data","chapter":"4 Classification: Alternative Techniques","heading":"4.2 Training and Test Data","text":"use Zoo dataset included R package mlbench\n(may install ). Zoo dataset containing 17 (mostly\nlogical) variables different 101 animals data frame 17\ncolumns (hair, feathers, eggs, milk, airborne, aquatic, predator,\ntoothed, backbone, breathes, venomous, fins, legs, tail, domestic,\ncatsize, type). convert data frame tidyverse tibble\n(optional).use package caret \nmake preparing training sets building classification (\nregression) models easier. great cheat sheet can found\n.Multi-core support can used cross-validation. Note: \ncommented work rJava used RWeka\n.Test data used model building process needs set\naside purely testing model completely built. \nuse 80% training.","code":"\ndata(Zoo, package=\"mlbench\")\nZoo <- as_tibble(Zoo)\nZoo## # A tibble: 101 × 17\n##    hair  feathers eggs  milk  airborne aquatic predator\n##    <lgl> <lgl>    <lgl> <lgl> <lgl>    <lgl>   <lgl>   \n##  1 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  2 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  3 FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE    \n##  4 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  5 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  6 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  7 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  8 FALSE FALSE    TRUE  FALSE FALSE    TRUE    FALSE   \n##  9 FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE    \n## 10 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n## # ℹ 91 more rows\n## # ℹ 10 more variables: toothed <lgl>, backbone <lgl>,\n## #   breathes <lgl>, venomous <lgl>, fins <lgl>,\n## #   legs <int>, tail <lgl>, domestic <lgl>,\n## #   catsize <lgl>, type <fct>\nlibrary(caret)\n##library(doMC, quietly = TRUE)\n##registerDoMC(cores = 4)\n##getDoParWorkers()\ninTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]\nZoo_train <- Zoo |> slice(inTrain)\nZoo_test <- Zoo |> slice(-inTrain)"},{"path":"classification-alternative-techniques.html","id":"fitting-different-classification-models-to-the-training-data","chapter":"4 Classification: Alternative Techniques","heading":"4.3 Fitting Different Classification Models to the Training Data","text":"Create fixed sampling scheme (10-folds) can compare fitted\nmodels later.fixed folds used train() argument\ntrControl = trainControl(method = \"cv\", indexOut = train_index)). \ndon’t need fixed folds, remove indexOut = train_index \ncode .help building models caret see: ? trainNote: careful many NA values data.\ntrain() cross-validation many fail cases. \ncase can remove features (columns) many NAs, omit\nNAs using na.omit() use imputation replace \nreasonable values (e.g., feature mean via kNN). Highly\nimbalanced datasets also problematic since chance \nfold contain examples class leading hard \nunderstand error message.","code":"\ntrain_index <- createFolds(Zoo_train$type, k = 10)"},{"path":"classification-alternative-techniques.html","id":"conditional-inference-tree-decision-tree","chapter":"4 Classification: Alternative Techniques","heading":"4.3.1 Conditional Inference Tree (Decision Tree)","text":"","code":"\nctreeFit <- Zoo_train |> train(type ~ .,\n  method = \"ctree\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nctreeFit## Conditional Inference Tree \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 76, 74, 74, 76, 74, ... \n## Resampling results across tuning parameters:\n## \n##   mincriterion  Accuracy  Kappa\n##   0.010         0.808     0.746\n##   0.255         0.808     0.746\n##   0.500         0.808     0.746\n##   0.745         0.808     0.746\n##   0.990         0.808     0.746\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was mincriterion\n##  = 0.99.\nplot(ctreeFit$finalModel)"},{"path":"classification-alternative-techniques.html","id":"c-4.5-decision-tree","chapter":"4 Classification: Alternative Techniques","heading":"4.3.2 C 4.5 Decision Tree","text":"","code":"\nC45Fit <- Zoo_train |> train(type ~ .,\n  method = \"J48\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nC45Fit## C4.5-like Trees \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 76, 73, 74, 74, 76, 76, ... \n## Resampling results across tuning parameters:\n## \n##   C      M  Accuracy  Kappa\n##   0.010  1  0.978     0.971\n##   0.010  2  0.978     0.971\n##   0.010  3  0.978     0.971\n##   0.010  4  0.907     0.879\n##   0.010  5  0.918     0.893\n##   0.133  1  0.978     0.971\n##   0.133  2  0.978     0.971\n##   0.133  3  0.978     0.971\n##   0.133  4  0.907     0.879\n##   0.133  5  0.918     0.893\n##   0.255  1  0.989     0.985\n##   0.255  2  0.989     0.985\n##   0.255  3  0.978     0.971\n##   0.255  4  0.907     0.879\n##   0.255  5  0.918     0.893\n##   0.378  1  0.989     0.985\n##   0.378  2  0.989     0.985\n##   0.378  3  0.978     0.971\n##   0.378  4  0.907     0.879\n##   0.378  5  0.918     0.893\n##   0.500  1  0.989     0.985\n##   0.500  2  0.989     0.985\n##   0.500  3  0.978     0.971\n##   0.500  4  0.907     0.879\n##   0.500  5  0.918     0.893\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final values used for the model were C = 0.255\n##  and M = 1.\nC45Fit$finalModel## J48 pruned tree\n## ------------------\n## \n## feathersTRUE <= 0\n## |   milkTRUE <= 0\n## |   |   toothedTRUE <= 0\n## |   |   |   airborneTRUE <= 0\n## |   |   |   |   predatorTRUE <= 0\n## |   |   |   |   |   legs <= 2: mollusc.et.al (2.0)\n## |   |   |   |   |   legs > 2: insect (2.0)\n## |   |   |   |   predatorTRUE > 0: mollusc.et.al (6.0)\n## |   |   |   airborneTRUE > 0: insect (5.0)\n## |   |   toothedTRUE > 0\n## |   |   |   finsTRUE <= 0\n## |   |   |   |   aquaticTRUE <= 0: reptile (3.0)\n## |   |   |   |   aquaticTRUE > 0\n## |   |   |   |   |   eggsTRUE <= 0: reptile (1.0)\n## |   |   |   |   |   eggsTRUE > 0: amphibian (4.0)\n## |   |   |   finsTRUE > 0: fish (11.0)\n## |   milkTRUE > 0: mammal (33.0)\n## feathersTRUE > 0: bird (16.0)\n## \n## Number of Leaves  :  10\n## \n## Size of the tree :   19"},{"path":"classification-alternative-techniques.html","id":"k-nearest-neighbors","chapter":"4 Classification: Alternative Techniques","heading":"4.3.3 K-Nearest Neighbors","text":"Note: kNN uses Euclidean distance, data standardized\n(scaled) first. legs measured 0 6 \nvariables 0 1. Scaling can directly performed \npreprocessing train using parameter preProcess = \"scale\".","code":"\nknnFit <- Zoo_train |> train(type ~ .,\n  method = \"knn\",\n  data = _,\n  preProcess = \"scale\",\n    tuneLength = 5,\n  tuneGrid=data.frame(k = 1:10),\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nknnFit## k-Nearest Neighbors \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## Pre-processing: scaled (16) \n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 73, 76, 74, 74, 75, ... \n## Resampling results across tuning parameters:\n## \n##   k   Accuracy  Kappa\n##    1  1.000     1.000\n##    2  0.978     0.971\n##    3  0.967     0.957\n##    4  0.943     0.926\n##    5  0.965     0.954\n##    6  0.916     0.891\n##    7  0.883     0.850\n##    8  0.872     0.835\n##    9  0.883     0.848\n##   10  0.908     0.881\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was k = 1.\nknnFit$finalModel## 1-nearest neighbor model\n## Training set outcome distribution:\n## \n##        mammal          bird       reptile \n##            33            16             4 \n##          fish     amphibian        insect \n##            11             4             7 \n## mollusc.et.al \n##             8"},{"path":"classification-alternative-techniques.html","id":"part-rule-based-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.3.4 PART (Rule-based classifier)","text":"","code":"\nrulesFit <- Zoo_train |> train(type ~ .,\n  method = \"PART\",\n  data = _,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\", indexOut = train_index))\nrulesFit## Rule-Based Classifier \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 77, 72, 77, 74, 74, 73, ... \n## Resampling results across tuning parameters:\n## \n##   threshold  pruned  Accuracy  Kappa\n##   0.010      yes     0.965     0.955\n##   0.010      no      0.988     0.984\n##   0.133      yes     0.965     0.955\n##   0.133      no      0.988     0.984\n##   0.255      yes     0.965     0.955\n##   0.255      no      0.988     0.984\n##   0.378      yes     0.965     0.955\n##   0.378      no      0.988     0.984\n##   0.500      yes     0.965     0.955\n##   0.500      no      0.988     0.984\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final values used for the model were threshold\n##  = 0.5 and pruned = no.\nrulesFit$finalModel## PART decision list\n## ------------------\n## \n## feathersTRUE <= 0 AND\n## milkTRUE > 0: mammal (33.0)\n## \n## feathersTRUE > 0: bird (16.0)\n## \n## toothedTRUE <= 0 AND\n## airborneTRUE <= 0 AND\n## predatorTRUE > 0: mollusc.et.al (6.0)\n## \n## toothedTRUE <= 0 AND\n## legs > 2: insect (7.0)\n## \n## finsTRUE > 0: fish (11.0)\n## \n## toothedTRUE > 0 AND\n## aquaticTRUE <= 0: reptile (3.0)\n## \n## aquaticTRUE > 0 AND\n## venomousTRUE <= 0: amphibian (3.0)\n## \n## aquaticTRUE <= 0: mollusc.et.al (2.0)\n## \n## : reptile (2.0/1.0)\n## \n## Number of Rules  :   9"},{"path":"classification-alternative-techniques.html","id":"linear-support-vector-machines","chapter":"4 Classification: Alternative Techniques","heading":"4.3.5 Linear Support Vector Machines","text":"","code":"\nsvmFit <- Zoo_train |> train(type ~.,\n  method = \"svmLinear\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nsvmFit## Support Vector Machines with Linear Kernel \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 73, 75, 75, 74, 74, 76, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   1         1    \n## \n## Tuning parameter 'C' was held constant at a value of 1\nsvmFit$finalModel## Support Vector Machine object of class \"ksvm\" \n## \n## SV type: C-svc  (classification) \n##  parameter : cost C = 1 \n## \n## Linear (vanilla) kernel function. \n## \n## Number of Support Vectors : 44 \n## \n## Objective Function Value : -0.143 -0.198 -0.148 -0.175 -0.0945 -0.104 -0.19 -0.0814 -0.154 -0.0917 -0.115 -0.177 -0.568 -0.104 -0.15 -0.119 -0.0478 -0.083 -0.123 -0.148 -0.58 \n## Training error : 0"},{"path":"classification-alternative-techniques.html","id":"random-forest","chapter":"4 Classification: Alternative Techniques","heading":"4.3.6 Random Forest","text":"","code":"\nrandomForestFit <- Zoo_train |> train(type ~ .,\n  method = \"rf\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nrandomForestFit## Random Forest \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 76, 75, 74, 73, 76, ... \n## Resampling results across tuning parameters:\n## \n##   mtry  Accuracy  Kappa\n##    2    0.976     0.968\n##    5    0.976     0.968\n##    9    0.976     0.968\n##   12    0.965     0.954\n##   16    0.976     0.969\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was mtry = 2.\nrandomForestFit$finalModel## \n## Call:\n##  randomForest(x = x, y = y, mtry = param$mtry) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 2\n## \n##         OOB estimate of  error rate: 3.61%\n## Confusion matrix:\n##               mammal bird reptile fish amphibian\n## mammal            33    0       0    0         0\n## bird               0   16       0    0         0\n## reptile            0    0       2    1         1\n## fish               0    0       0   11         0\n## amphibian          0    0       0    0         4\n## insect             0    0       0    0         0\n## mollusc.et.al      0    0       0    0         0\n##               insect mollusc.et.al class.error\n## mammal             0             0       0.000\n## bird               0             0       0.000\n## reptile            0             0       0.500\n## fish               0             0       0.000\n## amphibian          0             0       0.000\n## insect             7             0       0.000\n## mollusc.et.al      1             7       0.125"},{"path":"classification-alternative-techniques.html","id":"gradient-boosted-decision-trees-xgboost","chapter":"4 Classification: Alternative Techniques","heading":"4.3.7 Gradient Boosted Decision Trees (xgboost)","text":"","code":"\nxgboostFit <- Zoo_train |> train(type ~ .,\n  method = \"xgbTree\",\n  data = _,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\", indexOut = train_index),\n  tuneGrid = expand.grid(\n    nrounds = 20,\n    max_depth = 3,\n    colsample_bytree = .6,\n    eta = 0.1,\n    gamma=0,\n    min_child_weight = 1,\n    subsample = .5\n  ))\nxgboostFit## eXtreme Gradient Boosting \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 76, 75, 75, 74, 76, 74, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.976     0.969\n## \n## Tuning parameter 'nrounds' was held constant at\n##  a value of 1\n## Tuning parameter 'subsample' was\n##  held constant at a value of 0.5\nxgboostFit$finalModel## ##### xgb.Booster\n## raw: 111.3 Kb \n## call:\n##   xgboost::xgb.train(params = list(eta = param$eta, max_depth = param$max_depth, \n##     gamma = param$gamma, colsample_bytree = param$colsample_bytree, \n##     min_child_weight = param$min_child_weight, subsample = param$subsample), \n##     data = x, nrounds = param$nrounds, num_class = length(lev), \n##     objective = \"multi:softprob\")\n## params (as set within xgb.train):\n##   eta = \"0.1\", max_depth = \"3\", gamma = \"0\", colsample_bytree = \"0.6\", min_child_weight = \"1\", subsample = \"0.5\", num_class = \"7\", objective = \"multi:softprob\", validate_parameters = \"TRUE\"\n## xgb.attributes:\n##   niter\n## callbacks:\n##   cb.print.evaluation(period = print_every_n)\n## # of features: 16 \n## niter: 20\n## nfeatures : 16 \n## xNames : hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \n## problemType : Classification \n## tuneValue :\n##    nrounds max_depth eta gamma colsample_bytree\n## 1      20         3 0.1     0              0.6\n##   min_child_weight subsample\n## 1                1       0.5\n## obsLevels : mammal bird reptile fish amphibian insect mollusc.et.al \n## param :\n##  list()"},{"path":"classification-alternative-techniques.html","id":"artificial-neural-network","chapter":"4 Classification: Alternative Techniques","heading":"4.3.8 Artificial Neural Network","text":"","code":"\nnnetFit <- Zoo_train |> train(type ~ .,\n  method = \"nnet\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index),\n  trace = FALSE)\nnnetFit## Neural Network \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 77, 72, 75, 75, 76, ... \n## Resampling results across tuning parameters:\n## \n##   size  decay  Accuracy  Kappa\n##   1     0e+00  0.694     0.558\n##   1     1e-04  0.807     0.728\n##   1     1e-03  0.892     0.852\n##   1     1e-02  0.825     0.766\n##   1     1e-01  0.727     0.633\n##   3     0e+00  0.954     0.939\n##   3     1e-04  0.989     0.986\n##   3     1e-03  0.989     0.986\n##   3     1e-02  0.989     0.986\n##   3     1e-01  0.989     0.986\n##   5     0e+00  0.939     0.917\n##   5     1e-04  0.965     0.954\n##   5     1e-03  0.989     0.986\n##   5     1e-02  0.989     0.986\n##   5     1e-01  0.989     0.986\n##   7     0e+00  0.989     0.986\n##   7     1e-04  0.989     0.986\n##   7     1e-03  0.989     0.986\n##   7     1e-02  1.000     1.000\n##   7     1e-01  0.989     0.986\n##   9     0e+00  0.989     0.986\n##   9     1e-04  0.989     0.986\n##   9     1e-03  0.989     0.986\n##   9     1e-02  0.989     0.986\n##   9     1e-01  1.000     1.000\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final values used for the model were size = 7\n##  and decay = 0.01.\nnnetFit$finalModel## a 16-7-7 network with 175 weights\n## inputs: hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \n## output(s): .outcome \n## options were - softmax modelling  decay=0.01"},{"path":"classification-alternative-techniques.html","id":"comparing-models","chapter":"4 Classification: Alternative Techniques","heading":"4.4 Comparing Models","text":"Collect performance metrics models trained \ndata.Calculate summary statisticsPerform inference differences models. metric, \npair-wise differences computed tested assess \ndifference equal zero. default Bonferroni correction \nmultiple comparison used. Differences shown upper triangle\np-values lower triangle.perform similarly well except ctree (differences first row\nnegative p-values first column <.05 indicating\nnull-hypothesis difference 0 can rejected).","code":"\nresamps <- resamples(list(\n  ctree = ctreeFit,\n  C45 = C45Fit,\n  SVM = svmFit,\n  KNN = knnFit,\n  rules = rulesFit,\n  randomForest = randomForestFit,\n  xgboost = xgboostFit,\n  NeuralNet = nnetFit\n    ))\nresamps## \n## Call:\n## resamples.default(x = list(ctree = ctreeFit, C45\n##  = rulesFit, randomForest = randomForestFit, xgboost\n##  = xgboostFit, NeuralNet = nnetFit))\n## \n## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \n## Number of resamples: 10 \n## Performance metrics: Accuracy, Kappa \n## Time estimates for: everything, final model fit\nsummary(resamps)## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \n## Number of resamples: 10 \n## \n## Accuracy \n##               Min. 1st Qu. Median  Mean 3rd Qu.  Max.\n## ctree        0.750   0.778  0.778 0.808   0.851 0.889\n## C45          0.889   1.000  1.000 0.989   1.000 1.000\n## SVM          1.000   1.000  1.000 1.000   1.000 1.000\n## KNN          1.000   1.000  1.000 1.000   1.000 1.000\n## rules        0.875   1.000  1.000 0.988   1.000 1.000\n## randomForest 0.875   1.000  1.000 0.976   1.000 1.000\n## xgboost      0.875   1.000  1.000 0.976   1.000 1.000\n## NeuralNet    1.000   1.000  1.000 1.000   1.000 1.000\n##              NA's\n## ctree           0\n## C45             0\n## SVM             0\n## KNN             0\n## rules           0\n## randomForest    0\n## xgboost         0\n## NeuralNet       0\n## \n## Kappa \n##               Min. 1st Qu. Median  Mean 3rd Qu.  Max.\n## ctree        0.673   0.701  0.721 0.746   0.794 0.857\n## C45          0.850   1.000  1.000 0.985   1.000 1.000\n## SVM          1.000   1.000  1.000 1.000   1.000 1.000\n## KNN          1.000   1.000  1.000 1.000   1.000 1.000\n## rules        0.837   1.000  1.000 0.984   1.000 1.000\n## randomForest 0.833   1.000  1.000 0.968   1.000 1.000\n## xgboost      0.833   1.000  1.000 0.969   1.000 1.000\n## NeuralNet    1.000   1.000  1.000 1.000   1.000 1.000\n##              NA's\n## ctree           0\n## C45             0\n## SVM             0\n## KNN             0\n## rules           0\n## randomForest    0\n## xgboost         0\n## NeuralNet       0\nlibrary(lattice)\nbwplot(resamps, layout = c(3, 1))\ndifs <- diff(resamps)\ndifs## \n## Call:\n## diff.resamples(x = resamps)\n## \n## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \n## Metrics: Accuracy, Kappa \n## Number of differences: 28 \n## p-value adjustment: bonferroni\nsummary(difs)## \n## Call:\n## summary.diff.resamples(object = difs)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## Accuracy \n##              ctree    C45      SVM      KNN     \n## ctree                 -0.18095 -0.19206 -0.19206\n## C45          0.000109          -0.01111 -0.01111\n## SVM          3.49e-05 1.000000           0.00000\n## KNN          3.49e-05 1.000000 NA               \n## rules        5.75e-05 1.000000 1.000000 1.000000\n## randomForest 0.000126 1.000000 1.000000 1.000000\n## xgboost      0.001617 1.000000 1.000000 1.000000\n## NeuralNet    3.49e-05 1.000000 NA       NA      \n##              rules    randomForest xgboost  NeuralNet\n## ctree        -0.17956 -0.16845     -0.16845 -0.19206 \n## C45           0.00139  0.01250      0.01250 -0.01111 \n## SVM           0.01250  0.02361      0.02361  0.00000 \n## KNN           0.01250  0.02361      0.02361  0.00000 \n## rules                  0.01111      0.01111 -0.01250 \n## randomForest 1.000000               0.00000 -0.02361 \n## xgboost      1.000000 1.000000              -0.02361 \n## NeuralNet    1.000000 1.000000     1.000000          \n## \n## Kappa \n##              ctree    C45       SVM       KNN      \n## ctree                 -0.238879 -0.253879 -0.253879\n## C45          6.67e-05           -0.015000 -0.015000\n## SVM          2.17e-05 1.00000              0.000000\n## KNN          2.17e-05 1.00000   NA                 \n## rules        3.92e-05 1.00000   1.00000   1.00000  \n## randomForest 8.30e-05 1.00000   1.00000   1.00000  \n## xgboost      0.00135  1.00000   1.00000   1.00000  \n## NeuralNet    2.17e-05 1.00000   NA        NA       \n##              rules     randomForest xgboost  \n## ctree        -0.237552 -0.222212    -0.222927\n## C45           0.001327  0.016667     0.015952\n## SVM           0.016327  0.031667     0.030952\n## KNN           0.016327  0.031667     0.030952\n## rules                   0.015340     0.014626\n## randomForest 1.00000                -0.000714\n## xgboost      1.00000   1.00000               \n## NeuralNet    1.00000   1.00000      1.00000  \n##              NeuralNet\n## ctree        -0.253879\n## C45          -0.015000\n## SVM           0.000000\n## KNN           0.000000\n## rules        -0.016327\n## randomForest -0.031667\n## xgboost      -0.030952\n## NeuralNet"},{"path":"classification-alternative-techniques.html","id":"applying-the-chosen-model-to-the-test-data","chapter":"4 Classification: Alternative Techniques","heading":"4.5 Applying the Chosen Model to the Test Data","text":"models similarly well data. choose random\nforest model.Calculate confusion matrix held-test data.","code":"\npr <- predict(randomForestFit, Zoo_test)\npr##  [1] mammal        mollusc.et.al bird         \n##  [4] mammal        insect        bird         \n##  [7] mammal        mollusc.et.al mammal       \n## [10] mammal        bird          bird         \n## [13] fish          mammal        fish         \n## [16] mammal        bird          mammal       \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al\nconfusionMatrix(pr, reference = Zoo_test$type)## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian\n##   mammal             8    0       0    0         0\n##   bird               0    4       1    0         0\n##   reptile            0    0       0    0         0\n##   fish               0    0       0    2         0\n##   amphibian          0    0       0    0         0\n##   insect             0    0       0    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                Reference\n## Prediction      insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             1             0\n##   mollusc.et.al      0             2\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.944         \n##                  95% CI : (0.727, 0.999)\n##     No Information Rate : 0.444         \n##     P-Value [Acc > NIR] : 1.08e-05      \n##                                         \n##                   Kappa : 0.922         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       0.929\n## Pos Pred Value               1.000       0.800\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.444       0.222\n## Detection Rate               0.444       0.222\n## Detection Prevalence         0.444       0.278\n## Balanced Accuracy            1.000       0.964\n##                      Class: reptile Class: fish\n## Sensitivity                  0.0000       1.000\n## Specificity                  1.0000       1.000\n## Pos Pred Value                  NaN       1.000\n## Neg Pred Value               0.9444       1.000\n## Prevalence                   0.0556       0.111\n## Detection Rate               0.0000       0.111\n## Detection Prevalence         0.0000       0.111\n## Balanced Accuracy            0.5000       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                        NA        1.0000\n## Specificity                         1        1.0000\n## Pos Pred Value                     NA        1.0000\n## Neg Pred Value                     NA        1.0000\n## Prevalence                          0        0.0556\n## Detection Rate                      0        0.0556\n## Detection Prevalence                0        0.0556\n## Balanced Accuracy                  NA        1.0000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-alternative-techniques.html","id":"deep-learning-with-kerastensorflow","chapter":"4 Classification: Alternative Techniques","heading":"4.6 Deep Learning with keras/tensorflow","text":"keras package needs packages reticulate tensorflow. \ninstall keras need tohave working Python installation,install keras R package install.packages(\"keras\"), andinstall tensorflow/keras Python modules \nlibrary(keras); install_keras()Prepare data. data needs matrix \nnumeric/integer values. class variable needs one-hot encoded \nkeras function to_categorical() expects class index starting 0\ninstead 1.Deep learning uses large set hyper-parameters. Choices \nactivation function, number layers, number units per layer, \noptimizer, type regularization used.define simple network single dense hidden layer.\nNote keras, specify input size first dense layer.\noutput classifier categorical class value, therefore\nuse output layer softmax activation function, \ntraining specify\ncategorical cross-entropy loss function, accuracy\nadditional metric.\nAlso, L2 regularization used weights hidden layer \nreduce overfitting.model training, need specify batch size number \ntraining epochs. fitting process can also use fraction \ntraining data validation provide generalization loss/accuracy.create predictions model, convert one-hot\nencoding back class labels.","code":"\nlibrary(keras)\nX <- Zoo_train |> \n  select(!type) |> \n  mutate(across(everything(), as.integer)) |> \n  as.matrix()\nhead(X)##      hair feathers eggs milk airborne aquatic predator\n## [1,]    1        0    0    1        0       0        1\n## [2,]    0        0    1    0        0       1        1\n## [3,]    1        0    0    1        0       0        1\n## [4,]    1        0    0    1        0       0        1\n## [5,]    1        0    0    1        0       0        0\n## [6,]    1        0    0    1        0       0        0\n##      toothed backbone breathes venomous fins legs tail\n## [1,]       1        1        1        0    0    4    0\n## [2,]       1        1        0        0    1    0    1\n## [3,]       1        1        1        0    0    4    0\n## [4,]       1        1        1        0    0    4    1\n## [5,]       1        1        1        0    0    4    1\n## [6,]       1        1        1        0    0    4    1\n##      domestic catsize\n## [1,]        0       1\n## [2,]        0       0\n## [3,]        0       1\n## [4,]        0       1\n## [5,]        0       1\n## [6,]        1       1\ny <- Zoo_train |> \n  pull(\"type\") |> \n  {\\(x)(as.integer(x) - 1L)}() |>   ## make index start with 0\n  to_categorical()\nhead(y)##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [1,]    1    0    0    0    0    0    0\n## [2,]    0    0    0    1    0    0    0\n## [3,]    1    0    0    0    0    0    0\n## [4,]    1    0    0    0    0    0    0\n## [5,]    1    0    0    0    0    0    0\n## [6,]    1    0    0    0    0    0    0\nX_test <- Zoo_test |> \n  select(!type) |> \n  mutate(across(everything(), as.integer)) |> \n  as.matrix()\ny_test <- Zoo_test |> \n  pull(\"type\") |> \n  {\\(x)(as.integer(x) - 1L)}() |>\n  to_categorical()\nmodel <- keras_model_sequential(input_shape = c(ncol(X)), \n              name = \"single_hidden_layer_classifier\") |>\n  layer_dense(units = 10, activation = 'relu', \n              kernel_regularizer=regularizer_l2(l=0.01), name = \"hidden1\") |>\n  layer_dense(units = ncol(y), activation = 'softmax', name = \"output\")\n\nmodel## Model: \"single_hidden_layer_classifier\"\n## _______________________________________________________\n##  Layer (type)           Output Shape          Param #  \n## =======================================================\n##  hidden1 (Dense)        (None, 10)            170      \n##  output (Dense)         (None, 7)             77       \n## =======================================================\n## Total params: 247\n## Trainable params: 247\n## Non-trainable params: 0\n## _______________________________________________________\nmodel <- model |>  \n  compile(loss = 'categorical_crossentropy', \n          optimizer = 'adam', \n          metrics = 'accuracy')\nhistory <- model |>\n  fit(\n    X, \n    y,\n    batch_size = 10,\n    epochs = 100,\n    validation_split = .2\n  )\n\nplot(history)\nclass_labels <- levels(Zoo_train |> pull(type))\n\npr <- predict(model, X_test) |> \n  apply(MARGIN = 1, FUN = which.max)\npr <- factor(pr, labels = class_labels, levels = seq_along(class_labels))\n\npr##  [1] mammal        mollusc.et.al bird         \n##  [4] mammal        insect        bird         \n##  [7] mammal        mollusc.et.al mammal       \n## [10] mammal        bird          bird         \n## [13] fish          mammal        fish         \n## [16] mammal        bird          mammal       \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al\nconfusionMatrix(data = pr, \n                reference = Zoo_test$type)## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian\n##   mammal             8    0       0    0         0\n##   bird               0    4       1    0         0\n##   reptile            0    0       0    0         0\n##   fish               0    0       0    2         0\n##   amphibian          0    0       0    0         0\n##   insect             0    0       0    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                Reference\n## Prediction      insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             1             0\n##   mollusc.et.al      0             2\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.944         \n##                  95% CI : (0.727, 0.999)\n##     No Information Rate : 0.444         \n##     P-Value [Acc > NIR] : 1.08e-05      \n##                                         \n##                   Kappa : 0.922         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       0.929\n## Pos Pred Value               1.000       0.800\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.444       0.222\n## Detection Rate               0.444       0.222\n## Detection Prevalence         0.444       0.278\n## Balanced Accuracy            1.000       0.964\n##                      Class: reptile Class: fish\n## Sensitivity                  0.0000       1.000\n## Specificity                  1.0000       1.000\n## Pos Pred Value                  NaN       1.000\n## Neg Pred Value               0.9444       1.000\n## Prevalence                   0.0556       0.111\n## Detection Rate               0.0000       0.111\n## Detection Prevalence         0.0000       0.111\n## Balanced Accuracy            0.5000       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                        NA        1.0000\n## Specificity                         1        1.0000\n## Pos Pred Value                     NA        1.0000\n## Neg Pred Value                     NA        1.0000\n## Prevalence                          0        0.0556\n## Detection Rate                      0        0.0556\n## Detection Prevalence                0        0.0556\n## Balanced Accuracy                  NA        1.0000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-alternative-techniques.html","id":"comparing-decision-boundaries-of-popular-classification-techniques","chapter":"4 Classification: Alternative Techniques","heading":"4.7 Comparing Decision Boundaries of Popular Classification Techniques","text":"Classifiers create decision boundaries discriminate classes.\nDifferent classifiers able create different shapes decision\nboundaries (e.g., strictly linear) thus classifiers\nmay perform better certain datasets. page visualizes \ndecision boundaries found several popular classification methods.following plot adds decision boundary (black lines) \nclassification confidence (color intensity) evaluating classifier\nevenly spaced grid points. Note low resolution (make\nevaluation faster) make decision boundary look like \nsmall steps even (straight) line.","code":"\nlibrary(scales)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\n\ndecisionplot <- function(model, data, class_var, \n  predict_type = c(\"class\", \"prob\"), resolution = 3 * 72) {\n  # resolution is set to 72 dpi if the image is rendered  3 inches wide. \n  \n  y <- data |> pull(class_var)\n  x <- data |> dplyr::select(-all_of(class_var))\n  \n  # resubstitution accuracy\n  prediction <- predict(model, x, type = predict_type[1])\n  # LDA returns a list\n  if(is.list(prediction)) prediction <- prediction$class\n  prediction <- factor(prediction, levels = levels(y))\n  \n  cm <- confusionMatrix(data = prediction, \n                        reference = y)\n  acc <- cm$overall[\"Accuracy\"]\n  \n  # evaluate model on a grid\n  r <- sapply(x[, 1:2], range, na.rm = TRUE)\n  xs <- seq(r[1,1], r[2,1], length.out = resolution)\n  ys <- seq(r[1,2], r[2,2], length.out = resolution)\n  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))\n  colnames(g) <- colnames(r)\n  g <- as_tibble(g)\n  \n  ### guess how to get class labels from predict\n  ### (unfortunately not very consistent between models)\n  cl <- predict(model, g, type = predict_type[1])\n  \n  # LDA returns a list\n  prob <- NULL\n  if(is.list(cl)) { \n    prob <- cl$posterior\n    cl <- cl$class\n  } else\n    if(!is.na(predict_type[2]))\n      try(prob <- predict(model, g, type = predict_type[2]))\n  \n  # we visualize the difference in probability/score between the \n  # winning class and the second best class.\n  # don't use probability if predict for the classifier does not support it.\n  max_prob <- 1\n  if(!is.null(prob))\n    try({\n      max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))\n      max_prob <- max_prob[,1] - max_prob[,2]\n    }, silent = TRUE) \n  \n  cl <- factor(cl, levels = levels(y))\n  \n  g <- g |> add_column(prediction = cl, probability = max_prob)\n  \n  ggplot(g, mapping = aes(\n    x = .data[[colnames(g)[1]]], y = .data[[colnames(g)[2]]])) +\n    geom_raster(mapping = aes(fill = prediction, alpha = probability)) +\n    geom_contour(mapping = aes(z = as.numeric(prediction)), \n      bins = length(levels(cl)), size = .5, color = \"black\") +\n    geom_point(data = data, mapping =  aes(\n      x = .data[[colnames(data)[1]]], \n      y = .data[[colnames(data)[2]]],\n      shape = .data[[class_var]]), alpha = .7) + \n    scale_alpha_continuous(range = c(0,1), limits = c(0,1), guide = \"none\") +  \n    labs(subtitle = paste(\"Training accuracy:\", round(acc, 2)))\n}"},{"path":"classification-alternative-techniques.html","id":"iris-dataset","chapter":"4 Classification: Alternative Techniques","heading":"4.7.1 Iris Dataset","text":"easier visualization, use two dimensions Iris dataset.\nContour lines visualize density like mountains map.Note: overplotting use geom_jitter()\ninstead geom_point().","code":"\nset.seed(1000)\ndata(iris)\niris <- as_tibble(iris)\n\n### Three classes \n### (note: MASS also has a select function which hides dplyr's select)\nx <- iris |> dplyr::select(Sepal.Length, Sepal.Width, Species)\nx## # A tibble: 150 × 3\n##    Sepal.Length Sepal.Width Species\n##           <dbl>       <dbl> <fct>  \n##  1          5.1         3.5 setosa \n##  2          4.9         3   setosa \n##  3          4.7         3.2 setosa \n##  4          4.6         3.1 setosa \n##  5          5           3.6 setosa \n##  6          5.4         3.9 setosa \n##  7          4.6         3.4 setosa \n##  8          5           3.4 setosa \n##  9          4.4         2.9 setosa \n## 10          4.9         3.1 setosa \n## # ℹ 140 more rows\nggplot(x, aes(x = Sepal.Length, y = Sepal.Width, fill = Species)) +  \n  stat_density_2d(geom = \"polygon\", aes(alpha = ..level..)) +\n  geom_point()## Warning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0.\n## ℹ Please use `after_stat(level)` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated."},{"path":"classification-alternative-techniques.html","id":"k-nearest-neighbors-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.7.1.1 K-Nearest Neighbors Classifier","text":"Increasing \\(k\\) smooths decision boundary. \\(k=1\\), see white\nareas around points flowers two classes spot.\n, algorithm randomly chooses class prediction resulting\nmeandering decision boundary. predictions area \nstable every time ask class, may get different\nclass.","code":"\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 1)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (1 neighbor)\")## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 3)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (3 neighbor)\")\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 9)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (9 neighbor)\")"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.7.1.2 Naive Bayes Classifier","text":"","code":"\nmodel <- x |> e1071::naiveBayes(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"Naive Bayes\")"},{"path":"classification-alternative-techniques.html","id":"linear-discriminant-analysis","chapter":"4 Classification: Alternative Techniques","heading":"4.7.1.3 Linear Discriminant Analysis","text":"","code":"\nmodel <- x |> MASS::lda(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"LDA\")"},{"path":"classification-alternative-techniques.html","id":"multinomial-logistic-regression-implemented-in-nnet","chapter":"4 Classification: Alternative Techniques","heading":"4.7.1.4 Multinomial Logistic Regression (implemented in nnet)","text":"Multinomial logistic regression extension logistic regression\nproblems two classes.","code":"\nmodel <- x |> nnet::multinom(Species ~., data = _)## # weights:  12 (6 variable)\n## initial  value 164.791843 \n## iter  10 value 62.715967\n## iter  20 value 59.808291\n## iter  30 value 55.445984\n## iter  40 value 55.375704\n## iter  50 value 55.346472\n## iter  60 value 55.301707\n## iter  70 value 55.253532\n## iter  80 value 55.243230\n## iter  90 value 55.230241\n## iter 100 value 55.212479\n## final  value 55.212479 \n## stopped after 100 iterations\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(titel = \"Multinomial Logistic Regression\")"},{"path":"classification-alternative-techniques.html","id":"decision-trees-1","chapter":"4 Classification: Alternative Techniques","heading":"4.7.1.5 Decision Trees","text":"","code":"\nmodel <- x |> rpart::rpart(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"CART\")\nmodel <- x |> rpart::rpart(Species ~ ., data = _,\n  control = rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"CART (overfitting)\")\nmodel <- x |> C50::C5.0(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"C5.0\")\nmodel <- x |> randomForest::randomForest(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"Random Forest\")"},{"path":"classification-alternative-techniques.html","id":"svm","chapter":"4 Classification: Alternative Techniques","heading":"4.7.1.6 SVM","text":"","code":"\nmodel <- x |> e1071::svm(Species ~ ., data = _, kernel = \"linear\")\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (linear kernel)\")\nmodel <- x |> e1071::svm(Species ~ ., data = _, kernel = \"radial\")\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (radial kernel)\")\nmodel <- x |> e1071::svm(Species ~ ., data = _, kernel = \"polynomial\")\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (polynomial kernel)\")\nmodel <- x |> e1071::svm(Species ~ ., data = _, kernel = \"sigmoid\")\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (sigmoid kernel)\")"},{"path":"classification-alternative-techniques.html","id":"single-layer-feed-forward-neural-networks","chapter":"4 Classification: Alternative Techniques","heading":"4.7.1.7 Single Layer Feed-forward Neural Networks","text":"","code":"\nmodel <-x |> nnet::nnet(Species ~ ., data = _, size = 1, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (1 neuron)\")\nmodel <-x |> nnet::nnet(Species ~ ., data = _, size = 2, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (2 neurons)\")\nmodel <-x |> nnet::nnet(Species ~ ., data = _, size = 4, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (4 neurons)\")\nmodel <-x |> nnet::nnet(Species ~ ., data = _, size = 10, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (10 neurons)\")"},{"path":"classification-alternative-techniques.html","id":"circle-dataset","chapter":"4 Classification: Alternative Techniques","heading":"4.7.2 Circle Dataset","text":"set linearly separable!","code":"\nset.seed(1000)\n\nx <- mlbench::mlbench.circle(500)\n###x <- mlbench::mlbench.cassini(500)\n###x <- mlbench::mlbench.spirals(500, sd = .1)\n###x <- mlbench::mlbench.smiley(500)\nx <- cbind(as.data.frame(x$x), factor(x$classes))\ncolnames(x) <- c(\"x\", \"y\", \"class\")\nx <- as_tibble(x)\nx## # A tibble: 500 × 3\n##          x       y class\n##      <dbl>   <dbl> <fct>\n##  1 -0.344   0.448  1    \n##  2  0.518   0.915  2    \n##  3 -0.772  -0.0913 1    \n##  4  0.382   0.412  1    \n##  5  0.0328  0.438  1    \n##  6 -0.865  -0.354  2    \n##  7  0.477   0.640  2    \n##  8  0.167  -0.809  2    \n##  9 -0.568  -0.281  1    \n## 10 -0.488   0.638  2    \n## # ℹ 490 more rows\nggplot(x, aes(x = x, y = y, color = class)) + \n  geom_point()"},{"path":"classification-alternative-techniques.html","id":"k-nearest-neighbors-classifier-1","chapter":"4 Classification: Alternative Techniques","heading":"4.7.2.1 K-Nearest Neighbors Classifier","text":"","code":"\nmodel <- x |> caret::knn3(class ~ ., data = _, k = 1)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"kNN (1 neighbor)\")\nmodel <- x |> caret::knn3(class ~ ., data = _, k = 10)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"kNN (10 neighbor)\")"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier-1","chapter":"4 Classification: Alternative Techniques","heading":"4.7.2.2 Naive Bayes Classifier","text":"","code":"\nmodel <- x |> e1071::naiveBayes(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"naive Bayes\")"},{"path":"classification-alternative-techniques.html","id":"linear-discriminant-analysis-1","chapter":"4 Classification: Alternative Techniques","heading":"4.7.2.3 Linear Discriminant Analysis","text":"LDA find good model since true decision boundary \nlinear.","code":"\nmodel <- x |> MASS::lda(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + labs(title = \"LDA\")"},{"path":"classification-alternative-techniques.html","id":"logistic-regression-implemented-in-nnet","chapter":"4 Classification: Alternative Techniques","heading":"4.7.2.4 Logistic Regression (implemented in nnet)","text":"Multinomial logistic regression extension logistic regression\nproblems two classes. also tries find linear\ndecision boundary.","code":"\nmodel <- x |> nnet::multinom(class ~., data = _)## # weights:  4 (3 variable)\n## initial  value 346.573590 \n## final  value 346.308371 \n## converged\ndecisionplot(model, x, class_var = \"class\") + \n  labs(titel = \"Multinomial Logistic Regression\")"},{"path":"classification-alternative-techniques.html","id":"decision-trees-2","chapter":"4 Classification: Alternative Techniques","heading":"4.7.2.5 Decision Trees","text":"","code":"\nmodel <- x |> rpart::rpart(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"CART\")\nmodel <- x |> rpart::rpart(class ~ ., data = _,\n  control = rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"CART (overfitting)\")\nmodel <- x |> C50::C5.0(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"C5.0\")\nlibrary(randomForest)## randomForest 4.7-1.1## Type rfNews() to see new features/changes/bug fixes.## \n## Attaching package: 'randomForest'## The following object is masked from 'package:dplyr':\n## \n##     combine## The following object is masked from 'package:ggplot2':\n## \n##     margin\nmodel <- x |> randomForest(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"Random Forest\")"},{"path":"classification-alternative-techniques.html","id":"svm-1","chapter":"4 Classification: Alternative Techniques","heading":"4.7.2.6 SVM","text":"Linear SVM work data.","code":"\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"linear\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (linear kernel)\")## Warning: Computation failed in `stat_contour()`\n## Caused by error in `if (zero_range(range)) ...`:\n## ! missing value where TRUE/FALSE needed\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"radial\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (radial kernel)\")\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"polynomial\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (polynomial kernel)\")\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"sigmoid\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (sigmoid kernel)\")"},{"path":"classification-alternative-techniques.html","id":"single-layer-feed-forward-neural-networks-1","chapter":"4 Classification: Alternative Techniques","heading":"4.7.2.7 Single Layer Feed-forward Neural Networks","text":"","code":"\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 1, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + labs(title = \"NN (1 neuron)\")\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 2, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + labs(title = \"NN (2 neurons)\")\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 4, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + labs(title = \"NN (4 neurons)\")\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 10, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + labs(title = \"NN (10 neurons)\")"},{"path":"classification-alternative-techniques.html","id":"more-information-on-classification-with-r","chapter":"4 Classification: Alternative Techniques","heading":"4.8 More Information on Classification with R","text":"Package caret: http://topepo.github.io/caret/index.htmlTidymodels (machine learning tidyverse):\nhttps://www.tidymodels.org/R taskview machine learning:\nhttp://cran.r-project.org/web/views/MachineLearning.html","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"association-analysis-basic-concepts-and-algorithms","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5 Association Analysis: Basic Concepts and Algorithms","text":"Install packages used chapter:packages used chapter : arules (Hahsler et al. 2023), arulesViz (Hahsler 2023), mlbench (Leisch Dimitriadou. 2023), tidyverse (Wickham 2023b)","code":"\npkgs <- sort(c('tidyverse', 'arules', 'arulesViz', 'mlbench'))\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"introduciton","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.1 Introduciton","text":"Association rule mining\nplays vital role discovering hidden patterns relationships within large\ntransactional datasets. Applications range exploratory data analysis marketing building rule-based\nclassifiers.\nAgrawal, Imielinski, Swami (1993) introduced problem\nmining association rules transaction data follows (definition taken Hahsler, Grün, Hornik (2005)):Let \\(= \\{i_1,i_2,...,i_n\\}\\) set \\(n\\) binary attributes called items. Let \\(D = \\{t_1,t_2,...,t_m\\}\\) \nset transactions called database. transaction \\(D\\) unique transaction ID \ncontains subset items \\(\\). rule defined implication form \\(X \\Rightarrow Y\\) \n\\(X,Y \\subseteq \\) \\(X \\cap Y = \\emptyset\\) called itemsets. itemsets rules several quality measures can\ndefined. important measures support confidence. support \\(supp(X)\\) \nitemset \\(X\\) defined proportion transactions data set contain itemset.\nItemsets support surpasses user-defined threshold \\(\\sigma\\) called frequent itemsets. \nconfidence rule defined \\(conf(X \\Rightarrow Y) = supp(X \\cup Y)/supp(X)\\). Association rules rules\n\\(supp(X \\cup Y) \\ge \\sigma\\) \\(conf(X) \\ge \\delta\\) \\(\\sigma\\) \\(\\delta\\) user-defined thresholds.\nfound set association rules used reason data.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 5. Association Analysis: Basic Concepts \nAlgorithms","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"the-arules-package","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.2 The arules Package","text":"Association rule mining R implemented package arules.information arules package try: help(package=\"arules\")\nvignette(\"arules\") (also available \nCRAN)arules uses S4 object system implement classes methods.\nStandard R objects use S3 object\nsystem use formal class\ndefinitions usually implemented list class\nattribute. arules many R packages use S4 object\nsystem based formal class\ndefinitions member variables methods (similar \nobject-oriented programming languages like Java C++). important\ndifferences using S4 objects compared usual S3 objects :coercion (casting): (, \"class_name\")help classes: class? class_name","code":"\nlibrary(tidyverse)\nlibrary(arules)\nlibrary(arulesViz)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3 Transactions","text":"","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"create-transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3.1 Create Transactions","text":"use Zoo dataset mlbench.data data.frame need converted set \ntransactions row represents transaction column \ntranslated items. done using constructor\ntransactions(). Zoo data set means consider\nanimals transactions different traits (features) become\nitems animal . example animal antelope \nitem hair transaction.conversion gives warning discrete features (factor\nlogical) can directly translated items. Continuous\nfeatures need discretized first.column 13?Possible solution: Make legs /legsAlternatives:use unique value item:discretize (see\n? discretize\ndiscretization code Chapter\n2):Convert data set transactions","code":"\ndata(Zoo, package = \"mlbench\")\nhead(Zoo)##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n## bear      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## boar      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## buffalo   TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n##          predator toothed backbone breathes venomous\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE\n## bear         TRUE    TRUE     TRUE     TRUE    FALSE\n## boar         TRUE    TRUE     TRUE     TRUE    FALSE\n## buffalo     FALSE    TRUE     TRUE     TRUE    FALSE\n##           fins legs  tail domestic catsize   type\n## aardvark FALSE    4 FALSE    FALSE    TRUE mammal\n## antelope FALSE    4  TRUE    FALSE    TRUE mammal\n## bass      TRUE    0  TRUE    FALSE   FALSE   fish\n## bear     FALSE    4 FALSE    FALSE    TRUE mammal\n## boar     FALSE    4  TRUE    FALSE    TRUE mammal\n## buffalo  FALSE    4  TRUE    FALSE    TRUE mammal\ntrans <- transactions(Zoo)## Warning: Column(s) 13 not logical or factor. Applying\n## default discretization (see '? discretizeDF').\nsummary(Zoo[13])##       legs     \n##  Min.   :0.00  \n##  1st Qu.:2.00  \n##  Median :4.00  \n##  Mean   :2.84  \n##  3rd Qu.:4.00  \n##  Max.   :8.00\nggplot(Zoo, aes(legs)) + geom_bar()\nZoo$legs |> table()## \n##  0  2  4  5  6  8 \n## 23 27 38  1 10  2\nZoo_has_legs <- Zoo |> mutate(legs = legs > 0)\nggplot(Zoo_has_legs, aes(legs)) + geom_bar()\nZoo_has_legs$legs |> table()## \n## FALSE  TRUE \n##    23    78\nZoo_unique_leg_values <- Zoo |> mutate(legs = factor(legs))\nZoo_unique_leg_values$legs |> head()## [1] 4 4 0 4 4 4\n## Levels: 0 2 4 5 6 8\nZoo_discretized_legs <- Zoo |> mutate(\n  legs = discretize(legs, breaks = 2, method=\"interval\")\n)\ntable(Zoo_discretized_legs$legs)## \n## [0,4) [4,8] \n##    50    51\ntrans <- transactions(Zoo_has_legs)\ntrans## transactions in sparse format with\n##  101 transactions (rows) and\n##  23 items (columns)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"inspect-transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3.2 Inspect Transactions","text":"Look created items. still called column names since \ntransactions actually stored large sparse logical matrix (see\n).Compare original features (column names) ZooLook (first) transactions matrix. 1 indicates presence\nitem.Look transactions sets itemsPlot binary matrix. Dark dots represent 1s.Look relative frequency (=support) items data set. \nlook 10 frequent items.Alternative encoding: Also create items FALSE (use factor)","code":"\nsummary(trans)## transactions as itemMatrix in sparse format with\n##  101 rows (elements/itemsets/transactions) and\n##  23 columns (items) and a density of 0.361 \n## \n## most frequent items:\n## backbone breathes     legs     tail  toothed  (Other) \n##       83       80       78       75       61      462 \n## \n## element (itemset/transaction) length distribution:\n## sizes\n##  3  4  5  6  7  8  9 10 11 12 \n##  3  2  6  5  8 21 27 25  3  1 \n## \n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    3.00    8.00    9.00    8.31   10.00   12.00 \n## \n## includes extended item information - examples:\n##     labels variables levels\n## 1     hair      hair   TRUE\n## 2 feathers  feathers   TRUE\n## 3     eggs      eggs   TRUE\n## \n## includes extended transaction information - examples:\n##   transactionID\n## 1      aardvark\n## 2      antelope\n## 3          bass\ncolnames(trans)##  [1] \"hair\"               \"feathers\"          \n##  [3] \"eggs\"               \"milk\"              \n##  [5] \"airborne\"           \"aquatic\"           \n##  [7] \"predator\"           \"toothed\"           \n##  [9] \"backbone\"           \"breathes\"          \n## [11] \"venomous\"           \"fins\"              \n## [13] \"legs\"               \"tail\"              \n## [15] \"domestic\"           \"catsize\"           \n## [17] \"type=mammal\"        \"type=bird\"         \n## [19] \"type=reptile\"       \"type=fish\"         \n## [21] \"type=amphibian\"     \"type=insect\"       \n## [23] \"type=mollusc.et.al\"\ncolnames(Zoo)##  [1] \"hair\"     \"feathers\" \"eggs\"     \"milk\"    \n##  [5] \"airborne\" \"aquatic\"  \"predator\" \"toothed\" \n##  [9] \"backbone\" \"breathes\" \"venomous\" \"fins\"    \n## [13] \"legs\"     \"tail\"     \"domestic\" \"catsize\" \n## [17] \"type\"\nas(trans, \"matrix\")[1:3,]##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n##          predator toothed backbone breathes venomous\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE\n##           fins  legs  tail domestic catsize\n## aardvark FALSE  TRUE FALSE    FALSE    TRUE\n## antelope FALSE  TRUE  TRUE    FALSE    TRUE\n## bass      TRUE FALSE  TRUE    FALSE   FALSE\n##          type=mammal type=bird type=reptile type=fish\n## aardvark        TRUE     FALSE        FALSE     FALSE\n## antelope        TRUE     FALSE        FALSE     FALSE\n## bass           FALSE     FALSE        FALSE      TRUE\n##          type=amphibian type=insect type=mollusc.et.al\n## aardvark          FALSE       FALSE              FALSE\n## antelope          FALSE       FALSE              FALSE\n## bass              FALSE       FALSE              FALSE\ninspect(trans[1:3])##     items         transactionID\n## [1] {hair,                     \n##      milk,                     \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      catsize,                  \n##      type=mammal}      aardvark\n## [2] {hair,                     \n##      milk,                     \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      tail,                     \n##      catsize,                  \n##      type=mammal}      antelope\n## [3] {eggs,                     \n##      aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      type=fish}        bass\nimage(trans)\nitemFrequencyPlot(trans,topN = 20)\nggplot(\n  tibble(\n    Support = sort(itemFrequency(trans, type = \"absolute\"), decreasing = TRUE),\n    Item = seq_len(ncol(trans))\n  ), aes(x = Item, y = Support)) + geom_line()\nsapply(Zoo_has_legs, class)##      hair  feathers      eggs      milk  airborne \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##   aquatic  predator   toothed  backbone  breathes \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##  venomous      fins      legs      tail  domestic \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##   catsize      type \n## \"logical\"  \"factor\"\nZoo_factors <- Zoo_has_legs |> mutate(across(where(is.logical), factor))\nsapply(Zoo_factors, class)##     hair feathers     eggs     milk airborne  aquatic \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \n## predator  toothed backbone breathes venomous     fins \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \n##     legs     tail domestic  catsize     type \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\"\nsummary(Zoo_factors)##     hair     feathers     eggs       milk   \n##  FALSE:58   FALSE:81   FALSE:42   FALSE:60  \n##  TRUE :43   TRUE :20   TRUE :59   TRUE :41  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##   airborne   aquatic    predator   toothed  \n##  FALSE:77   FALSE:65   FALSE:45   FALSE:40  \n##  TRUE :24   TRUE :36   TRUE :56   TRUE :61  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##   backbone   breathes   venomous     fins   \n##  FALSE:18   FALSE:21   FALSE:93   FALSE:84  \n##  TRUE :83   TRUE :80   TRUE : 8   TRUE :17  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##     legs       tail     domestic   catsize  \n##  FALSE:23   FALSE:26   FALSE:88   FALSE:57  \n##  TRUE :78   TRUE :75   TRUE :13   TRUE :44  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##             type   \n##  mammal       :41  \n##  bird         :20  \n##  reptile      : 5  \n##  fish         :13  \n##  amphibian    : 4  \n##  insect       : 8  \n##  mollusc.et.al:10\ntrans_factors <- transactions(Zoo_factors)\ntrans_factors## transactions in sparse format with\n##  101 transactions (rows) and\n##  39 items (columns)\nitemFrequencyPlot(trans_factors, topN = 20)\n## Select transactions that contain a certain item\ntrans_insects <- trans_factors[trans %in% \"type=insect\"]\ntrans_insects## transactions in sparse format with\n##  8 transactions (rows) and\n##  39 items (columns)\ninspect(trans_insects)##     items             transactionID\n## [1] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=FALSE,               \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          flea    \n## [2] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          gnat    \n## [3] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=TRUE,                \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=TRUE,                \n##      catsize=FALSE,                \n##      type=insect}          honeybee\n## [4] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          housefly\n## [5] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=TRUE,                \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          ladybird\n## [6] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          moth    \n## [7] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=FALSE,               \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          termite \n## [8] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=TRUE,                \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          wasp"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"vertical-layout-transaction-id-lists","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3.3 Vertical Layout (Transaction ID Lists)","text":"default layout transactions horizontal layout (.e. \ntransaction row). vertical layout represents transaction data\nlist transaction IDs item (= transaction ID lists).","code":"\nvertical <- as(trans, \"tidLists\")\nas(vertical, \"matrix\")[1:10, 1:5]##          aardvark antelope  bass  bear  boar\n## hair         TRUE     TRUE FALSE  TRUE  TRUE\n## feathers    FALSE    FALSE FALSE FALSE FALSE\n## eggs        FALSE    FALSE  TRUE FALSE FALSE\n## milk         TRUE     TRUE FALSE  TRUE  TRUE\n## airborne    FALSE    FALSE FALSE FALSE FALSE\n## aquatic     FALSE    FALSE  TRUE FALSE FALSE\n## predator     TRUE    FALSE  TRUE  TRUE  TRUE\n## toothed      TRUE     TRUE  TRUE  TRUE  TRUE\n## backbone     TRUE     TRUE  TRUE  TRUE  TRUE\n## breathes     TRUE     TRUE FALSE  TRUE  TRUE"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"frequent-itemsets","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.4 Frequent Itemsets","text":"","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"mine-frequent-itemsets","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.4.1 Mine Frequent Itemsets","text":"dataset already huge number possible itemsetsFind frequent itemsets (target=“frequent”) default settings.Default minimum support .1 (10%). Note: use small\ndata set. larger datasets default minimum support might \nlow may run memory. probably want start \nhigher minimum support like .5 (50%) work way .order find itemsets effect 5 animals need go \nsupport 5%.Sort supportLook frequent itemsets many items (set breaks manually since\nAutomatically chosen breaks look bad)","code":"\n2^ncol(trans)## [1] 8388608\nits <- apriori(trans, parameter=list(target = \"frequent\"))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##          NA    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen            target  ext\n##        5     0.1      1     10 frequent itemsets TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [18 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans, parameter = list(target =\n## \"frequent\")): Mining stopped (maxlen reached). Only\n## patterns up to a length of 10 returned!##  done [0.00s].\n## sorting transactions ... done [0.00s].\n## writing ... [1465 set(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nits## set of 1465 itemsets\n5/nrow(trans)## [1] 0.0495\nits <- apriori(trans, parameter=list(target = \"frequent\", support = 0.05))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##          NA    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen            target  ext\n##        5    0.05      1     10 frequent itemsets TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans, parameter = list(target =\n## \"frequent\", support = 0.05)): Mining stopped (maxlen\n## reached). Only patterns up to a length of 10 returned!##  done [0.00s].\n## sorting transactions ... done [0.00s].\n## writing ... [2537 set(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nits## set of 2537 itemsets\nits <- sort(its, by = \"support\")\nits |> head(n = 10) |> inspect()##      items                      support count\n## [1]  {backbone}                 0.822   83   \n## [2]  {breathes}                 0.792   80   \n## [3]  {legs}                     0.772   78   \n## [4]  {tail}                     0.743   75   \n## [5]  {backbone, tail}           0.733   74   \n## [6]  {breathes, legs}           0.723   73   \n## [7]  {backbone, breathes}       0.683   69   \n## [8]  {backbone, legs}           0.634   64   \n## [9]  {backbone, breathes, legs} 0.634   64   \n## [10] {toothed}                  0.604   61\nggplot(tibble(`Itemset Size` = factor(size(its))), aes(`Itemset Size`)) + \n  geom_bar()\nits[size(its) > 8] |> inspect()##      items         support count\n## [1]  {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.2376    24\n## [2]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       catsize,                  \n##       type=mammal}  0.1584    16\n## [3]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       type=mammal}  0.1485    15\n## [4]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1386    14\n## [5]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [6]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [7]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [8]  {milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [9]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize}      0.1287    13\n## [10] {hair,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [11] {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [12] {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       domestic,                 \n##       catsize,                  \n##       type=mammal}  0.0594     6\n## [13] {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       domestic,                 \n##       type=mammal}  0.0594     6\n## [14] {feathers,                 \n##       eggs,                     \n##       airborne,                 \n##       predator,                 \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       type=bird}    0.0594     6"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"concise-representation-of-itemsets","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.4.2 Concise Representation of Itemsets","text":"Find maximal frequent itemsets (superset frequent)Find closed frequent itemsets (superset frequent)","code":"\nits_max <- its[is.maximal(its)]\nits_max## set of 22 itemsets\nits_max |> head(by = \"support\") |> inspect()##     items         support count\n## [1] {hair,                     \n##      milk,                     \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      tail,                     \n##      catsize,                  \n##      type=mammal}  0.1287    13\n## [2] {eggs,                     \n##      aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      type=fish}    0.0891     9\n## [3] {aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes}     0.0792     8\n## [4] {aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      catsize}      0.0693     7\n## [5] {eggs,                     \n##      venomous}     0.0594     6\n## [6] {predator,                 \n##      venomous}     0.0594     6\nits_closed <- its[is.closed(its)]\nits_closed## set of 230 itemsets\nits_closed |> head(by = \"support\") |> inspect()##     items            support count\n## [1] {backbone}       0.822   83   \n## [2] {breathes}       0.792   80   \n## [3] {legs}           0.772   78   \n## [4] {tail}           0.743   75   \n## [5] {backbone, tail} 0.733   74   \n## [6] {breathes, legs} 0.723   73\ncounts <- c(\n  frequent=length(its),\n  closed=length(its_closed),\n  maximal=length(its_max)\n)\n\nggplot(as_tibble(counts, rownames = \"Itemsets\"),\n  aes(Itemsets, counts)) + geom_bar(stat = \"identity\")"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"association-rules","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5 Association Rules","text":"","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"mine-association-rules","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.1 Mine Association Rules","text":"use APRIORI algorithm (see\n? apriori)Look rules highest liftCreate rules using alternative encoding (“FALSE” item)","code":"\nrules <- apriori(trans, parameter = list(support = 0.05, confidence = 0.9))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##         0.9    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen target  ext\n##        5    0.05      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans, parameter = list(support =\n## 0.05, confidence = 0.9)): Mining stopped (maxlen\n## reached). Only patterns up to a length of 10 returned!##  done [0.00s].\n## writing ... [7174 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nlength(rules)## [1] 7174\nrules |> head() |> inspect()##     lhs                     rhs        support\n## [1] {type=insect}        => {eggs}     0.0792 \n## [2] {type=insect}        => {legs}     0.0792 \n## [3] {type=insect}        => {breathes} 0.0792 \n## [4] {type=mollusc.et.al} => {eggs}     0.0891 \n## [5] {type=fish}          => {fins}     0.1287 \n## [6] {type=fish}          => {aquatic}  0.1287 \n##     confidence coverage lift count\n## [1] 1.0        0.0792   1.71  8   \n## [2] 1.0        0.0792   1.29  8   \n## [3] 1.0        0.0792   1.26  8   \n## [4] 0.9        0.0990   1.54  9   \n## [5] 1.0        0.1287   5.94 13   \n## [6] 1.0        0.1287   2.81 13\nrules |> head() |> quality()##   support confidence coverage lift count\n## 1  0.0792        1.0   0.0792 1.71     8\n## 2  0.0792        1.0   0.0792 1.29     8\n## 3  0.0792        1.0   0.0792 1.26     8\n## 4  0.0891        0.9   0.0990 1.54     9\n## 5  0.1287        1.0   0.1287 5.94    13\n## 6  0.1287        1.0   0.1287 2.81    13\nrules <- sort(rules, by = \"lift\")\nrules |> head(n = 10) |> inspect()##      lhs            rhs         support confidence coverage lift count\n## [1]  {eggs,                                                           \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [2]  {eggs,                                                           \n##       aquatic,                                                        \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [3]  {eggs,                                                           \n##       predator,                                                       \n##       fins}      => {type=fish}  0.0891          1   0.0891 7.77     9\n## [4]  {eggs,                                                           \n##       toothed,                                                        \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [5]  {eggs,                                                           \n##       fins,                                                           \n##       tail}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [6]  {eggs,                                                           \n##       backbone,                                                       \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [7]  {eggs,                                                           \n##       aquatic,                                                        \n##       predator,                                                       \n##       fins}      => {type=fish}  0.0891          1   0.0891 7.77     9\n## [8]  {eggs,                                                           \n##       aquatic,                                                        \n##       toothed,                                                        \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [9]  {eggs,                                                           \n##       aquatic,                                                        \n##       fins,                                                           \n##       tail}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [10] {eggs,                                                           \n##       aquatic,                                                        \n##       backbone,                                                       \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\nr <- apriori(trans_factors)## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##         0.8    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen target  ext\n##        5     0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[39 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [34 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans_factors): Mining stopped\n## (maxlen reached). Only patterns up to a length of 10\n## returned!##  done [0.08s].\n## writing ... [1517191 rule(s)] done [0.24s].\n## creating S4 object  ... done [0.79s].\nr## set of 1517191 rules\nprint(object.size(r), unit = \"Mb\")## 110.2 Mb\ninspect(r[1:10])##      lhs                rhs              support\n## [1]  {}              => {feathers=FALSE} 0.802  \n## [2]  {}              => {backbone=TRUE}  0.822  \n## [3]  {}              => {fins=FALSE}     0.832  \n## [4]  {}              => {domestic=FALSE} 0.871  \n## [5]  {}              => {venomous=FALSE} 0.921  \n## [6]  {domestic=TRUE} => {predator=FALSE} 0.109  \n## [7]  {domestic=TRUE} => {aquatic=FALSE}  0.119  \n## [8]  {domestic=TRUE} => {legs=TRUE}      0.119  \n## [9]  {domestic=TRUE} => {breathes=TRUE}  0.119  \n## [10] {domestic=TRUE} => {backbone=TRUE}  0.119  \n##      confidence coverage lift count\n## [1]  0.802      1.000    1.00 81   \n## [2]  0.822      1.000    1.00 83   \n## [3]  0.832      1.000    1.00 84   \n## [4]  0.871      1.000    1.00 88   \n## [5]  0.921      1.000    1.00 93   \n## [6]  0.846      0.129    1.90 11   \n## [7]  0.923      0.129    1.43 12   \n## [8]  0.923      0.129    1.20 12   \n## [9]  0.923      0.129    1.17 12   \n## [10] 0.923      0.129    1.12 12\nr |> head(n = 10, by = \"lift\") |> inspect()##      lhs                  rhs         support confidence coverage lift count\n## [1]  {breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [2]  {eggs=TRUE,                                                            \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [3]  {milk=FALSE,                                                           \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [4]  {breathes=FALSE,                                                       \n##       fins=TRUE,                                                            \n##       legs=FALSE}      => {type=fish}   0.129          1    0.129 7.77    13\n## [5]  {aquatic=TRUE,                                                         \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [6]  {hair=FALSE,                                                           \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [7]  {eggs=TRUE,                                                            \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [8]  {milk=FALSE,                                                           \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [9]  {toothed=TRUE,                                                         \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [10] {breathes=FALSE,                                                       \n##       fins=TRUE,                                                            \n##       tail=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"calculate-additional-interest-measures","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.2 Calculate Additional Interest Measures","text":"Add measures rulesFind rules score high Phi correlation","code":"\ninterestMeasure(rules[1:10], measure = c(\"phi\", \"gini\"),\n  trans = trans)##      phi  gini\n## 1  1.000 0.224\n## 2  1.000 0.224\n## 3  0.814 0.149\n## 4  1.000 0.224\n## 5  1.000 0.224\n## 6  1.000 0.224\n## 7  0.814 0.149\n## 8  1.000 0.224\n## 9  1.000 0.224\n## 10 1.000 0.224\nquality(rules) <- cbind(quality(rules),\n  interestMeasure(rules, measure = c(\"phi\", \"gini\"),\n    trans = trans))\nrules |> head(by = \"phi\") |> inspect()##     lhs            rhs         support confidence coverage lift count phi  gini\n## [1] {eggs,                                                                     \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [2] {eggs,                                                                     \n##      aquatic,                                                                  \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [3] {eggs,                                                                     \n##      toothed,                                                                  \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [4] {eggs,                                                                     \n##      fins,                                                                     \n##      tail}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [5] {eggs,                                                                     \n##      backbone,                                                                 \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [6] {eggs,                                                                     \n##      aquatic,                                                                  \n##      toothed,                                                                  \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"mine-using-templates","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.3 Mine Using Templates","text":"Sometimes beneficial specify items \nrule. apriori can use parameter appearance specify \n(see\n? APappearance).\nfollowing restrict rules animal type RHS \nitem LHS.Saving rules CSV-file opened Excel tools.write(rules, file = \"rules.csv\", quote = TRUE)","code":"\ntype <- grep(\"type=\", itemLabels(trans), value = TRUE)\ntype## [1] \"type=mammal\"        \"type=bird\"         \n## [3] \"type=reptile\"       \"type=fish\"         \n## [5] \"type=amphibian\"     \"type=insect\"       \n## [7] \"type=mollusc.et.al\"\nrules_type <- apriori(trans, appearance= list(rhs = type))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##         0.8    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen target  ext\n##        5     0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[7 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [18 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans, appearance = list(rhs =\n## type)): Mining stopped (maxlen reached). Only patterns\n## up to a length of 10 returned!##  done [0.00s].\n## writing ... [571 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nrules_type |> sort(by = \"lift\") |> head() |> inspect()##     lhs            rhs         support confidence coverage lift count\n## [1] {eggs,                                                           \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13\n## [2] {eggs,                                                           \n##      aquatic,                                                        \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13\n## [3] {eggs,                                                           \n##      toothed,                                                        \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13\n## [4] {eggs,                                                           \n##      fins,                                                           \n##      tail}      => {type=fish}   0.129          1    0.129 7.77    13\n## [5] {eggs,                                                           \n##      backbone,                                                       \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13\n## [6] {eggs,                                                           \n##      aquatic,                                                        \n##      toothed,                                                        \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"association-rule-visualization","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.6 Association Rule Visualization","text":"Default scatterplotNote jitter (randomly move points) added show many\nrules confidence support value. Without jitter:Grouped plotAs graph","code":"\nlibrary(arulesViz)\nplot(rules)## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\nplot(rules, control = list(jitter = 0))\nplot(rules, shading = \"order\")## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n##plot(rules, interactive = TRUE)\nplot(rules, method = \"grouped\")\n##plot(rules, method = \"grouped\", engine = \"interactive\")\nplot(rules, method = \"graph\")## Warning: Too many rules supplied. Only plotting the\n## best 100 using 'lift' (change control parameter max if\n## needed).\nplot(rules |> head(by = \"phi\", n = 100), method = \"graph\")"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-visualizations","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.7 Interactive Visualizations","text":"use association rules mined Iris dataset \nfollowing examples.Convert data transactions. Note features numeric \nneed discretized. conversion automatically applies\nfrequency-based discretization 3 classes numeric feature\n(warning).Next, mine association rules.","code":"\ndata(iris)\nsummary(iris)##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.35  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width        Species  \n##  Min.   :0.1   setosa    :50  \n##  1st Qu.:0.3   versicolor:50  \n##  Median :1.3   virginica :50  \n##  Mean   :1.2                  \n##  3rd Qu.:1.8                  \n##  Max.   :2.5\niris_trans <- transactions(iris)## Warning: Column(s) 1, 2, 3, 4 not logical or factor.\n## Applying default discretization (see '?\n## discretizeDF').\niris_trans |> head() |> inspect()##     items                      transactionID\n## [1] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       1\n## [2] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[2.9,3.2),                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       2\n## [3] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       3\n## [4] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[2.9,3.2),                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       4\n## [5] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       5\n## [6] {Sepal.Length=[5.4,6.3),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       6\nrules <- apriori(iris_trans, parameter = list(support = 0.1, confidence = 0.8))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##         0.8    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen target  ext\n##        5     0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 15 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[15 item(s), 150 transaction(s)] done [0.00s].\n## sorting and recoding items ... [15 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 done [0.00s].\n## writing ... [144 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nrules## set of 144 rules"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-inspect-with-sorting-filtering-and-paging","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.7.1 Interactive Inspect With Sorting, Filtering and Paging","text":"","code":"\ninspectDT(rules)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"scatter-plot-1","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.7.2 Scatter Plot","text":"Plot rules scatter plot using interactive html widget. avoid\noverplotting, jitter added automatically. Set jitter = 0 disable\njitter. Hovering rules shows rule information. Note:\nplotly/javascript well many points, plot selects\ntop 1000 rules warning rules supplied.","code":"\nplot(rules, engine = \"html\")## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter."},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"matrix-visualization","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.7.3 Matrix Visualization","text":"Plot rules matrix using interactive html widget.","code":"\nplot(rules, method = \"matrix\", engine = \"html\") "},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"visualization-as-graph","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.7.4 Visualization as Graph","text":"Plot rules graph using interactive html widget. Note: used\njavascript library well many graph nodes, plot\nselects top 100 rules (warning).","code":"\nplot(rules, method = \"graph\", engine = \"html\")## Warning: Too many rules supplied. Only plotting the\n## best 100 using 'lift' (change control parameter max if\n## needed)."},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-rule-explorer","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.7.5 Interactive Rule Explorer","text":"can specify rule set dataset. explore rules can \nmined iris, use: ruleExplorer(iris)rule explorer creates interactive Shiny application can \nused locally deployed server sharing. deployed version \nruleExplorer available\n(using\nshinyapps.io).","code":""},{"path":"association-analysis-advanced-concepts.html","id":"association-analysis-advanced-concepts","chapter":"6 Association Analysis: Advanced Concepts","heading":"6 Association Analysis: Advanced Concepts","text":"code available Chapter. topics already covered code previous chapter.","code":""},{"path":"clustering-analysis.html","id":"clustering-analysis","chapter":"7 Clustering Analysis","heading":"7 Clustering Analysis","text":"packages used chapter : cluster (Maechler et al. 2022), dbscan (Hahsler Piekenbrock 2023), e1071 (Meyer et al. 2023), factoextra (Kassambara Mundt 2020), fpc (Hennig 2023), GGally (Schloerke et al. 2021), kernlab (Karatzoglou, Smola, Hornik 2023), mclust (Fraley, Raftery, Scrucca 2022), mlbench (Leisch Dimitriadou. 2023), scatterpie (Yu 2023), seriation (Hahsler, Buchta, Hornik 2023), tidyverse (Wickham 2023b)","code":"\npkgs <- sort(c('tidyverse', 'factoextra', 'dbscan', 'cluster', 'mclust', \n  'kernlab', 'e1071', 'scatterpie', 'fpc', 'seriation', 'mlbench', 'GGally'\n))\n  \npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"clustering-analysis.html","id":"introduction-4","chapter":"7 Clustering Analysis","heading":"7.1 Introduction","text":"Cluster analysis clustering\ntask grouping set objects way objects group (called cluster) similar (sense) groups (clusters).Clustering also called unsupervised learning, tries directly learns structure data\nrely availability correct answer class label supervised learning .\nClustering often used exploratory analysis preprocess data grouping.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 7. Cluster Analysis: Basic Concepts \nAlgorithms","code":""},{"path":"clustering-analysis.html","id":"data-preparation","chapter":"7 Clustering Analysis","heading":"7.2 Data Preparation","text":"use small clean toy dataset called Ruspini \nincluded R package cluster.Ruspini data set, consisting 75 points four groups \npopular illustrating clustering techniques. simple data\nset well separated clusters. original dataset points\nordered group. can shuffle data (rows) using sample_frac\nsamples default 100%.","code":"\nlibrary(tidyverse)\ndata(ruspini, package = \"cluster\")\nruspini <- as_tibble(ruspini) |> \n  sample_frac()\nruspini## # A tibble: 75 × 2\n##        x     y\n##    <int> <int>\n##  1   115   117\n##  2   111   126\n##  3    85    96\n##  4    12    88\n##  5    36    72\n##  6    28   147\n##  7    24    58\n##  8    99   128\n##  9    46   142\n## 10    38   151\n## # ℹ 65 more rows"},{"path":"clustering-analysis.html","id":"data-cleaning","chapter":"7 Clustering Analysis","heading":"7.2.1 Data cleaning","text":"clustering algorithms necessary handle missing values\noutliers (e.g., remove observations). details see Section\n“Outlier removal” . data set missing values strong\noutlier looks like clear groups.","code":"\nggplot(ruspini, aes(x = x, y = y)) + geom_point()\nsummary(ruspini)##        x               y        \n##  Min.   :  4.0   Min.   :  4.0  \n##  1st Qu.: 31.5   1st Qu.: 56.5  \n##  Median : 52.0   Median : 96.0  \n##  Mean   : 54.9   Mean   : 92.0  \n##  3rd Qu.: 76.5   3rd Qu.:141.5  \n##  Max.   :117.0   Max.   :156.0"},{"path":"clustering-analysis.html","id":"scale-data","chapter":"7 Clustering Analysis","heading":"7.2.2 Scale data","text":"Clustering algorithms use distances variables largest\nnumber range dominate distance calculation. summary shows\nissue Ruspini dataset , x y,\nroughly 0 150. data analysts still scale\ncolumn data zero mean unit standard deviation\n(z-scores).\nNote: standard scale() function scales whole data\nmatrix implement function single vector apply \nnumeric columns.scaling, z-scores fall range \\([-3,3]\\) (z-scores\nmeasured standard deviations mean), \\(0\\) means\naverage.","code":"\n## I use this till tidyverse implements a scale function\nscale_numeric <- function(x) mutate_if(x, is.numeric, function(y) as.vector(scale(y)))\n\nruspini_scaled <- ruspini |> \n  scale_numeric()\nsummary(ruspini_scaled)##        x                y         \n##  Min.   :-1.668   Min.   :-1.807  \n##  1st Qu.:-0.766   1st Qu.:-0.729  \n##  Median :-0.094   Median : 0.082  \n##  Mean   : 0.000   Mean   : 0.000  \n##  3rd Qu.: 0.709   3rd Qu.: 1.016  \n##  Max.   : 2.037   Max.   : 1.314"},{"path":"clustering-analysis.html","id":"clustering-methods","chapter":"7 Clustering Analysis","heading":"7.3 Clustering methods","text":"","code":""},{"path":"clustering-analysis.html","id":"k-means-clustering","chapter":"7 Clustering Analysis","heading":"7.3.1 k-means Clustering","text":"k-means implicitly\nassumes Euclidean distances. use \\(k = 4\\) clusters run \nalgorithm 10 times random initialized centroids. best result \nreturned.km R object implemented list. clustering vector\ncontains cluster assignment data row can accessed\nusing km$cluster. add cluster assignment column \nscaled dataset (make factor since represents nominal label).Add centroids plot.Use factoextra package visualization","code":"\nkm <- kmeans(ruspini_scaled, centers = 4, nstart = 10)\nkm## K-means clustering with 4 clusters of sizes 23, 17, 15, 20\n## \n## Cluster means:\n##        x      y\n## 1 -0.360  1.109\n## 2  1.419  0.469\n## 3  0.461 -1.491\n## 4 -1.139 -0.556\n## \n## Clustering vector:\n##  [1] 2 2 2 4 4 1 4 2 1 1 1 1 4 3 3 2 4 3 2 1 1 1 2 2 3\n## [26] 2 1 3 3 2 1 3 3 4 2 1 3 4 4 3 1 4 1 3 2 4 3 1 2 1\n## [51] 1 2 2 4 4 2 1 4 3 4 1 1 2 4 4 4 1 1 3 3 4 1 1 4 4\n## \n## Within cluster sum of squares by cluster:\n## [1] 2.66 3.64 1.08 2.71\n##  (between_SS / total_SS =  93.2 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"       \n## [4] \"withinss\"     \"tot.withinss\" \"betweenss\"   \n## [7] \"size\"         \"iter\"         \"ifault\"\nruspini_clustered <- ruspini_scaled |> add_column(cluster = factor(km$cluster))\nruspini_clustered## # A tibble: 75 × 3\n##         x       y cluster\n##     <dbl>   <dbl> <fct>  \n##  1  1.97   0.513  2      \n##  2  1.84   0.698  2      \n##  3  0.987  0.0816 2      \n##  4 -1.41  -0.0827 4      \n##  5 -0.619 -0.411  4      \n##  6 -0.881  1.13   1      \n##  7 -1.01  -0.699  4      \n##  8  1.45   0.739  2      \n##  9 -0.291  1.03   1      \n## 10 -0.553  1.21   1      \n## # ℹ 65 more rows\nggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point()\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\ncentroids## # A tibble: 4 × 3\n##   cluster      x      y\n##   <chr>    <dbl>  <dbl>\n## 1 1       -0.360  1.11 \n## 2 2        1.42   0.469\n## 3 3        0.461 -1.49 \n## 4 4       -1.14  -0.556\nggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)\nlibrary(factoextra)\nfviz_cluster(km, data = ruspini_scaled, centroids = TRUE, repel = TRUE, ellipse.type = \"norm\")"},{"path":"clustering-analysis.html","id":"inspect-clusters","chapter":"7 Clustering Analysis","heading":"7.3.1.1 Inspect clusters","text":"inspect clusters created 4-cluster k-means solution. \nfollowing code can adapted used clustering methods.","code":""},{"path":"clustering-analysis.html","id":"cluster-profiles","chapter":"7 Clustering Analysis","heading":"7.3.1.1.1 Cluster Profiles","text":"Inspect centroids horizontal bar charts organized cluster.\ngroup plots cluster, change data format \n“long”-format using pivot operation. use colors match \nclusters scatter plots.","code":"\nggplot(pivot_longer(centroids, cols = c(x, y), names_to = \"feature\"),\n  aes(x = value, y = feature, fill = cluster)) +\n  geom_bar(stat = \"identity\") +\n  facet_grid(rows = vars(cluster))"},{"path":"clustering-analysis.html","id":"extract-a-single-cluster","chapter":"7 Clustering Analysis","heading":"7.3.1.1.2 Extract a single cluster","text":"need filter rows corresponding cluster index. \nnext example calculates summary statistics plots data\npoints cluster 1.happens try cluster 8 centers?","code":"\ncluster1 <- ruspini_clustered |> \n  filter(cluster == 1)\ncluster1## # A tibble: 23 × 3\n##           x     y cluster\n##       <dbl> <dbl> <fct>  \n##  1 -0.881   1.13  1      \n##  2 -0.291   1.03  1      \n##  3 -0.553   1.21  1      \n##  4 -0.553   1.09  1      \n##  5 -0.160   1.03  1      \n##  6 -0.717   1.27  1      \n##  7  0.00393 1.29  1      \n##  8 -0.0944  1.23  1      \n##  9 -0.750   1.17  1      \n## 10 -0.0289  0.657 1      \n## # ℹ 13 more rows\nsummary(cluster1)##        x                y         cluster\n##  Min.   :-0.881   Min.   :0.657   1:23   \n##  1st Qu.:-0.603   1st Qu.:1.036   2: 0   \n##  Median :-0.357   Median :1.129   3: 0   \n##  Mean   :-0.360   Mean   :1.109   4: 0   \n##  3rd Qu.:-0.127   3rd Qu.:1.221          \n##  Max.   : 0.266   Max.   :1.314\nggplot(cluster1, aes(x = x, y = y)) + geom_point() +\n  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2))\nfviz_cluster(kmeans(ruspini_scaled, centers = 8), data = ruspini_scaled,\n  centroids = TRUE,  geom = \"point\", ellipse.type = \"norm\")"},{"path":"clustering-analysis.html","id":"hierarchical-clustering","chapter":"7 Clustering Analysis","heading":"7.3.2 Hierarchical Clustering","text":"Hierarchical clustering starts distance matrix. dist() defaults\nmethod=“Euclidean”. Note: Distance matrices become large\nquickly (size time complexity \\(O(n^2)\\) \\(n\\) number \ndata points). possible calculate store matrix \nsmall data sets (maybe hundred thousand data points) main\nmemory. data large can use sampling.hclust() implements agglomerative hierarchical\nclustering. \ncluster using complete link.Hierarchical clustering return cluster assignments \ndendrogram. standard plot function plots dendrogram.Use factoextra (ggplot version). can specify number clusters\nvisualize dendrogram cut clusters.plotting options dendrograms, including plotting parts large\ndendrograms can found .Extract cluster assignments cutting dendrogram four parts\nadd cluster id data.Try 8 clusters (Note: fviz_cluster needs list data \ncluster labels hclust)Clustering single link","code":"\nd <- dist(ruspini_scaled)\nhc <- hclust(d, method = \"complete\")\nplot(hc)\nfviz_dend(hc, k = 4)## Warning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as of ggplot2\n## 3.3.4.\n## ℹ The deprecated feature was likely used in the factoextra package.\n##   Please report the issue at <https://github.com/kassambara/factoextra/issues>.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nclusters <- cutree(hc, k = 4)\ncluster_complete <- ruspini_scaled |>\n  add_column(cluster = factor(clusters))\ncluster_complete## # A tibble: 75 × 3\n##         x       y cluster\n##     <dbl>   <dbl> <fct>  \n##  1  1.97   0.513  1      \n##  2  1.84   0.698  1      \n##  3  0.987  0.0816 1      \n##  4 -1.41  -0.0827 2      \n##  5 -0.619 -0.411  2      \n##  6 -0.881  1.13   3      \n##  7 -1.01  -0.699  2      \n##  8  1.45   0.739  1      \n##  9 -0.291  1.03   3      \n## 10 -0.553  1.21   3      \n## # ℹ 65 more rows\nggplot(cluster_complete, aes(x, y, color = cluster)) +\n  geom_point()\nfviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc, k = 8)), geom = \"point\")\nhc_single <- hclust(d, method = \"single\")\nfviz_dend(hc_single, k = 4)\nfviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc_single, k = 4)), geom = \"point\")"},{"path":"clustering-analysis.html","id":"density-based-clustering-with-dbscan","chapter":"7 Clustering Analysis","heading":"7.3.3 Density-based clustering with DBSCAN","text":"DBSCAN stands “Density-Based\nSpatial Clustering Applications Noise.” groups together\npoints closely packed together treats points low-density\nregions outliers.Parameters: minPts defines many points epsilon\nneighborhood needed make point core point. often chosen\nsmoothing parameter. use minPts = 4.decide epsilon, knee kNN distance plot often used.\nNote minPts contains point , k-nearest neighbor\n. therefore use k = minPts - 1! knee around\neps = .32.run dbscanNote: Cluster 0 represents outliers).Play eps (neighborhood size) MinPts (minimum points needed\ncore cluster)","code":"\nlibrary(dbscan)## \n## Attaching package: 'dbscan'## The following object is masked from 'package:stats':\n## \n##     as.dendrogram\nkNNdistplot(ruspini_scaled, k = 3)\nabline(h = .32, col = \"red\")\ndb <- dbscan(ruspini_scaled, eps = .32, minPts = 4)\ndb## DBSCAN clustering for 75 objects.\n## Parameters: eps = 0.32, minPts = 4\n## Using euclidean distances and borderpoints = TRUE\n## The clustering contains 4 cluster(s) and 5 noise points.\n## \n##  0  1  2  3  4 \n##  5 12 20 23 15 \n## \n## Available fields: cluster, eps, minPts, dist,\n##                   borderPoints\nstr(db)## List of 5\n##  $ cluster     : int [1:75] 1 1 0 2 2 3 2 1 3 3 ...\n##  $ eps         : num 0.32\n##  $ minPts      : num 4\n##  $ dist        : chr \"euclidean\"\n##  $ borderPoints: logi TRUE\n##  - attr(*, \"class\")= chr [1:2] \"dbscan_fast\" \"dbscan\"\nggplot(ruspini_scaled |> add_column(cluster = factor(db$cluster)),\n  aes(x, y, color = cluster)) + geom_point()\nfviz_cluster(db, ruspini_scaled, geom = \"point\")"},{"path":"clustering-analysis.html","id":"partitioning-around-medoids-pam","chapter":"7 Clustering Analysis","heading":"7.3.4 Partitioning Around Medoids (PAM)","text":"PAM tries solve \n\\(k\\)-medoids problem. problem similar \\(k\\)-means, uses\nmedoids instead centroids represent clusters. Like hierarchical\nclustering, typically works precomputed distance matrix. \nadvantage can use distance metric just Euclidean\ndistances. Note: medoid central data point \nmiddle cluster.","code":"\nlibrary(cluster)## \n## Attaching package: 'cluster'## The following object is masked _by_ '.GlobalEnv':\n## \n##     ruspini\nd <- dist(ruspini_scaled)\nstr(d)##  'dist' num [1:2775] 0.227 1.074 3.429 2.75 2.918 ...\n##  - attr(*, \"Size\")= int 75\n##  - attr(*, \"Diag\")= logi FALSE\n##  - attr(*, \"Upper\")= logi FALSE\n##  - attr(*, \"method\")= chr \"Euclidean\"\n##  - attr(*, \"call\")= language dist(x = ruspini_scaled)\np <- pam(d, k = 4)\np## Medoids:\n##      ID   \n## [1,] 49 49\n## [2,] 46 46\n## [3,] 41 41\n## [4,] 44 44\n## Clustering vector:\n##  [1] 1 1 1 2 2 3 2 1 3 3 3 3 2 4 4 1 2 4 1 3 3 3 1 1 4\n## [26] 1 3 4 4 1 3 4 4 2 1 3 4 2 2 4 3 2 3 4 1 2 4 3 1 3\n## [51] 3 1 1 2 2 1 3 2 4 2 3 3 1 2 2 2 3 3 4 4 2 3 3 2 2\n## Objective function:\n## build  swap \n## 0.442 0.319 \n## \n## Available components:\n## [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\" \n## [5] \"isolation\"  \"clusinfo\"   \"silinfo\"    \"diss\"      \n## [9] \"call\"\nruspini_clustered <- ruspini_scaled |> \n  add_column(cluster = factor(p$cluster))\n\nmedoids <- as_tibble(ruspini_scaled[p$medoids, ], rownames = \"cluster\")\nmedoids## # A tibble: 4 × 3\n##   cluster      x      y\n##   <chr>    <dbl>  <dbl>\n## 1 1        1.45   0.554\n## 2 2       -1.18  -0.555\n## 3 3       -0.357  1.17 \n## 4 4        0.463 -1.46\nggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + \n  geom_point() +\n  geom_point(data = medoids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)\n## __Note:__ `fviz_cluster` needs the original data.\nfviz_cluster(c(p, list(data = ruspini_scaled)), geom = \"point\", ellipse.type = \"norm\")"},{"path":"clustering-analysis.html","id":"gaussian-mixture-models","chapter":"7 Clustering Analysis","heading":"7.3.5 Gaussian Mixture Models","text":"Gaussian mixture\nmodels\nassume data set result drawing data set \nGaussian distributions distribution represents cluster.\nEstimation algorithms try identify location parameters \ndistributions thus can used find clusters. Mclust() uses\nBayesian Information Criterion (BIC) find number clusters\n(model selection). BIC uses likelihood penalty term guard\noverfitting.Rerun fixed number 4 clusters","code":"\nlibrary(mclust)## Package 'mclust' version 6.0.0\n## Type 'citation(\"mclust\")' for citing this R package in publications.## \n## Attaching package: 'mclust'## The following object is masked from 'package:purrr':\n## \n##     map\nm <- Mclust(ruspini_scaled)\nsummary(m)## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEI (diagonal, equal volume and shape) model\n## with 5 components: \n## \n##  log-likelihood  n df  BIC  ICL\n##           -91.3 75 16 -252 -252\n## \n## Clustering table:\n##  1  2  3  4  5 \n## 14  3 20 23 15\nplot(m, what = \"classification\")\nm <- Mclust(ruspini_scaled, G=4)\nsummary(m)## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEI (diagonal, equal volume and shape) model\n## with 4 components: \n## \n##  log-likelihood  n df  BIC  ICL\n##            -102 75 13 -259 -259\n## \n## Clustering table:\n##  1  2  3  4 \n## 17 20 23 15\nplot(m, what = \"classification\")"},{"path":"clustering-analysis.html","id":"spectral-clustering","chapter":"7 Clustering Analysis","heading":"7.3.6 Spectral clustering","text":"Spectral clustering\nworks embedding data points partitioning problem \nsubspace k largest eigenvectors normalized affinity/kernel\nmatrix. uses simple clustering method like k-means.","code":"\nlibrary(\"kernlab\")## \n## Attaching package: 'kernlab'## The following object is masked from 'package:scales':\n## \n##     alpha## The following object is masked from 'package:arules':\n## \n##     size## The following object is masked from 'package:purrr':\n## \n##     cross## The following object is masked from 'package:ggplot2':\n## \n##     alpha\ncluster_spec <- specc(as.matrix(ruspini_scaled), centers = 4)\ncluster_spec## Spectral Clustering object of class \"specc\" \n## \n##  Cluster memberships: \n##  \n## 1 1 1 1 1 2 1 1 2 2 2 2 1 3 4 1 1 4 1 2 2 2 1 1 4 1 2 3 4 1 2 3 4 1 1 2 4 1 1 4 2 1 2 4 1 1 4 2 1 2 2 1 1 1 1 1 2 1 4 1 2 2 1 1 1 1 2 2 4 3 1 2 2 1 1 \n##  \n## Gaussian Radial Basis kernel function. \n##  Hyperparameter : sigma =  41.7670067458421 \n## \n## Centers:  \n##         [,1]    [,2]\n## [1,]  0.0367 -0.0849\n## [2,] -0.3595  1.1091\n## [3,]  0.7416 -1.4789\n## [4,]  0.3586 -1.4957\n## \n## Cluster size:  \n## [1] 37 23  4 11\n## \n## Within-cluster sum of squares:  \n## [1] 75.6 52.0 18.8 37.1\nggplot(ruspini_scaled |> \n         add_column(cluster = factor(cluster_spec)),\n  aes(x, y, color = cluster)) + \n  geom_point()"},{"path":"clustering-analysis.html","id":"fuzzy-c-means-clustering","chapter":"7 Clustering Analysis","heading":"7.3.7 Fuzzy C-Means Clustering","text":"fuzzy clustering\nversion k-means clustering problem. data point degree\nmembership cluster.Plot membership (shown small pie charts)","code":"\nlibrary(\"e1071\")\n\ncluster_cmeans <- cmeans(as.matrix(ruspini_scaled), centers = 4)\ncluster_cmeans## Fuzzy c-means clustering with 4 clusters\n## \n## Cluster centers:\n##        x      y\n## 1 -0.376  1.114\n## 2  0.455 -1.476\n## 3 -1.137 -0.555\n## 4  1.505  0.516\n## \n## Memberships:\n##              1        2        3        4\n##  [1,] 3.39e-02 0.031838 0.018432 9.16e-01\n##  [2,] 2.68e-02 0.020544 0.013080 9.40e-01\n##  [3,] 1.10e-01 0.118859 0.065467 7.06e-01\n##  [4,] 9.82e-02 0.045277 0.828834 2.77e-02\n##  [5,] 9.31e-02 0.097120 0.768391 4.14e-02\n##  [6,] 8.62e-01 0.025668 0.075850 3.63e-02\n##  [7,] 9.54e-03 0.012774 0.973170 4.51e-03\n##  [8,] 1.48e-02 0.008725 0.006153 9.70e-01\n##  [9,] 9.89e-01 0.002182 0.004627 4.27e-03\n## [10,] 9.75e-01 0.004817 0.011470 8.41e-03\n## [11,] 9.79e-01 0.004135 0.010326 6.88e-03\n## [12,] 9.60e-01 0.007890 0.015164 1.73e-02\n## [13,] 2.69e-02 0.027262 0.934341 1.15e-02\n## [14,] 2.34e-02 0.892180 0.038484 4.59e-02\n## [15,] 1.02e-02 0.953744 0.018343 1.78e-02\n## [16,] 2.46e-03 0.001840 0.001160 9.95e-01\n## [17,] 3.66e-02 0.054960 0.890097 1.83e-02\n## [18,] 1.14e-02 0.947252 0.024935 1.65e-02\n## [19,] 1.74e-01 0.177389 0.107509 5.41e-01\n## [20,] 9.25e-01 0.014629 0.037153 2.37e-02\n## [21,] 8.92e-01 0.019994 0.033363 5.51e-02\n## [22,] 9.40e-01 0.011470 0.020462 2.85e-02\n## [23,] 4.09e-02 0.040522 0.022924 8.96e-01\n## [24,] 7.44e-03 0.004748 0.003221 9.85e-01\n## [25,] 9.53e-03 0.953143 0.025463 1.19e-02\n## [26,] 2.18e-01 0.183757 0.128308 4.70e-01\n## [27,] 9.19e-01 0.015519 0.041982 2.38e-02\n## [28,] 1.09e-02 0.950362 0.020593 1.82e-02\n## [29,] 1.35e-03 0.993633 0.003167 1.85e-03\n## [30,] 5.07e-04 0.000411 0.000250 9.99e-01\n## [31,] 7.51e-01 0.051906 0.092068 1.05e-01\n## [32,] 9.02e-03 0.959100 0.017339 1.45e-02\n## [33,] 1.66e-03 0.992177 0.003861 2.30e-03\n## [34,] 3.00e-02 0.040506 0.914867 1.46e-02\n## [35,] 1.13e-02 0.009962 0.005870 9.73e-01\n## [36,] 9.10e-01 0.016805 0.048384 2.45e-02\n## [37,] 1.73e-02 0.912542 0.049812 2.04e-02\n## [38,] 5.52e-02 0.059385 0.860423 2.50e-02\n## [39,] 5.31e-02 0.033635 0.895310 1.80e-02\n## [40,] 1.08e-02 0.947018 0.028739 1.34e-02\n## [41,] 9.98e-01 0.000451 0.000964 8.88e-04\n## [42,] 2.54e-02 0.022071 0.942653 9.90e-03\n## [43,] 7.55e-01 0.044822 0.067224 1.33e-01\n## [44,] 5.05e-05 0.999766 0.000110 7.42e-05\n## [45,] 9.62e-02 0.053620 0.039261 8.11e-01\n## [46,] 4.47e-04 0.000437 0.998932 1.84e-04\n## [47,] 3.35e-03 0.983990 0.008241 4.42e-03\n## [48,] 9.92e-01 0.001497 0.003385 2.77e-03\n## [49,] 1.32e-03 0.000943 0.000609 9.97e-01\n## [50,] 7.02e-01 0.050971 0.071389 1.76e-01\n## [51,] 9.47e-01 0.010355 0.025638 1.73e-02\n## [52,] 1.39e-02 0.013539 0.007574 9.65e-01\n## [53,] 8.07e-03 0.005042 0.003456 9.83e-01\n## [54,] 2.25e-02 0.035817 0.930269 1.14e-02\n## [55,] 4.92e-02 0.043110 0.887727 2.00e-02\n## [56,] 1.27e-01 0.046128 0.039417 7.88e-01\n## [57,] 9.75e-01 0.004750 0.012064 7.76e-03\n## [58,] 2.26e-02 0.015521 0.954072 7.84e-03\n## [59,] 1.40e-03 0.993497 0.003079 2.03e-03\n## [60,] 1.36e-02 0.010238 0.971283 4.91e-03\n## [61,] 9.29e-01 0.013313 0.037605 1.97e-02\n## [62,] 9.27e-01 0.013940 0.024785 3.40e-02\n## [63,] 1.93e-02 0.019236 0.010680 9.51e-01\n## [64,] 3.84e-02 0.074030 0.866516 2.11e-02\n## [65,] 3.14e-03 0.003403 0.992094 1.36e-03\n## [66,] 1.71e-02 0.013810 0.962604 6.49e-03\n## [67,] 9.76e-01 0.004635 0.009541 9.54e-03\n## [68,] 9.89e-01 0.002238 0.004484 4.75e-03\n## [69,] 1.01e-02 0.952362 0.024151 1.34e-02\n## [70,] 1.11e-02 0.949234 0.020387 1.93e-02\n## [71,] 4.51e-02 0.033977 0.904508 1.64e-02\n## [72,] 9.96e-01 0.000705 0.001560 1.32e-03\n## [73,] 9.69e-01 0.005928 0.011252 1.35e-02\n## [74,] 4.26e-02 0.063341 0.872767 2.13e-02\n## [75,] 2.10e-02 0.029083 0.939773 1.01e-02\n## \n## Closest hard clustering:\n##  [1] 4 4 4 3 3 1 3 4 1 1 1 1 3 2 2 4 3 2 4 1 1 1 4 4 2\n## [26] 4 1 2 2 4 1 2 2 3 4 1 2 3 3 2 1 3 1 2 4 3 2 1 4 1\n## [51] 1 4 4 3 3 4 1 3 2 3 1 1 4 3 3 3 1 1 2 2 3 1 1 3 3\n## \n## Available components:\n## [1] \"centers\"     \"size\"        \"cluster\"    \n## [4] \"membership\"  \"iter\"        \"withinerror\"\n## [7] \"call\"\nlibrary(\"scatterpie\")\nggplot()  +\n  geom_scatterpie(data = cbind(ruspini_scaled, cluster_cmeans$membership),\n    aes(x = x, y = y), cols = colnames(cluster_cmeans$membership), legend_name = \"Membership\") + coord_equal()"},{"path":"clustering-analysis.html","id":"internal-cluster-validation","chapter":"7 Clustering Analysis","heading":"7.4 Internal Cluster Validation","text":"","code":""},{"path":"clustering-analysis.html","id":"compare-the-clustering-quality","chapter":"7 Clustering Analysis","heading":"7.4.1 Compare the Clustering Quality","text":"two popular quality metrics within-cluster sum \nsquares (WCSS) used \n\\(k\\)-means \naverage silhouette\nwidth. Look \nwithin.cluster.ss avg.silwidth .Notes: * load fpc since NAMESPACE overwrites dbscan. *\nclustering (second argument ) supplied vector\nnumbers (cluster IDs) factor (use .integer() \nconvert factor ID).Read ? cluster.stats explanation available indices.","code":"\n##library(fpc)\nfpc::cluster.stats(d, km$cluster)## $n\n## [1] 75\n## \n## $cluster.number\n## [1] 4\n## \n## $cluster.size\n## [1] 23 17 15 20\n## \n## $min.cluster.size\n## [1] 15\n## \n## $noisen\n## [1] 0\n## \n## $diameter\n## [1] 1.159 1.463 0.836 1.119\n## \n## $average.distance\n## [1] 0.429 0.581 0.356 0.482\n## \n## $median.distance\n## [1] 0.393 0.502 0.338 0.449\n## \n## $separation\n## [1] 0.768 0.768 1.158 1.158\n## \n## $average.toother\n## [1] 2.15 2.29 2.31 2.16\n## \n## $separation.matrix\n##       [,1]  [,2] [,3] [,4]\n## [1,] 0.000 0.768 1.96 1.22\n## [2,] 0.768 0.000 1.31 1.34\n## [3,] 1.958 1.308 0.00 1.16\n## [4,] 1.220 1.340 1.16 0.00\n## \n## $ave.between.matrix\n##      [,1] [,2] [,3] [,4]\n## [1,] 0.00 1.92 2.75 1.89\n## [2,] 1.92 0.00 2.22 2.77\n## [3,] 2.75 2.22 0.00 1.87\n## [4,] 1.89 2.77 1.87 0.00\n## \n## $average.between\n## [1] 2.22\n## \n## $average.within\n## [1] 0.463\n## \n## $n.between\n## [1] 2091\n## \n## $n.within\n## [1] 684\n## \n## $max.diameter\n## [1] 1.46\n## \n## $min.separation\n## [1] 0.768\n## \n## $within.cluster.ss\n## [1] 10.1\n## \n## $clus.avg.silwidths\n##     1     2     3     4 \n## 0.745 0.681 0.807 0.721 \n## \n## $avg.silwidth\n## [1] 0.737\n## \n## $g2\n## NULL\n## \n## $g3\n## NULL\n## \n## $pearsongamma\n## [1] 0.842\n## \n## $dunn\n## [1] 0.525\n## \n## $dunn2\n## [1] 3.23\n## \n## $entropy\n## [1] 1.37\n## \n## $wb.ratio\n## [1] 0.209\n## \n## $ch\n## [1] 324\n## \n## $cwidegap\n## [1] 0.315 0.415 0.235 0.261\n## \n## $widestgap\n## [1] 0.415\n## \n## $sindex\n## [1] 0.858\n## \n## $corrected.rand\n## NULL\n## \n## $vi\n## NULL\nsapply(\n  list(\n    km = km$cluster,\n    hc_compl = cutree(hc, k = 4),\n    hc_single = cutree(hc_single, k = 4)\n  ),\n  FUN = function(x)\n    fpc::cluster.stats(d, x))[c(\"within.cluster.ss\", \"avg.silwidth\"), ]##                   km    hc_compl hc_single\n## within.cluster.ss 10.1  10.1     10.1     \n## avg.silwidth      0.737 0.737    0.737"},{"path":"clustering-analysis.html","id":"silhouette-plot","chapter":"7 Clustering Analysis","heading":"7.4.2 Silhouette plot","text":"Note: silhouette plot show correctly R Studio \nmany objects (bars missing). work open \nnew plotting device windows(), x11() quartz().ggplot visualization using factoextra","code":"\nlibrary(cluster)\nplot(silhouette(km$cluster, d))\nfviz_silhouette(silhouette(km$cluster, d))##   cluster size ave.sil.width\n## 1       1   23          0.75\n## 2       2   17          0.68\n## 3       3   15          0.81\n## 4       4   20          0.72"},{"path":"clustering-analysis.html","id":"find-optimal-number-of-clusters-for-k-means","chapter":"7 Clustering Analysis","heading":"7.4.3 Find Optimal Number of Clusters for k-means","text":"","code":"\nggplot(ruspini_scaled, aes(x, y)) + geom_point()\n## We will use different methods and try 1-10 clusters.\nset.seed(1234)\nks <- 2:10"},{"path":"clustering-analysis.html","id":"elbow-method-within-cluster-sum-of-squares","chapter":"7 Clustering Analysis","heading":"7.4.3.1 Elbow Method: Within-Cluster Sum of Squares","text":"Calculate within-cluster sum squares different numbers \nclusters look knee \nelbow \nplot. (nstart = 5 just repeats k-means 5 times returns best\nsolution)","code":"\nWCSS <- sapply(ks, FUN = function(k) {\n  kmeans(ruspini_scaled, centers = k, nstart = 5)$tot.withinss\n  })\n\nggplot(tibble(ks, WCSS), aes(ks, WCSS)) + geom_line() +\n  geom_vline(xintercept = 4, color = \"red\", linetype = 2)"},{"path":"clustering-analysis.html","id":"average-silhouette-width","chapter":"7 Clustering Analysis","heading":"7.4.3.2 Average Silhouette Width","text":"Plot average silhouette width different number clusters \nlook maximum plot.","code":"\nASW <- sapply(ks, FUN=function(k) {\n  fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart = 5)$cluster)$avg.silwidth\n  })\n\nbest_k <- ks[which.max(ASW)]\nbest_k## [1] 4\nggplot(tibble(ks, ASW), aes(ks, ASW)) + geom_line() +\n  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"},{"path":"clustering-analysis.html","id":"dunn-index","chapter":"7 Clustering Analysis","heading":"7.4.3.3 Dunn Index","text":"Use Dunn index (another\ninternal measure given min. separation/ max. diameter)","code":"\nDI <- sapply(ks, FUN=function(k) {\n  fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart=5)$cluster)$dunn\n})\n\nbest_k <- ks[which.max(DI)]\nggplot(tibble(ks, DI), aes(ks, DI)) + geom_line() +\n  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"},{"path":"clustering-analysis.html","id":"gap-statistic","chapter":"7 Clustering Analysis","heading":"7.4.3.4 Gap Statistic","text":"Compares change within-cluster dispersion expected \nnull model (see ? clusGap). default method choose \nsmallest k value Gap(k) 1 standard error\naway first local maximum.Note: methods can also used hierarchical clustering.many methods indices proposed determine \nnumber clusters. See, e.g., package\nNbClust.","code":"\nlibrary(cluster)\nk <- clusGap(ruspini_scaled, FUN = kmeans,  nstart = 10, K.max = 10)\nk## Clustering Gap statistic [\"clusGap\"] from call:\n## clusGap(x = ruspini_scaled, FUNcluster = kmeans, K.max = 10, nstart = 10)\n## B=100 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n##  --> Number of clusters (method 'firstSEmax', SE.factor=1): 4\n##       logW E.logW     gap SE.sim\n##  [1,] 3.50   3.47 -0.0308 0.0357\n##  [2,] 3.07   3.15  0.0762 0.0374\n##  [3,] 2.68   2.90  0.2247 0.0380\n##  [4,] 2.11   2.70  0.5971 0.0363\n##  [5,] 1.99   2.57  0.5827 0.0347\n##  [6,] 1.86   2.45  0.5871 0.0365\n##  [7,] 1.73   2.35  0.6156 0.0395\n##  [8,] 1.64   2.26  0.6157 0.0413\n##  [9,] 1.56   2.17  0.6113 0.0409\n## [10,] 1.51   2.09  0.5799 0.0393\nplot(k)"},{"path":"clustering-analysis.html","id":"visualizing-the-distance-matrix","chapter":"7 Clustering Analysis","heading":"7.4.4 Visualizing the Distance Matrix","text":"Inspect distance matrix first 5 objects.false-color image visualizes value matrix pixel \ncolor representing value.Rows columns objects ordered data set.\ndiagonal represents distance object \ndefinition distance 0 (dark line). Visualizing unordered\ndistance matrix show much structure, can reorder \nmatrix (rows columns) using k-means cluster labels cluster\n1 4. clear block structure representing clusters becomes\nvisible.Plot function dissplot package seriation rearranges matrix\nadds lines cluster labels. lower half plot, \nshows average dissimilarities clusters. function organizes\nobjects cluster reorders clusters objects within\nclusters similar objects closer together.reordering dissplot makes misspecification k visible \nblocks.Using factoextra","code":"\nggplot(ruspini_scaled, aes(x, y, color = factor(km$cluster))) + geom_point()\nd <- dist(ruspini_scaled)\nas.matrix(d)[1:5, 1:5]##       1     2    3     4     5\n## 1 0.000 0.227 1.07 3.429 2.750\n## 2 0.227 0.000 1.05 3.338 2.697\n## 3 1.074 1.052 0.00 2.399 1.680\n## 4 3.429 3.338 2.40 0.000 0.853\n## 5 2.750 2.697 1.68 0.853 0.000\nlibrary(seriation)\npimage(d, col = bluered(100))\npimage(d, order=order(km$cluster), col = bluered(100))\ndissplot(d, labels = km$cluster, options = list(main = \"k-means with k=4\"))\ndissplot(d, labels = kmeans(ruspini_scaled, centers = 3)$cluster, col = bluered(100))\ndissplot(d, labels = kmeans(ruspini_scaled, centers = 9)$cluster, col = bluered(100))\nfviz_dist(d)"},{"path":"clustering-analysis.html","id":"external-cluster-validation","chapter":"7 Clustering Analysis","heading":"7.5 External Cluster Validation","text":"External cluster validation uses ground truth information. , \nuser idea data grouped. known\nclass label provided clustering algorithm.use artificial data set known groups.Prepare dataFind optimal number Clusters k-meansUse within sum squares (look knee)Looks like 7 clustersHierarchical clustering: use single-link mouth \nnon-convex chaining may help.Find optimal number clustersThe maximum clearly 4 clusters.Compare ground truth corrected (=adjusted) Rand index\n(ARI),\nvariation information (VI)\nindex,\nentropy\n\npurity.cluster_stats computes ARI VI comparative measures. define\nfunctions entropy purity :calculate measures (comparison also use random “clusterings” \n4 6 clusters)Notes:Hierarchical clustering found perfect clustering.Entropy purity heavily impacted number clusters\n(clusters improve metric).corrected rand index shows clearly random clusterings\nrelationship ground truth (close 0). \nhelpful property.Read ? cluster.stats explanation available indices.","code":"\nlibrary(mlbench)\nset.seed(1234)\nshapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)\nplot(shapes)\ntruth <- as.integer(shapes$class)\nshapes <- shapes$x\ncolnames(shapes) <- c(\"x\", \"y\")\n\nshapes <- shapes |> scale() |> as_tibble()\n\nggplot(shapes, aes(x, y)) + geom_point()\nks <- 2:20\nWCSS <- sapply(ks, FUN = function(k) {\n  kmeans(shapes, centers = k, nstart = 10)$tot.withinss\n})\n\nggplot(tibble(ks, WCSS), aes(ks, WCSS)) + geom_line()\nkm <- kmeans(shapes, centers = 7, nstart = 10)\n\nggplot(shapes |> add_column(cluster = factor(km$cluster)), aes(x, y, color = cluster)) +\n  geom_point()\nd <- dist(shapes)\nhc <- hclust(d, method = \"single\")\nASW <- sapply(ks, FUN = function(k) {\n  fpc::cluster.stats(d, cutree(hc, k))$avg.silwidth\n})\n\nggplot(tibble(ks, ASW), aes(ks, ASW)) + geom_line()\nhc_4 <- cutree(hc, 4)\n\nggplot(shapes |> add_column(cluster = factor(hc_4)), aes(x, y, color = cluster)) +\n  geom_point()\nentropy <- function(cluster, truth) {\n  k <- max(cluster, truth)\n  cluster <- factor(cluster, levels = 1:k)\n  truth <- factor(truth, levels = 1:k)\n  w <- table(cluster)/length(cluster)\n\n  cnts <- sapply(split(truth, cluster), table)\n  p <- sweep(cnts, 1, rowSums(cnts), \"/\")\n  p[is.nan(p)] <- 0\n  e <- -p * log(p, 2)\n\n  sum(w * rowSums(e, na.rm = TRUE))\n}\n\npurity <- function(cluster, truth) {\n  k <- max(cluster, truth)\n  cluster <- factor(cluster, levels = 1:k)\n  truth <- factor(truth, levels = 1:k)\n  w <- table(cluster)/length(cluster)\n\n  cnts <- sapply(split(truth, cluster), table)\n  p <- sweep(cnts, 1, rowSums(cnts), \"/\")\n  p[is.nan(p)] <- 0\n\n  sum(w * apply(p, 1, max))\n}\nrandom_4 <- sample(1:4, nrow(shapes), replace = TRUE)\nrandom_6 <- sample(1:6, nrow(shapes), replace = TRUE)\n\nr <- rbind(\n  kmeans_7 = c(\n    unlist(fpc::cluster.stats(d, km$cluster, truth, compareonly = TRUE)),\n    entropy = entropy(km$cluster, truth),\n    purity = purity(km$cluster, truth)\n    ),\n  hc_4 = c(\n    unlist(fpc::cluster.stats(d, hc_4, truth, compareonly = TRUE)),\n    entropy = entropy(hc_4, truth),\n    purity = purity(hc_4, truth)\n    ),\n  random_4 = c(\n    unlist(fpc::cluster.stats(d, random_4, truth, compareonly = TRUE)),\n    entropy = entropy(random_4, truth),\n    purity = purity(random_4, truth)\n    ),\n  random_6 = c(\n    unlist(fpc::cluster.stats(d, random_6, truth, compareonly = TRUE)),\n    entropy = entropy(random_6, truth),\n    purity = purity(random_6, truth)\n    )\n  )\nr##          corrected.rand    vi entropy purity\n## kmeans_7        0.63823 0.571   0.229  0.464\n## hc_4            1.00000 0.000   0.000  1.000\n## random_4       -0.00324 2.683   1.988  0.288\n## random_6       -0.00213 3.076   1.728  0.144"},{"path":"clustering-analysis.html","id":"advanced-data-preparation-for-clustering","chapter":"7 Clustering Analysis","heading":"7.6 Advanced Data Preparation for Clustering","text":"","code":""},{"path":"clustering-analysis.html","id":"outlier-removal","chapter":"7 Clustering Analysis","heading":"7.6.1 Outlier Removal","text":"clustering algorithms perform complete assignment (.e., data\npoints need assigned cluster). Outliers affect \nclustering. useful identify outliers remove strong outliers\nprior clustering. density based method identify outlier \nLOF (Local Outlier\nFactor). related dbscan compares density around point\ndensities around neighbors (specify \nneighborhood size \\(k\\)). LOF value regular data point 1. \nlarger LOF value gets, likely point outlier.Add clear outlier scaled Ruspini dataset 10 standard\ndeviations average x axis.","code":"\nlibrary(dbscan)\nruspini_scaled_outlier <- ruspini_scaled |> add_case(x=10,y=0)"},{"path":"clustering-analysis.html","id":"visual-inspection-of-the-data","chapter":"7 Clustering Analysis","heading":"7.6.1.1 Visual inspection of the data","text":"Outliers can identified using summary statistics, histograms,\nscatterplots (pairs plots), boxplots, etc. use pairs plot\n(diagonal contains smoothed histograms). outlier visible \nsingle separate point scatter plot long tail \nsmoothed histogram x (expect observations \nfall range \\[-3,3\\] normalized data).outlier problem k-meansThis problem can fixed increasing number clusters \nremoving small clusters post-processing step identifying \nremoving outliers clustering.","code":"\nlibrary(\"GGally\")\nggpairs(ruspini_scaled_outlier, progress = FALSE)\nkm <- kmeans(ruspini_scaled_outlier, centers = 4, nstart = 10)\nruspini_scaled_outlier_km <- ruspini_scaled_outlier|>\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"},{"path":"clustering-analysis.html","id":"local-outlier-factor-lof","chapter":"7 Clustering Analysis","heading":"7.6.1.2 Local Outlier Factor (LOF)","text":"Local Outlier\nFactor related\nconcepts DBSCAN can help identify potential outliers. Calculate\nLOF (choose local neighborhood size 10 density estimation),Plot points sorted increasing LOF look knee.Choose threshold 1.Analyze found outliers (might interesting data points) cluster data without .many outlier removal strategies available. See, e.g.,\npackage outliers.","code":"\nlof <- lof(ruspini_scaled_outlier, minPts= 10)\nlof##  [1]  1.084  1.006  1.368  1.212  1.083  1.175  0.978\n##  [8]  0.998  0.972  0.965  0.990  1.041  1.051  1.252\n## [15]  1.026  0.938  1.020  1.024  1.572  1.068  1.209\n## [22]  1.070  1.138  1.002  0.996  1.598  1.034  1.071\n## [29]  1.015  0.939  1.626  0.976  0.997  1.083  1.021\n## [36]  1.040  1.021  1.209  1.088  0.981  0.932  1.013\n## [43]  1.397  0.994  1.160  0.941  0.993  0.943  0.933\n## [50]  1.494  1.021  1.006  0.992  0.954  1.085  1.143\n## [57]  0.982  1.020  0.986  0.918  1.034  1.104  1.004\n## [64]  1.041  0.983  1.039  0.985  1.019  0.963  1.059\n## [71]  1.001  0.934  1.051  1.023  0.977 17.899\nggplot(ruspini_scaled_outlier |> add_column(lof = lof), aes(x, y, color = lof)) +\n    geom_point() + scale_color_gradient(low = \"gray\", high = \"red\")\nggplot(tibble(index = seq_len(length(lof)), lof = sort(lof)), aes(index, lof)) +\n  geom_line() +\n  geom_hline(yintercept = 1, color = \"red\", linetype = 2)\nggplot(ruspini_scaled_outlier |> add_column(outlier = lof >= 2), aes(x, y, color = outlier)) +\n  geom_point()\nruspini_scaled_clean <- ruspini_scaled_outlier  |> filter(lof < 2)\n\nkm <- kmeans(ruspini_scaled_clean, centers = 4, nstart = 10)\nruspini_scaled_clean_km <- ruspini_scaled_clean|>\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_clean_km, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"},{"path":"clustering-analysis.html","id":"clustering-tendency","chapter":"7 Clustering Analysis","heading":"7.6.2 Clustering Tendency","text":"clustering algorithms always produce clustering, even \ndata contain cluster structure. typically good check\ncluster tendency attempting cluster data.use smiley data.","code":"\nlibrary(mlbench)\nshapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)$x\ncolnames(shapes) <- c(\"x\", \"y\")\nshapes <- as_tibble(shapes)"},{"path":"clustering-analysis.html","id":"scatter-plots","chapter":"7 Clustering Analysis","heading":"7.6.2.1 Scatter plots","text":"first step visual inspection using scatter plots.Cluster tendency typically indicated several separated point\nclouds. Often appropriate number clusters can also visually\nobtained counting number point clouds. see four clusters,\nmouth convex/spherical thus pose problems \nalgorithms like k-means.data two features can use pairs plot\n(scatterplot matrix) look scatterplot first two principal\ncomponents using PCA.","code":"\nggplot(shapes, aes(x = x, y = y)) + geom_point()"},{"path":"clustering-analysis.html","id":"visual-analysis-for-cluster-tendency-assessment-vat","chapter":"7 Clustering Analysis","heading":"7.6.2.2 Visual Analysis for Cluster Tendency Assessment (VAT)","text":"VAT reorders objects show potential clustering tendency \nblock structure (dark blocks along main diagonal). scale data\nusing Euclidean distance.iVAT uses largest distances possible paths two\nobjects instead direct distances make block structure\nbetter visible.","code":"\nlibrary(seriation)\n\nd_shapes <- dist(scale(shapes))\nVAT(d_shapes, col = bluered(100))\niVAT(d_shapes, col = bluered(100))"},{"path":"clustering-analysis.html","id":"hopkins-statistic","chapter":"7 Clustering Analysis","heading":"7.6.2.3 Hopkins statistic","text":"factoextra can also create VAT plot calculate Hopkins\nstatistic assess\nclustering tendency. Hopkins statistic, sample size \\(n\\) \ndrawn data compares nearest neighbor distribution\nsimulated dataset drawn random uniform distribution (see\ndetailed\nexplanation).\nvalues >.5 indicates usually clustering tendency.plots show strong cluster structure 4 clusters.","code":"\nget_clust_tendency(shapes, n = 10)## $hopkins_stat\n## [1] 0.907\n## \n## $plot"},{"path":"clustering-analysis.html","id":"data-without-clustering-tendency","chapter":"7 Clustering Analysis","heading":"7.6.2.4 Data Without Clustering Tendency","text":"point clouds visible, just noise.little clustering structure visible indicating low\nclustering tendency clustering performed data.\nHowever, k-means can used partition data \\(k\\) regions \nroughly equivalent size. can used data-driven\ndiscretization space.","code":"\ndata_random <- tibble(x = runif(500), y = runif(500))\nggplot(data_random, aes(x, y)) + geom_point()\nd_random <- dist(data_random)\nVAT(d_random, col = bluered(100))\niVAT(d_random, col = bluered(100))\nget_clust_tendency(data_random, n = 10, graph = FALSE)## $hopkins_stat\n## [1] 0.464\n## \n## $plot\n## NULL"},{"path":"clustering-analysis.html","id":"k-means-on-data-without-clustering-tendency","chapter":"7 Clustering Analysis","heading":"7.6.2.5 k-means on Data Without Clustering Tendency","text":"happens perform k-means data inherent\nclustering structure?k-means discretizes space similarly sized regions.","code":"\nkm <- kmeans(data_random, centers = 4)\nrandom_clustered<- data_random |> add_column(cluster = factor(km$cluster))\nggplot(random_clustered, aes(x = x, y = y, color = cluster)) + geom_point()"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Agrawal, Rakesh, Tomasz Imielinski, Arun Swami. 1993. “Mining Association Rules Sets Items Large Databases.” Proceedings 1993 Acm Sigmod International Conference Management Data, 207–16. Washington, D.C., United States: ACM Press.Allaire, JJ, François Chollet. 2023. Keras: R Interface Keras. https://tensorflow.rstudio.com/.Bates, Douglas, Martin Maechler, Mikael Jagan. 2023. Matrix: Sparse Dense Matrix Classes Methods. https://R-Forge.R-project.org/projects/matrix/.Breiman, Leo, Adele Cutler, Andy Liaw, Matthew Wiener. 2022. RandomForest: Breiman Cutler’s Random Forests Classification Regression. https://www.stat.berkeley.edu/~breiman/RandomForests/.Fraley, Chris, Adrian E. Raftery, Luca Scrucca. 2022. Mclust: Gaussian Mixture Modelling Model-Based Clustering, Classification, Density Estimation. https://mclust-org.github.io/mclust/.Grolemund, Garrett, Hadley Wickham. 2011. “Dates Times Made Easy lubridate.” Journal Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.Hahsler, Michael. 2017a. “Experimental Comparison Seriation Methods One-Mode Two-Way Data.” European Journal Operational Research 257 (1): 133–43. https://doi.org/10.1016/j.ejor.2016.08.066.———. 2017b. “ArulesViz: Interactive Visualization Association Rules R.” R Journal 9 (2): 163–75. https://doi.org/10.32614/RJ-2017-047.———. 2021. R Companion Introduction Data Mining. Online Book. https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book.———. 2023. ArulesViz: Visualizing Association Rules Frequent Itemsets. https://github.com/mhahsler/arulesViz.Hahsler, Michael, Christian Buchta, Bettina Gruen, Kurt Hornik. 2023. Arules: Mining Association Rules Frequent Itemsets. https://github.com/mhahsler/arules.Hahsler, Michael, Christian Buchta, Kurt Hornik. 2023. Seriation: Infrastructure Ordering Objects Using Seriation. https://github.com/mhahsler/seriation.Hahsler, Michael, Sudheer Chelluboina, Kurt Hornik, Christian Buchta. 2011. “Arules R-Package Ecosystem: Analyzing Interesting Patterns Large Transaction Datasets.” Journal Machine Learning Research 12: 1977–81. https://jmlr.csail.mit.edu/papers/v12/hahsler11a.html.Hahsler, Michael, Bettina Gruen, Kurt Hornik. 2005. “Arules – Computational Environment Mining Association Rules Frequent Item Sets.” Journal Statistical Software 14 (15): 1–25. https://doi.org/10.18637/jss.v014.i15.Hahsler, Michael, Bettina Grün, Kurt Hornik. 2005. “Arules – Computational Environment Mining Association Rules Frequent Item Sets.” Journal Statistical Software 14 (15): 1–25. http://www.jstatsoft.org/v14/i15/.Hahsler, Michael, Kurt Hornik, Christian Buchta. 2008. “Getting Things Order: Introduction R Package Seriation.” Journal Statistical Software 25 (3): 1–34. https://doi.org/10.18637/jss.v025.i03.Hahsler, Michael, Matthew Piekenbrock. 2023. Dbscan: Density-Based Spatial Clustering Applications Noise (Dbscan) Related Algorithms. https://github.com/mhahsler/dbscan.Hahsler, Michael, Matthew Piekenbrock, Derek Doran. 2019. “dbscan: Fast Density-Based Clustering R.” Journal Statistical Software 91 (1): 1–30. https://doi.org/10.18637/jss.v091.i01.Hennig, Christian. 2023. Fpc: Flexible Procedures Clustering. https://www.unibo./sitoweb/christian.hennig/en/.Hornik, Kurt. 2023. RWeka: R/Weka Interface. https://CRAN.R-project.org/package=RWeka.Hornik, Kurt, Christian Buchta, Achim Zeileis. 2009. “Open-Source Machine Learning: R Meets Weka.” Computational Statistics 24 (2): 225–32. https://doi.org/10.1007/s00180-008-0119-7.Karatzoglou, Alexandros, Alex Smola, Kurt Hornik. 2023. Kernlab: Kernel-Based Machine Learning Lab. https://CRAN.R-project.org/package=kernlab.Karatzoglou, Alexandros, Alex Smola, Kurt Hornik, Achim Zeileis. 2004. “Kernlab – S4 Package Kernel Methods R.” Journal Statistical Software 11 (9): 1–20. https://doi.org/10.18637/jss.v011.i09.Kassambara, Alboukadel. 2022. Ggcorrplot: Visualization Correlation Matrix Using Ggplot2. http://www.sthda.com/english/wiki/ggcorrplot-visualization---correlation-matrix-using-ggplot2.Kassambara, Alboukadel, Fabian Mundt. 2020. Factoextra: Extract Visualize Results Multivariate Data Analyses. http://www.sthda.com/english/rpkgs/factoextra.Kuhn, Max. 2023. Caret: Classification Regression Training. https://github.com/topepo/caret/.Kuhn, Max, Ross Quinlan. 2023. C50: C5.0 Decision Trees Rule-Based Models. https://topepo.github.io/C5.0/.Kuhn, Max. 2008. “Building Predictive Models R Using Caret Package.” Journal Statistical Software 28 (5): 1–26. https://doi.org/10.18637/jss.v028.i05.Leisch, Friedrich, Evgenia Dimitriadou. 2023. Mlbench: Machine Learning Benchmark Problems. https://CRAN.R-project.org/package=mlbench.Liaw, Andy, Matthew Wiener. 2002. “Classification Regression randomForest.” R News 2 (3): 18–22. https://CRAN.R-project.org/doc/Rnews/.Maechler, Martin, Peter Rousseeuw, Anja Struyf, Mia Hubert. 2022. Cluster: \"Finding Groups Data\": Cluster Analysis Extended Rousseeuw et Al. https://svn.r-project.org/R-packages/trunk/cluster/.Meyer, David, Christian Buchta. 2022. Proxy: Distance Similarity Measures. https://CRAN.R-project.org/package=proxy.Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, Friedrich Leisch. 2023. E1071: Misc Functions Department Statistics, Probability Theory Group (Formerly: E1071), Tu Wien. https://CRAN.R-project.org/package=e1071.Milborrow, Stephen. 2022. Rpart.plot: Plot Rpart Models: Enhanced Version Plot.rpart. http://www.milbo.org/rpart-plot/index.html.Müller, Kirill, Hadley Wickham. 2023. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.Newman, D. J., S. Hettich, C. L. Blake, C. J. Merz. 1998. “UCI Repository Machine Learning Databases.” University California, Irvine, Dept. Information; Computer Sciences. http://www.ics.uci.edu/~mlearn/MLRepository.html.R Core Team. 2023. R: Language Environment Statistical Computing. Vienna, Austria: R Foundation Statistical Computing. https://www.R-project.org/.Ripley, Brian. 2023a. MASS: Support Functions Datasets Venables Ripley’s Mass. http://www.stats.ox.ac.uk/pub/MASS4/.———. 2023b. Nnet: Feed-Forward Neural Networks Multinomial Log-Linear Models. http://www.stats.ox.ac.uk/pub/MASS4/.Robin, Xavier, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez, Markus Müller. 2011. “PROC: Open-Source Package R S+ Analyze Compare Roc Curves.” BMC Bioinformatics 12: 77.———. 2023. PROC: Display Analyze Roc Curves. http://expasy.org/tools/pROC/.Romanski, Piotr, Lars Kotthoff, Patrick Schratz. 2021. FSelector: Selecting Attributes. https://github.com/larskotthoff/fselector.Sarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization R. New York: Springer. http://lmdvr.r-forge.r-project.org.———. 2023. Lattice: Trellis Graphics R. https://lattice.r-forge.r-project.org/.Schloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, Jason Crowley. 2021. GGally: Extension Ggplot2. https://CRAN.R-project.org/package=GGally.Scrucca, Luca, Michael Fop, T. Brendan Murphy, Adrian E. Raftery. 2016. “mclust 5: Clustering, Classification Density Estimation Using Gaussian Finite Mixture Models.” R Journal 8 (1): 289–317. https://doi.org/10.32614/RJ-2016-021.Sievert, Carson. 2020. Interactive Web-Based Data Visualization R, Plotly, Shiny. Chapman; Hall/CRC. https://plotly-r.com.Sievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, Pedro Despouy. 2023. Plotly: Create Interactive Web Graphics via Plotly.js. https://CRAN.R-project.org/package=plotly.Spinu, Vitalie, Garrett Grolemund, Hadley Wickham. 2023. Lubridate: Make Dealing Dates Little Easier. https://CRAN.R-project.org/package=lubridate.Tan, Pang-Ning, Michael S. Steinbach, Anuj Karpatne, Vipin Kumar. 2017. Introduction Data Mining. 2nd Edition. Pearson. https://www-users.cs.umn.edu/~kumar001/dmbook.Tan, Pang-Ning, Michael S. Steinbach, Vipin Kumar. 2005. Introduction Data Mining. 1st Edition. Addison-Wesley. https://www-users.cs.umn.edu/~kumar001/dmbook/firsted.php.Therneau, Terry, Beth Atkinson. 2022. Rpart: Recursive Partitioning Regression Trees. https://CRAN.R-project.org/package=rpart.Tillé, Yves, Alina Matei. 2021. Sampling: Survey Sampling. https://CRAN.R-project.org/package=sampling.Venables, W. N., B. D. Ripley. 2002a. Modern Applied Statistics S. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.———. 2002b. Modern Applied Statistics S. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.Venables, W. N., D. M. Smith, R Core Team. 2021. Introduction R.Wickham, Hadley. 2016. Ggplot2: Elegant Graphics Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.———. 2022. Stringr: Simple, Consistent Wrappers Common String Operations. https://CRAN.R-project.org/package=stringr.———. 2023a. Forcats: Tools Working Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.———. 2023b. Tidyverse: Easily Install Load Tidyverse. https://CRAN.R-project.org/package=tidyverse.Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome tidyverse.” Journal Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington. 2023. Ggplot2: Create Elegant Data Visualisations Using Grammar Graphics. https://CRAN.R-project.org/package=ggplot2.Wickham, Hadley, Romain François, Lionel Henry, Kirill Müller, Davis Vaughan. 2023. Dplyr: Grammar Data Manipulation. https://CRAN.R-project.org/package=dplyr.Wickham, Hadley, Garrett Grolemund. 2017. R Data Science: Import, Tidy, Transform, Visualize, Model Data. 1st ed. O’Reilly Media, Inc. https://r4ds..co.nz/.Wickham, Hadley, Lionel Henry. 2023. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.Wickham, Hadley, Jim Hester, Jennifer Bryan. 2023. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.Wickham, Hadley, Dana Seidel. 2022. Scales: Scale Functions Visualization. https://CRAN.R-project.org/package=scales.Wickham, Hadley, Davis Vaughan, Maximilian Girlich. 2023. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.Wilkinson, Leland. 2005. Grammar Graphics (Statistics Computing). Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.Witten, Ian H., Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools Techniques. 2nd ed. San Francisco: Morgan Kaufmann.Yu, Guangchuang. 2023. Scatterpie: Scatter Pie Plot. https://CRAN.R-project.org/package=scatterpie.","code":""}]
