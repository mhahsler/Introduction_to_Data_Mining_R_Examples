[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"companion book contains documented R examples accompany several chapters\npopular data mining textbook Introduction Data\nMining Pang-Ning\nTan, Michael Steinbach, Anuj Karpatne Vipin Kumar.\nintended replacement textbook since cover \ntheory, guide accompanying textbook.\ncompanion\nbook can used either edition: 1st edition (Tan, Steinbach, Kumar 2005) 2nd\nedition (Tan et al. 2017). sections numbered match 2nd edition. Sections\nmarked asterisk additional content covered \ntextbook.code examples collected book developed course\nCS 5/7331 Data Mining taught\nadvanced undergraduate graduate level\nComputer Science Department \nSMU since Spring 2013 regularly\nupdated improved.\nlearning method used book learning--.\ncode examples throughout book \nwritten self-contained manner can copy paste portion code,\ntry provided dataset apply directly \ndata. Instructors can use companion component create \nintroduction data mining course advanced undergraduates graduate\nstudents proficient programming basic statistics knowledge.\ncomplete set slides (PDF PowerPoint) provided \nbook’s GitHub page.latest update includes use popular\npackages meta-package tidyverse (Wickham 2023c) including\nggplot2 (Wickham, Chang, et al. 2024) data wrangling visualization, along \ncaret (M. Kuhn 2023) model building evaluation.\nPlease use edit function within book visit book’s\nGitHub project\npage\nsubmit corrections suggest improvements. cite book, use:Michael Hahsler (2021). R Companion Introduction Data\nMining. Online Book.\nhttps://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/hope book helps learn use R efficiently \ndata mining projects.Michael Hahsler","code":""},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":" online version \nbook licensed Creative Commons\nAttribution-NonCommercial-ShareAlike 4.0 International\nLicense.cover art based \n“rocks” \nstebulus licensed CC\n\n2.0.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Data mining goal \nfinding patterns large data sets. popular data mining textbook\nIntroduction Data\nMining (Tan et al. 2017)\ncovers many important aspects data mining. companion contains\nannotated R code examples complement textbook. make following\nalong easier, follow chapters data mining textbook \norganized main data mining tasks:Data covers types data also includes data preparation \nexploratory data analysis chapter.Data covers types data also includes data preparation \nexploratory data analysis chapter.Classification: Basic Concepts Techniques introduces decision\ntrees, model training evaluation.Classification: Basic Concepts Techniques introduces decision\ntrees, model training evaluation.Classification: Alternative techniques introduces compares\nmethods including rule-based classifiers, nearest neighbor\nclassifiers, naive Bayes classifier, logistic regression \nartificial neural networks.Classification: Alternative techniques introduces compares\nmethods including rule-based classifiers, nearest neighbor\nclassifiers, naive Bayes classifier, logistic regression \nartificial neural networks.Association Analysis: Basic Concepts Algorithms covers\nfrequent itemset association rule generation analysis\nincluding visualization. code covers also topics discussed\ntextbook’s Chapter 6.Association Analysis: Basic Concepts Algorithms covers\nfrequent itemset association rule generation analysis\nincluding visualization. code covers also topics discussed\ntextbook’s Chapter 6.UnusedUnusedCluster Analysis discusses clustering approaches including\nk-means, hierarchical clustering, DBSCAN evaluate\nclustering results.Cluster Analysis discusses clustering approaches including\nk-means, hierarchical clustering, DBSCAN evaluate\nclustering results.completeness, added chapters [Regression*] \n[Logistic Regression*]. Sections book followed asterisk\ncontain code examples methods described data\nmining textbook.book assumes \nfamiliar basics R, run R code, install packages.\nrest chapter provide overview point \ncan learn R used packages.","code":""},{"path":"introduction.html","id":"used-software","chapter":"1 Introduction","heading":"1.1 Used Software","text":"use book need \nR RStudio\nDesktop installed.book chapter use set packages must installed. \ninstallation code can found beginning chapter. \ncode install packages used chapter:code examples book use R package collection tidyverse\n(Wickham 2023c) manipulate data. Tidyverse also includes package\nggplot2 (Wickham, Chang, et al. 2024) visualization. Tidyverse packages make\nworking data R convenient. Data analysis data mining\nreports typically done creating\nR Markdown documents. Everything R built top \ncore R programming language packages automatically\ninstalled R. referred Base-R.","code":"\npkgs <- c('tidyverse')\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"introduction.html","id":"base-r","chapter":"1 Introduction","heading":"1.2 Base-R","text":"Base-R covered detail Introduction \nR. \nimportant difference R many programming languages\nlike Python R functional (extensions) \nvectorized.","code":""},{"path":"introduction.html","id":"vectors","chapter":"1 Introduction","heading":"1.2.1 Vectors","text":"Scalars exist R. basic data structure vector real\nnumbers. Special values like infinity missing (NA) can used.","code":"\nx <- c(10.4, 5.6, Inf, NA, 21.7) # c combines values\nx\n## [1] 10.4  5.6  Inf   NA 21.7\ny <- seq(from = 0, to = 40, length.out = 5) # create a sequence\ny\n## [1]  0 10 20 30 40"},{"path":"introduction.html","id":"vectorized-operations","chapter":"1 Introduction","heading":"1.2.2 Vectorized Operations","text":"Operations vectorized loops typically necessary.Comparisons performed element wise return logical vector\n(R’s name Boolean).","code":"\nx + 1\n## [1] 11.4  6.6  Inf   NA 22.7\nx > y\n## [1]  TRUE FALSE  TRUE    NA FALSE"},{"path":"introduction.html","id":"subsetting-vectors","chapter":"1 Introduction","heading":"1.2.3 Subsetting Vectors","text":"can select vector elements using [ operator like \nprogramming languages index starts 1.can also assign values selection. example, get rid \ninfinite values.","code":"\nx[1:3] # elements 1 through 3\n## [1] 10.4  5.6  Inf\nx[-1] # all but the first\n## [1]  5.6  Inf   NA 21.7\nx[!is.na(x)] # all but the missing values\n## [1] 10.4  5.6  Inf 21.7\nx[!is.finite(x)] <- NA\nx\n## [1] 10.4  5.6   NA   NA 21.7"},{"path":"introduction.html","id":"functions","chapter":"1 Introduction","heading":"1.2.4 Functions","text":"Functions work like many languages. However, also operate\nvectorized arguments can specified positional named. \nincrement function can defined :Calling increment function vector x.R many built-functions like min(), max(), mean() \nsum().","code":"\ninc <- function(x, by = 1) { \n    x + by \n  }\ninc(x, by = 2)\n## [1] 12.4  7.6   NA   NA 23.7"},{"path":"introduction.html","id":"strings","chapter":"1 Introduction","heading":"1.2.5 Strings","text":"R uses character vectors character stands string \nlike programming language single character. R accepts\ndouble single quotation marks strings.Strings can combined using paste().Note paste vectorized string \"World!\" used twice \nmatch length string. behavior called R recycling\nworks one vector’s length exact multiple \nvector’s length. special case one vector one element \nparticularly often used. used already expression\nx + 1 one recycled element x.","code":"\nstring <- c(\"Hello\", \"Goodbye\")\nstring\n## [1] \"Hello\"   \"Goodbye\"\npaste(string, \"World!\")\n## [1] \"Hello World!\"   \"Goodbye World!\""},{"path":"introduction.html","id":"plotting","chapter":"1 Introduction","heading":"1.2.6 Plotting","text":"Basic plotting Base-R done calling plot().plot functions pairs(), hist(), barplot(). book,\nfocus plots created ggplot2 package.","code":"\nplot(x, y)"},{"path":"introduction.html","id":"objects","chapter":"1 Introduction","heading":"1.2.7 Objects","text":"often used data structures include list, data.frame, matrix,\nfactor. R, everything object. Objects printed \neither just using object’s name put explicitly \nprint() function.many objects, summary can created.Objects class.functions generic, meaning something else\ndepending class first argument. example, plot() \nmany methods implemented visualize data different classes. \nspecific function specified period method name.\nalso exists default method. plot, method \nplot.default(). Different methods may manual page, \nspecifying class dot can help finding documentation.often useful know information stored object.\nstr() returns humanly readable string representation object.much learn R. highly recommended go \nofficial\nIntroduction \nR manual.\nalso good Base R Cheat Sheet\navailable.","code":"\nx\n## [1] 10.4  5.6   NA   NA 21.7\nsummary(x)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##     5.6     8.0    10.4    12.6    16.1    21.7       2\nclass(x)\n## [1] \"numeric\"\nstr(x)\n##  num [1:5] 10.4 5.6 NA NA 21.7"},{"path":"introduction.html","id":"r-markdown","chapter":"1 Introduction","heading":"1.3 R Markdown","text":"R Markdown simple method include R code inside text document\nwritten using markdown syntax. Using R markdown especially convention analyze\ndata compose data mining report. RStudio makes creating \ntranslating R Markdown document easy. Just choose\nFile -> New File -> R Markdown... RStudio create small\ndemo markdown document already includes examples code \ninclude plots. can switch visual mode prefer \nSee Get editor.convert R markdown document HTML, PDF Word, just click \nKnit button. code executed combined text\ncomplete document.Examples detailed documentation can found RStudio’s R Markdown\nwebsite R Markdown Cheatsheet.","code":""},{"path":"introduction.html","id":"tidyverse","chapter":"1 Introduction","heading":"1.4 Tidyverse","text":"tidyverse (Wickham 2023c) collection many useful packages \nwork well together sharing design principles data structures.\ntidyverse also includes ggplot2 (Wickham, Chang, et al. 2024) visualization.book, useoften tidyverse tibbles replace R’s built-data.frames,pipe operator |> chain functions together, anddata transformation functions like filter(), arrange(),\nselect(), group_by(), mutate() provided tidyverse\npackage dplyr (Wickham, François, et al. 2023).good introduction can found Section Data\nTransformation (Wickham, Çetinkaya-Rundel, Grolemund 2023).Load tidyverse packages.","code":"\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"},{"path":"introduction.html","id":"tibbles","chapter":"1 Introduction","heading":"1.4.1 Tibbles","text":"short example analyses vitamin C content different\nfruits. get familiar basic syntax. Data tables \nR called data.frames. Tidyverse introduces version called\ntibbles. create tibble price dollars per pound \nvitamin C content milligrams (mg) per pound four different types\nfruit.","code":"\nfruit <- tibble(\n  name = c(\"apple\", \"banana\", \"mango\", \"orange\"), \n  price = c(2.5, 2.0, 4.0, 3.5), \n  vitamin_c = c(20, 45, 130, 250))\nfruit\n## # A tibble: 4 × 3\n##   name   price vitamin_c\n##   <chr>  <dbl>     <dbl>\n## 1 apple    2.5        20\n## 2 banana   2          45\n## 3 mango    4         130\n## 4 orange   3.5       250"},{"path":"introduction.html","id":"transformations","chapter":"1 Introduction","heading":"1.4.2 Transformations","text":"Now, can add column vitamin C (mg) dollar buys\nusing mutate() change table. filter rows \nfruit provides 20 mg, finally arrange data\nrows vitamin C per dollar largest smallest.pipes operator |> lets pass value left (often \nresult function) first argument function \nright. makes composing sequence function calls transform\ndata much easier write read. code starts fruit\ndata pipes three transformation functions. final\nresult assigned <- variable.can create summary statistics price using summarize().Often, want apply function multiple columns. can\nachieved using across().dplyr syntax evaluation slightly different standard R\nmay lead confusion. One example column names can \nreferences without quotation marks. useful reference resource\nworking dplyr RStudio Data Transformation Cheatsheet\ncovers two pages almost everything need also\ncontains contains simple example code can take modify \nuse case.","code":"\naffordable_vitamin_c_sources <- fruit |>\n  mutate(vitamin_c_per_dollar = vitamin_c / price) |> \n  filter(vitamin_c_per_dollar > 20) |>\n  arrange(desc(vitamin_c_per_dollar))\n\naffordable_vitamin_c_sources \n## # A tibble: 3 × 4\n##   name   price vitamin_c vitamin_c_per_dollar\n##   <chr>  <dbl>     <dbl>                <dbl>\n## 1 orange   3.5       250                 71.4\n## 2 mango    4         130                 32.5\n## 3 banana   2          45                 22.5\naffordable_vitamin_c_sources |> \n  summarize(min = min(price), \n            mean = mean(price), \n            max = max(price))\n## # A tibble: 1 × 3\n##     min  mean   max\n##   <dbl> <dbl> <dbl>\n## 1     2  3.17     4\naffordable_vitamin_c_sources |> \n  summarize(across(c(price, vitamin_c), mean))\n## # A tibble: 1 × 2\n##   price vitamin_c\n##   <dbl>     <dbl>\n## 1  3.17      142."},{"path":"introduction.html","id":"ggplot2","chapter":"1 Introduction","heading":"1.4.3 ggplot2","text":"visualization, use mainly ggplot2. gg ggplot2\nstands Grammar Graphics introduced Wilkinson (2005). \nmain idea every graph built basic components:data,coordinate system, andvisual marks representing data (geoms).ggplot2, components combined using + operator.Since typically use Cartesian coordinate system, ggplot uses \ndefault. geom_ function uses stat_ function calculate\nvisualizes. example, geom_bar uses stat_count create\nbar chart counting often value appears data (see\n? geom_bar). geom_point just uses stat \"identity\" display\npoints using coordinates . great introduction can\nfound Section Data\nVisualization (Wickham, Çetinkaya-Rundel, Grolemund 2023),\nuseful RStudio’s Data Visualization Cheatsheet.can visualize fruit data scatter plot.easy add geoms. example, can add regression line\nusing geom_smooth method \"lm\" (linear model). suppress\nconfidence interval since 3 data points.Alternatively, can visualize fruit’s vitamin C content per\ndollar using bar chart.Note geom_bar default uses stat_count function \naggregate data counting, just want visualize value \ntibble, specify identity function instead.","code":"\nggplot(data, mapping = aes(x = ..., y = ..., color = ...)) +\n  geom_point()\nggplot(fruit, aes(x = price, y = vitamin_c)) + \n  geom_point()\nggplot(fruit, aes(x = price, y = vitamin_c)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n## `geom_smooth()` using formula = 'y ~ x'\nggplot(fruit, aes(x = name, y = vitamin_c)) + \n  geom_bar(stat = \"identity\")"},{"path":"data.html","id":"data","chapter":"2 Data","heading":"2 Data","text":"chapter provides examples cleaning preparing data data\nmining. addition, data exploration covered chapter.Install packages used chapter:packages used chapter : arules (Hahsler et al. 2023), caret (M. Kuhn 2023), factoextra (Kassambara Mundt 2020), GGally (Schloerke et al. 2024), ggcorrplot (Kassambara 2023), hexbin (Carr, Lewin-Koh, Maechler 2023), palmerpenguins (Horst, Hill, Gorman 2022), plotly (Sievert et al. 2024), proxy (Meyer Buchta 2022), sampling (Tillé Matei 2023), seriation (Hahsler, Buchta, Hornik 2024), tidyverse (Wickham 2023c)Data data mining typically organized tabular form, rows containing\nobjects interest columns representing attributes describing objects.\ndiscuss topics like data quality, sampling, feature selection,\nmeasure similarities objects features.\nsecond part chapter deals data exploration visualization.","code":"\npkgs <- c(\"arules\", \"caret\", \"factoextra\", \"GGally\", \n          \"ggcorrplot\", \"hexbin\", \"palmerpenguins\", \"plotly\", \n          \"proxy\", \"sampling\", \"seriation\", \"tidyverse\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"data.html","id":"types-of-data","chapter":"2 Data","heading":"2.1 Types of Data","text":"","code":""},{"path":"data.html","id":"attributes-and-measurement","chapter":"2 Data","heading":"2.1.1 Attributes and Measurement","text":"values features can measured several scales\nranging \nsimple labels way numbers. scales come four levels.scales build meaning ordinal variable also characteristics \nnominal variable added order information.\noften differentiate interval ratio scale rarely\nneed calculate percentages statistics require meaningful zero value.\nfollowing code gives example vectors nominal, ordinal (levels defines order)\ninterval scale.","code":"\nfactor(c(\"red\", \"green\", \"green\", \"blue\"))\n## [1] red   green green blue \n## Levels: blue green red\n\nfactor(\"S\", \"L\", \"M\", \"S\", levels = c(\"S\", \"M\", \"L\"), ordered = TRUE)\n## [1] L1\n## Levels: L1 < L2\n\nc(1, 2, 3, 4, 3)\n## [1] 1 2 3 4 3"},{"path":"data.html","id":"the-iris-dataset","chapter":"2 Data","heading":"2.1.2 The Iris Dataset","text":"use toy dataset comes R. Fisher’s iris\ndataset gives \nmeasurements centimeters variables sepal length, sepal width\npetal length, petal width representing features 150 flowers (objects).\ndataset contains 50\nflowers 3 species iris. species Iris Setosa,\nIris Versicolor, Iris Virginica. details see ? iris.load iris data set. Datasets come R R packages can\nloaded data(). standard format data R \ndata.frame. convert data.frame tidyverse tibble.see data contains 150 rows (flowers) 5 features. tibbles\nshow first rows show features, \nfit screen width. can call print define many rows\nshow using parameter n force print show features \nchanging width infinity.","code":"\nlibrary(tidyverse)\ndata(iris)\niris <- as_tibble(iris)\niris\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # ℹ 140 more rows\nprint(iris, n = 3, width = Inf)\n## # A tibble: 150 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          5.1         3.5          1.4         0.2 setosa \n## 2          4.9         3            1.4         0.2 setosa \n## 3          4.7         3.2          1.3         0.2 setosa \n## # ℹ 147 more rows"},{"path":"data.html","id":"data-quality","chapter":"2 Data","heading":"2.2 Data Quality","text":"Assessing quality available data crucial start\nusing data. Start summary statistics column \nidentify outliers missing values. easiest way use base R\nfunction summary().can also summarize\nindividual columns using tidyverse’s dplyr functions.Using across(), multiple columns can summarized. Un following,\ncalculate numeric columns using mean function.find outliers data problems, need look small\nvalues (often suspicious large number zeros) using min \nextremely large values using max. Comparing median mean tells us \ndistribution symmetric.visual method inspect data use scatterplot matrix (\nuse ggpairs() package GGally). plot, can\nvisually identify noise data points outliers (points far\nmajority points).useful visualization combines many visualizations used understand \ndata check quality issues. Rows columns features data.\nalso specified aesthetic want group species using different color.visualizations diagonal panels show \nsmoothed histograms distribution feature.\nplot tries pick good number bins histogram\n(see messages ). distribution can checked close normal,\nunimodal highly skewed. Also, can see different groups overlapping\nseparable feature. example, three distributions Sepal.Width\nalmost identical meaning hard distinguish \ndifferent species using feature alone. Petal.Lenght Petal.Width\nmuch better.visualizations diagonal panels show \nsmoothed histograms distribution feature.\nplot tries pick good number bins histogram\n(see messages ). distribution can checked close normal,\nunimodal highly skewed. Also, can see different groups overlapping\nseparable feature. example, three distributions Sepal.Width\nalmost identical meaning hard distinguish \ndifferent species using feature alone. Petal.Lenght Petal.Width\nmuch better.lower-left triangle panels contain scatterplots pairs features. \nuseful see features correlated (pearson correlation\ncoefficient printed upper-right triangle). example,\nPetal.Length Petal.Width highly correlated overall\nmakes sense since larger plants longer wider petals.\nInside Setosa group correlation lot weaker.\ncan also see groups \nwell separated using projections two variables. Almost panels show \nSetosa forms point cloud well separated two classes \nVersicolor Virginica overlap.\ncan also see outliers far data points group.\nSee can spot one red dot far away others.lower-left triangle panels contain scatterplots pairs features. \nuseful see features correlated (pearson correlation\ncoefficient printed upper-right triangle). example,\nPetal.Length Petal.Width highly correlated overall\nmakes sense since larger plants longer wider petals.\nInside Setosa group correlation lot weaker.\ncan also see groups \nwell separated using projections two variables. Almost panels show \nSetosa forms point cloud well separated two classes \nVersicolor Virginica overlap.\ncan also see outliers far data points group.\nSee can spot one red dot far away others.last row/column represents data set class label Species.\nnominal variable plots different. bottom row panels\nshow (regular) histograms. last column shows boxplots represent\ndistribution different features group. Dots represent\noutliers. Finally, bottom-right panel contains counts different\ngroups barplot. data set, group number observations.last row/column represents data set class label Species.\nnominal variable plots different. bottom row panels\nshow (regular) histograms. last column shows boxplots represent\ndistribution different features group. Dots represent\noutliers. Finally, bottom-right panel contains counts different\ngroups barplot. data set, group number observations.Many data mining methods require complete data, data \ncontain missing values (NA). remove missing values duplicates\n(identical data points might mistake data), often\n:Note one non-unique case gone leaving 149 flowers. data\ncontain missing values, , also \ndropped. Typically, spend lot time data cleaning.","code":"\nsummary(iris)\n##   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  \n##  Median :5.80   Median :3.00   Median :4.35   Median :1.3  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \niris |> \n  summarize(mean = mean(Sepal.Length))\n## # A tibble: 1 × 1\n##    mean\n##   <dbl>\n## 1  5.84\niris |> \n  summarize(across(where(is.numeric), mean))\n## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         5.84        3.06         3.76        1.20\nlibrary(GGally)\n## Registered S3 method overwritten by 'GGally':\n##   method from   \n##   +.gg   ggplot2\nggpairs(iris, aes(color = Species), progress = FALSE)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nclean.data <- iris |> \n  drop_na() |> \n  unique()\n\nsummary(clean.data)\n##   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  \n##  Median :5.80   Median :3.00   Median :4.30   Median :1.3  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.75   Mean   :1.2  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :49  \n##                 \n##                 \n## "},{"path":"data.html","id":"data-preprocessing","chapter":"2 Data","heading":"2.3 Data Preprocessing","text":"","code":""},{"path":"data.html","id":"aggregation","chapter":"2 Data","heading":"2.3.1 Aggregation","text":"Data often contains groups want compare groups. group\niris dataset species calculate summary statistic \ngroup.Using information, can compare features differ \ngroups.","code":"\niris |> \n  group_by(Species) |> \n  summarize(across(everything(), mean))\n## # A tibble: 3 × 5\n##   Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <fct>             <dbl>       <dbl>        <dbl>       <dbl>\n## 1 setosa             5.01        3.43         1.46       0.246\n## 2 versicolor         5.94        2.77         4.26       1.33 \n## 3 virginica          6.59        2.97         5.55       2.03\n\niris |> \n  group_by(Species) |> \n  summarize(across(everything(), median))\n## # A tibble: 3 × 5\n##   Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <fct>             <dbl>       <dbl>        <dbl>       <dbl>\n## 1 setosa              5           3.4         1.5          0.2\n## 2 versicolor          5.9         2.8         4.35         1.3\n## 3 virginica           6.5         3           5.55         2"},{"path":"data.html","id":"sampling","chapter":"2 Data","heading":"2.3.2 Sampling","text":"Sampling often\nused data mining reduce dataset size modeling \nvisualization.","code":""},{"path":"data.html","id":"random-sampling","chapter":"2 Data","heading":"2.3.2.1 Random Sampling","text":"built-sample function can sample vector. sample\nreplacement.often want sample rows dataset. can done \nsampling without replacement vector row indices (using \nfunctions seq() nrow()). sample vector used \nsubset rows dataset.dplyr tidyverse lets us sample rows tibbles directly using\nslice_sample(). set random number generator seed make \nresults reproducible.","code":"\nsample(c(\"A\", \"B\", \"C\"), size = 10, replace = TRUE)\n##  [1] \"A\" \"A\" \"C\" \"B\" \"C\" \"A\" \"C\" \"B\" \"C\" \"B\"\ntake <- sample(seq(nrow(iris)), size = 15)\ntake\n##  [1] 132  95  72 108  23  25 101  33  44  76  59 119 131  28  41\n\niris[take, ]\n## # A tibble: 15 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n##  1          7.9         3.8          6.4         2   virginica \n##  2          5.6         2.7          4.2         1.3 versicolor\n##  3          6.1         2.8          4           1.3 versicolor\n##  4          7.3         2.9          6.3         1.8 virginica \n##  5          4.6         3.6          1           0.2 setosa    \n##  6          4.8         3.4          1.9         0.2 setosa    \n##  7          6.3         3.3          6           2.5 virginica \n##  8          5.2         4.1          1.5         0.1 setosa    \n##  9          5           3.5          1.6         0.6 setosa    \n## 10          6.6         3            4.4         1.4 versicolor\n## 11          6.6         2.9          4.6         1.3 versicolor\n## 12          7.7         2.6          6.9         2.3 virginica \n## 13          7.4         2.8          6.1         1.9 virginica \n## 14          5.2         3.5          1.5         0.2 setosa    \n## 15          5           3.5          1.3         0.3 setosa\nset.seed(1000)\ns <- iris |> \n  slice_sample(n = 15)\n\nlibrary(GGally)\nggpairs(s, aes(color = Species), progress = FALSE)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"data.html","id":"stratified-sampling","chapter":"2 Data","heading":"2.3.2.2 Stratified Sampling","text":"Stratified sampling\nmethod sampling population can partitioned \nsubpopulations, controlling proportions subpopulation\nresulting sample.following, subpopulations different types species\nwant make sure sample number (5) flowers \n. library sampling provides function stratified\nsampling. column ID_unit resulting data.frame contains \nrow numbers sampled rows. can use slice() dplyr \nselect sampled rows.","code":"\nlibrary(sampling)\nid2 <- strata(iris, stratanames = \"Species\", \n              size = c(5,5,5), method = \"srswor\")\nid2\n##        Species ID_unit Prob Stratum\n## 7       setosa       7  0.1       1\n## 9       setosa       9  0.1       1\n## 10      setosa      10  0.1       1\n## 24      setosa      24  0.1       1\n## 48      setosa      48  0.1       1\n## 58  versicolor      58  0.1       2\n## 62  versicolor      62  0.1       2\n## 74  versicolor      74  0.1       2\n## 78  versicolor      78  0.1       2\n## 99  versicolor      99  0.1       2\n## 106  virginica     106  0.1       3\n## 107  virginica     107  0.1       3\n## 127  virginica     127  0.1       3\n## 135  virginica     135  0.1       3\n## 145  virginica     145  0.1       3\n\ns2 <- iris |> \n  slice(id2$ID_unit)\n\nggpairs(s2, aes(color = Species), progress = FALSE)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"data.html","id":"dimensionality-reduction","chapter":"2 Data","heading":"2.3.3 Dimensionality Reduction","text":"number features often called dimensional data following \nidea feature (least numeric features) can seen axis data.Common feature preprocessing includes\ndimensionality reduction\ntries represent high-dimensional data low-dimensional space \nlow-dimensional representation retains meaningful properties (e.g., information \nsimilarity distances) original data. special case feature selection.important feature preprocessing includes discretization feature scaling.","code":""},{"path":"data.html","id":"principal-components-analysis-pca","chapter":"2 Data","heading":"2.3.3.1 Principal Components Analysis (PCA)","text":"PCA\ncalculates principal components (set new orthonormal basis vectors\ndata space) data points first principal\ncomponent explains variability data, second next\n. data analysis, PCA used project\nhigh-dimensional data points onto first (typically two)\nprincipal components visualization scatter plot \npreprocessing modeling (e.g., k-means clustering). Points\ncloser together high-dimensional original space, tend\nalso closer together projected lower-dimensional space,can use interactive 3-d plot (package plotly) look \nthree four dimensions iris dataset. Note hard\nvisualize 3 dimensions.principal components can calculated matrix using \nfunction prcomp(). select numeric columns (unselecting \nspecies column) convert tibble matrix \ncalculation.important principal component can also seen using \nscree plot. plot\nfunction result prcomp function visualizes much\nvariability data explained additional principal\ncomponent.Note first principal component (PC1) explains \nvariability iris dataset.find information stored object pc, can\ninspect raw object (display structure).object pc (like objects R) list class\nattribute. list element x contains data points projected \nprincipal components. can convert matrix tibble \nadd species column original dataset back (since rows\norder), display data projected first\ntwo principal components.Flowers displayed close together projection also\nclose together original 4-dimensional space. Since first\nprincipal component represents variability, can also show\ndata projected PC1.see can perfectly separate species Setosa using just \nfirst principal component. two species harder separate.plot projected data original axes added arrows \ncalled biplot. arrows\n(original axes) align roughly axes projection, \ncorrelated (linearly dependent).can also display old new axes.see Petal.Width Petal.Length point direction \nindicates highly correlated. also roughly aligned\nPC1 (called Dim1 plot) means PC1 represents \nvariability two variables. Sepal.Width almost aligned\ny-axis therefore represented PC2 (Dim2).\nPetal.Width/Petal.Length Sepal.Width almost 90 degrees,\nindicating close uncorrelated. Sepal.Length \ncorrelated variables represented , PC1 PC2\nprojection.exist methods embed data higher dimensions \nlower-dimensional space. popular method project data lower\ndimensions visualization t-distributed stochastic neighbor\nembedding (t-SNE) available package Rtsne.","code":"\n# library(plotly) # I don't load the package because it's namespace clashes with select in dplyr.\nplotly::plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, z = ~Sepal.Width, \n      color = ~Species, size = 1) |> \n  plotly::add_markers()\npc <- iris |> \n  select(-Species) |> \n  as.matrix() |> \n  prcomp()\nsummary(pc)\n## Importance of components:\n##                          PC1    PC2    PC3     PC4\n## Standard deviation     2.056 0.4926 0.2797 0.15439\n## Proportion of Variance 0.925 0.0531 0.0171 0.00521\n## Cumulative Proportion  0.925 0.9777 0.9948 1.00000\nplot(pc, type = \"line\")\nstr(pc)\n## List of 5\n##  $ sdev    : num [1:4] 2.056 0.493 0.28 0.154\n##  $ rotation: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 -0.6566 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n##   .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\"\n##  $ center  : Named num [1:4] 5.84 3.06 3.76 1.2\n##   ..- attr(*, \"names\")= chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n##  $ scale   : logi FALSE\n##  $ x       : num [1:150, 1:4] -2.68 -2.71 -2.89 -2.75 -2.73 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\"\n##  - attr(*, \"class\")= chr \"prcomp\"\niris_projected <- as_tibble(pc$x) |> \n  add_column(Species = iris$Species)\n\nggplot(iris_projected, aes(x = PC1, y = PC2, color = Species)) + \n  geom_point()\nggplot(iris_projected, \n  aes(x = PC1, y = 0, color = Species)) + \n  geom_point() +\n  scale_y_continuous(expand=c(0,0)) +\n  theme(axis.text.y = element_blank(),\n      axis.title.y = element_blank()\n  )\nlibrary(factoextra)\n## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_pca(pc)\nfviz_pca_var(pc)"},{"path":"data.html","id":"multi-dimensional-scaling-mds","chapter":"2 Data","heading":"2.3.3.2 Multi-Dimensional Scaling (MDS)","text":"MDS similar\nPCA. Instead data points, starts pairwise distances (.e.,\ndistance matrix) produces space points placed \nrepresent distances well possible. axes space\ncalled components similar principal components \nPCA.First, calculate distance matrix (Euclidean distances) 4-d\nspace iris dataset.Metric (classic) MDS tries construct space points lower\ndistances placed closer together. project data represented \ndistance matrix k = 2 dimensions.resulting projection similar (except rotation reflection)\nresult projection using PCA.","code":"\nd <- iris |> \n  select(-Species) |> \n  dist()\nfit <- cmdscale(d, k = 2)\ncolnames(fit) <- c(\"comp1\", \"comp2\")\nfit <- as_tibble(fit) |> \n  add_column(Species = iris$Species)\n\nggplot(fit, aes(x = comp1, y = comp2, color = Species)) + geom_point()"},{"path":"data.html","id":"non-parametric-multidimensional-scaling","chapter":"2 Data","heading":"2.3.3.3 Non-Parametric Multidimensional Scaling","text":"Non-parametric multidimensional scaling performs MDS relaxing \nneed linear relationships. Methods available package MASS \nfunctions isoMDS() sammon().","code":""},{"path":"data.html","id":"feature-subset-selection","chapter":"2 Data","heading":"2.3.4 Feature Subset Selection","text":"Feature selection process identifying features \nused create model. talk feature selection \ndiscuss classification models Chapter 3 Feature Selection*.","code":""},{"path":"data.html","id":"discretization","chapter":"2 Data","heading":"2.3.5 Discretization","text":"data mining methods require discrete data. Discretization converts\ncontinuous features discrete features. example, \ndiscretize continuous feature Petal.Width. perform\ndiscretization, look distribution see gives\nus idea group continuous values set \ndiscrete values. histogram visualizes distribution single\ncontinuous feature.bins histogram represent discretization using fixed bin\nwidth. R function cut() performs equal interval width\ndiscretization.discretization methods include equal frequency discretization \nusing k-means clustering. methods implemented several R\npackages. use implementation package arules \nvisualize results histograms blue lines separate\nintervals assigned discrete value.user needs decide number intervals used method.","code":"\nggplot(iris, aes(x = Petal.Width)) + geom_histogram(binwidth = .2)\niris |> \n  pull(Sepal.Width) |> \n  cut(breaks = 3)\n##   [1] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (3.6,4.4]\n##   [7] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (3.6,4.4] (2.8,3.6]\n##  [13] (2.8,3.6] (2.8,3.6] (3.6,4.4] (3.6,4.4] (3.6,4.4] (2.8,3.6]\n##  [19] (3.6,4.4] (3.6,4.4] (2.8,3.6] (3.6,4.4] (2.8,3.6] (2.8,3.6]\n##  [25] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [31] (2.8,3.6] (2.8,3.6] (3.6,4.4] (3.6,4.4] (2.8,3.6] (2.8,3.6]\n##  [37] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]  \n##  [43] (2.8,3.6] (2.8,3.6] (3.6,4.4] (2.8,3.6] (3.6,4.4] (2.8,3.6]\n##  [49] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]  \n##  [55] (2,2.8]   (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [61] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [67] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [73] (2,2.8]   (2,2.8]   (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6]\n##  [79] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2,2.8]   (2,2.8]  \n##  [85] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [91] (2,2.8]   (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n##  [97] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]   (2.8,3.6] (2,2.8]  \n## [103] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6]\n## [109] (2,2.8]   (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n## [115] (2,2.8]   (2.8,3.6] (2.8,3.6] (3.6,4.4] (2,2.8]   (2,2.8]  \n## [121] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6] (2.8,3.6]\n## [127] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (3.6,4.4]\n## [133] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## [139] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6]\n## [145] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## Levels: (2,2.8] (2.8,3.6] (3.6,4.4]\nlibrary(arules)\n## Loading required package: Matrix\n## \n## Attaching package: 'Matrix'\n## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack\n## \n## Attaching package: 'arules'\n## The following object is masked from 'package:dplyr':\n## \n##     recode\n## The following objects are masked from 'package:base':\n## \n##     abbreviate, write\niris |> pull(Petal.Width) |> \n  discretize(method = \"interval\", breaks = 3)\n##   [1] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##   [7] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [13] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [19] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [25] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [31] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [37] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [43] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [49] [0.1,0.9) [0.1,0.9) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [55] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [61] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [67] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [1.7,2.5] [0.9,1.7)\n##  [73] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [1.7,2.5]\n##  [79] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [85] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [91] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [97] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [1.7,2.5] [1.7,2.5]\n## [103] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [109] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [115] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7)\n## [121] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [127] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) [1.7,2.5] [1.7,2.5]\n## [133] [1.7,2.5] [0.9,1.7) [0.9,1.7) [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [139] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [145] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## attr(,\"discretized:breaks\")\n## [1] 0.1 0.9 1.7 2.5\n## attr(,\"discretized:method\")\n## [1] interval\n## Levels: [0.1,0.9) [0.9,1.7) [1.7,2.5]\n\niris |> pull(Petal.Width) |> \n  discretize(method = \"frequency\", breaks = 3)\n##   [1] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##   [6] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [11] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [16] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [21] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [26] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [31] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [36] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [41] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [46] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [51] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [56] [0.867,1.6) [1.6,2.5]   [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [61] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [66] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [71] [1.6,2.5]   [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [76] [0.867,1.6) [0.867,1.6) [1.6,2.5]   [0.867,1.6) [0.867,1.6)\n##  [81] [0.867,1.6) [0.867,1.6) [0.867,1.6) [1.6,2.5]   [0.867,1.6)\n##  [86] [1.6,2.5]   [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [91] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [96] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n## [101] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [106] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [111] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [116] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [0.867,1.6)\n## [121] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [126] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [131] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [0.867,1.6) [0.867,1.6)\n## [136] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [141] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [146] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## attr(,\"discretized:breaks\")\n## [1] 0.100 0.867 1.600 2.500\n## attr(,\"discretized:method\")\n## [1] frequency\n## Levels: [0.1,0.867) [0.867,1.6) [1.6,2.5]\n\niris |> pull(Petal.Width) |> \n  discretize(method = \"cluster\", breaks = 3)\n##   [1] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##   [6] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [11] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [16] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [21] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [26] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [31] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [36] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [41] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [46] [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [51] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [56] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [61] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [66] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [71] [1.71,2.5]   [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [76] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [81] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [86] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [91] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [96] [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71) [0.792,1.71)\n## [101] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [106] [1.71,2.5]   [0.792,1.71) [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [111] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [116] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [0.792,1.71)\n## [121] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [126] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [0.792,1.71)\n## [131] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [0.792,1.71) [0.792,1.71)\n## [136] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [141] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [146] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## attr(,\"discretized:breaks\")\n## [1] 0.100 0.792 1.705 2.500\n## attr(,\"discretized:method\")\n## [1] cluster\n## Levels: [0.1,0.792) [0.792,1.71) [1.71,2.5]\n\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept = iris |> pull(Petal.Width) |> \n        discretize(method = \"interval\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: interval\", \n       subtitle = \"Blue lines are boundaries\")\n\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept = iris |> pull(Petal.Width) |> \n        discretize(method = \"frequency\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: frequency\", \n       subtitle = \"Blue lines are boundaries\")\n\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept = iris |> pull(Petal.Width) |> \n               discretize(method = \"cluster\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: cluster\", \n       subtitle = \"Blue lines are boundaries\")"},{"path":"data.html","id":"variable-transformation-standardization","chapter":"2 Data","heading":"2.3.6 Variable Transformation: Standardization","text":"Standardizing (scaling, normalizing) range features values \nimportant make comparable. popular method convert\nvalues feature \nz-scores. subtracting\nmean (centering) dividing standard deviation (scaling).\nstandardized feature mean zero measured \nstandard deviations mean. Positive values indicate many\nstandard deviation original feature value average.\nNegative standardized values indicate -average values.Tidyverse currently simple scale function, \nmake one provides wrapper standard scale function R.\nmutates numeric columns using anonymous function\nuses base R scale function converts result vector.standardized feature mean zero “normal” values\nfall range \\([-3,3]\\) measured standard deviations average.\nNegative values mean smaller average positive values mean larger average.","code":"\nscale_numeric <- function(x) \n  x |> \n  mutate(across(where(is.numeric), function(y) as.vector(scale(y))))\niris.scaled <- iris |> \n  scale_numeric()\niris.scaled\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1       -0.898      1.02          -1.34       -1.31 setosa \n##  2       -1.14      -0.132         -1.34       -1.31 setosa \n##  3       -1.38       0.327         -1.39       -1.31 setosa \n##  4       -1.50       0.0979        -1.28       -1.31 setosa \n##  5       -1.02       1.25          -1.34       -1.31 setosa \n##  6       -0.535      1.93          -1.17       -1.05 setosa \n##  7       -1.50       0.786         -1.34       -1.18 setosa \n##  8       -1.02       0.786         -1.28       -1.31 setosa \n##  9       -1.74      -0.361         -1.34       -1.31 setosa \n## 10       -1.14       0.0979        -1.28       -1.44 setosa \n## # ℹ 140 more rows\nsummary(iris.scaled)\n##   Sepal.Length     Sepal.Width      Petal.Length     Petal.Width    \n##  Min.   :-1.864   Min.   :-2.426   Min.   :-1.562   Min.   :-1.442  \n##  1st Qu.:-0.898   1st Qu.:-0.590   1st Qu.:-1.222   1st Qu.:-1.180  \n##  Median :-0.052   Median :-0.132   Median : 0.335   Median : 0.132  \n##  Mean   : 0.000   Mean   : 0.000   Mean   : 0.000   Mean   : 0.000  \n##  3rd Qu.: 0.672   3rd Qu.: 0.557   3rd Qu.: 0.760   3rd Qu.: 0.788  \n##  Max.   : 2.484   Max.   : 3.080   Max.   : 1.780   Max.   : 1.706  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## "},{"path":"data.html","id":"measures-of-similarity-and-dissimilarity","chapter":"2 Data","heading":"2.4 Measures of Similarity and Dissimilarity","text":"Proximities help quantifying similar two objects .\nSimilariy concept geometry.\nbest-known\nway define similarity Euclidean distance, proximities can measured \ndifferent ways depending information objects.R stores proximity dissimilarities/distances matrices. Similarities\nfirst converted dissimilarities. Distances symmetric, .e.,\ndistance B distance B . R\ntherefore stores triangle (typically lower triangle) \ndistance matrix.","code":""},{"path":"data.html","id":"minkowsky-distances","chapter":"2 Data","heading":"2.4.1 Minkowsky Distances","text":"Minkowsky\ndistance family\nmetric distances including Euclidean Manhattan distance. avoid\none feature dominate distance calculation, scaled data \ntypically used. select first 5 flowers example.Different types Minkowsky distance matrices first 5\nflowers can calculated using dist().see lower triangle distance matrices stored\n(note rows start row 2).","code":"\niris_sample <- iris.scaled |> \n  select(-Species) |> \n  slice(1:5)\niris_sample\n## # A tibble: 5 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1       -0.898      1.02          -1.34       -1.31\n## 2       -1.14      -0.132         -1.34       -1.31\n## 3       -1.38       0.327         -1.39       -1.31\n## 4       -1.50       0.0979        -1.28       -1.31\n## 5       -1.02       1.25          -1.34       -1.31\ndist(iris_sample, method = \"euclidean\")\n##       1     2     3     4\n## 2 1.172                  \n## 3 0.843 0.522            \n## 4 1.100 0.433 0.283      \n## 5 0.259 1.382 0.988 1.246\ndist(iris_sample, method = \"manhattan\")\n##       1     2     3     4\n## 2 1.389                  \n## 3 1.228 0.757            \n## 4 1.578 0.648 0.463      \n## 5 0.350 1.497 1.337 1.687\ndist(iris_sample, method = \"maximum\")\n##       1     2     3     4\n## 2 1.147                  \n## 3 0.688 0.459            \n## 4 0.918 0.362 0.229      \n## 5 0.229 1.377 0.918 1.147"},{"path":"data.html","id":"distances-for-binary-data","chapter":"2 Data","heading":"2.4.2 Distances for Binary Data","text":"Binary data can encodes 0 1 (numeric) TRUE \nFALSE (logical).","code":"\nb <- rbind(\n  c(0,0,0,1,1,1,1,0,0,1),\n  c(0,0,1,1,1,0,0,1,0,0)\n  )\nb\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    0    0    0    1    1    1    1    0    0     1\n## [2,]    0    0    1    1    1    0    0    1    0     0\n\nb_logical <- apply(b, MARGIN = 2, as.logical)\nb_logical\n##       [,1]  [,2]  [,3] [,4] [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n## [1,] FALSE FALSE FALSE TRUE TRUE  TRUE  TRUE FALSE FALSE  TRUE\n## [2,] FALSE FALSE  TRUE TRUE TRUE FALSE FALSE  TRUE FALSE FALSE"},{"path":"data.html","id":"hamming-distance","chapter":"2 Data","heading":"2.4.2.1 Hamming Distance","text":"Hamming distance\nnumber mismatches two binary vectors. 0-1 data\nequivalent Manhattan distance also squared\nEuclidean distance.","code":"\ndist(b, method = \"manhattan\")\n##   1\n## 2 5\ndist(b, method = \"euclidean\")^2\n##   1\n## 2 5"},{"path":"data.html","id":"jaccard-index","chapter":"2 Data","heading":"2.4.2.2 Jaccard Index","text":"Jaccard index \nsimilarity measure focuses matching 1s. R converts \nsimilarity dissimilarity using \\(d_{J} = 1 - s_{J}\\).","code":"\ndist(b, method = \"binary\")\n##       1\n## 2 0.714"},{"path":"data.html","id":"distances-for-mixed-data","chapter":"2 Data","heading":"2.4.3 Distances for Mixed Data","text":"distance measures work numeric data. Often, \nmixture numbers nominal ordinal features like data:important nominal features stored factors \ncharacter (<chr>).","code":"\npeople <- tibble(\n  height = c(      160,    185,    170),\n  weight = c(       52,     90,     75),\n  sex    = c( \"female\", \"male\", \"male\")\n)\npeople\n## # A tibble: 3 × 3\n##   height weight sex   \n##    <dbl>  <dbl> <chr> \n## 1    160     52 female\n## 2    185     90 male  \n## 3    170     75 male\npeople <- people |> \n  mutate(across(where(is.character), factor))\npeople\n## # A tibble: 3 × 3\n##   height weight sex   \n##    <dbl>  <dbl> <fct> \n## 1    160     52 female\n## 2    185     90 male  \n## 3    170     75 male"},{"path":"data.html","id":"gowers-coefficient","chapter":"2 Data","heading":"2.4.3.1 Gower’s Coefficient","text":"Gower’s coefficient similarity works mixed data \ncalculating appropriate similarity feature \naggregating single measure. package proxy implements\nGower’s coefficient converted distance.Gower’s coefficient calculation implicitly scales data \ncalculates distances feature individually, need\nscale data first.","code":"\nlibrary(proxy)\n## \n## Attaching package: 'proxy'\n## The following object is masked from 'package:Matrix':\n## \n##     as.matrix\n## The following objects are masked from 'package:stats':\n## \n##     as.dist, dist\n## The following object is masked from 'package:base':\n## \n##     as.matrix\nd_Gower <- dist(people, method = \"Gower\")\nd_Gower\n##       1     2\n## 2 1.000      \n## 3 0.668 0.332"},{"path":"data.html","id":"using-euclidean-distance-with-mixed-data","chapter":"2 Data","heading":"2.4.3.2 Using Euclidean Distance with Mixed Data","text":"Sometimes methods (e.g., k-means) can use Euclidean distance. \ncase, nominal features can converted 0-1 dummy variables.\nscaling, Euclidean distance result usable distance\nmeasure.use package caret create dummy variables.Note feature sex now two columns. want height,\nweight sex influence distance measure, \nneed weight sex columns 1/2 scaling.distance using dummy variables consistent Gower’s distance.\nHowever, note Gower’s distance scaled 0 1 \nEuclidean distance .","code":"\nlibrary(caret)\n## Loading required package: lattice\n## \n## Attaching package: 'caret'\n## The following object is masked from 'package:sampling':\n## \n##     cluster\n## The following object is masked from 'package:purrr':\n## \n##     lift\ndata_dummy <- dummyVars(~., people) |> \n  predict(people)\ndata_dummy\n##   height weight sex.female sex.male\n## 1    160     52          1        0\n## 2    185     90          0        1\n## 3    170     75          0        1\nweight_matrix <- matrix(c(1, 1, 1/2, 1/2), ncol = 4, nrow = nrow(data_dummy), byrow = TRUE)\ndata_dummy_scaled <- scale(data_dummy) * weight_matrix\n\nd_dummy <- dist(data_dummy_scaled)\nd_dummy\n##      1    2\n## 2 3.06     \n## 3 1.89 1.43\nggplot(tibble(d_dummy, d_Gower), aes(x = d_dummy, y = d_Gower)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n## Don't know how to automatically pick scale for object of type <dist>.\n## Defaulting to continuous.\n## Don't know how to automatically pick scale for object of type <dist>.\n## Defaulting to continuous.\n## `geom_smooth()` using formula = 'y ~ x'"},{"path":"data.html","id":"additional-proximity-measures-available-in-package-proxy","chapter":"2 Data","heading":"2.4.4 Additional proximity Measures Available in Package proxy","text":"package proxy implements wide array distances.Note loading package proxy replaces dist function R.\ncan specify dist function use specifying package \ncall. example stats::dist() calls default function R\n(package stats part R) proxy::dist() calls \nversion package proxy.","code":"\nlibrary(proxy)\npr_DB$get_entry_names()\n##  [1] \"Jaccard\"         \"Kulczynski1\"     \"Kulczynski2\"    \n##  [4] \"Mountford\"       \"Fager\"           \"Russel\"         \n##  [7] \"simple matching\" \"Hamman\"          \"Faith\"          \n## [10] \"Tanimoto\"        \"Dice\"            \"Phi\"            \n## [13] \"Stiles\"          \"Michael\"         \"Mozley\"         \n## [16] \"Yule\"            \"Yule2\"           \"Ochiai\"         \n## [19] \"Simpson\"         \"Braun-Blanquet\"  \"cosine\"         \n## [22] \"angular\"         \"eJaccard\"        \"eDice\"          \n## [25] \"correlation\"     \"Chi-squared\"     \"Phi-squared\"    \n## [28] \"Tschuprow\"       \"Cramer\"          \"Pearson\"        \n## [31] \"Gower\"           \"Euclidean\"       \"Mahalanobis\"    \n## [34] \"Bhjattacharyya\"  \"Manhattan\"       \"supremum\"       \n## [37] \"Minkowski\"       \"Canberra\"        \"Wave\"           \n## [40] \"divergence\"      \"Kullback\"        \"Bray\"           \n## [43] \"Soergel\"         \"Levenshtein\"     \"Podani\"         \n## [46] \"Chord\"           \"Geodesic\"        \"Whittaker\"      \n## [49] \"Hellinger\"       \"fJaccard\""},{"path":"data.html","id":"data-exploration","chapter":"2 Data","heading":"2.5 Data Exploration*","text":"following code covers important part \ndata exploration. space reasons, chapter moved \nprinted textbook \nData Exploration Web Chapter.use iris dataset.","code":"\nlibrary(tidyverse)\ndata(iris)\niris <- as_tibble(iris)\niris\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # ℹ 140 more rows"},{"path":"data.html","id":"basic-statistics","chapter":"2 Data","heading":"2.5.1 Basic statistics","text":"Get summary statistics (using base R)Get mean standard deviation sepal lengthData missing values result statistics NA. Adding \nparameter na.rm = TRUE can used statistics functions \nignore missing values.Outliers typically smallest largest values feature.\nmake mean robust outliers, can trim 10% \nobservations end distribution.Sepal length outliers, trimmed mean almost\nidentical.calculate summary set features (e.g., numeric\nfeatures), tidyverse provides across((.numeric), fun).median absolute deviation (MAD) another measure dispersion.","code":"\nsummary(iris)\n##   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  \n##  Median :5.80   Median :3.00   Median :4.35   Median :1.3  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \niris |> \n  summarize(avg_Sepal.Length = mean(Sepal.Length), sd_Sepal.Length = sd(Sepal.Length))\n## # A tibble: 1 × 2\n##   avg_Sepal.Length sd_Sepal.Length\n##              <dbl>           <dbl>\n## 1             5.84           0.828\nmean(c(1, 2, NA, 3, 4, 5))\n## [1] NA\nmean(c(1, 2, NA, 3, 4, 5),  na.rm = TRUE)\n## [1] 3\niris |>\n  summarize(avg_Sepal.Length = mean(Sepal.Length),\n            trimmed_avg_Sepal.Length = mean(Sepal.Length, trim = .1))\n## # A tibble: 1 × 2\n##   avg_Sepal.Length trimmed_avg_Sepal.Length\n##              <dbl>                    <dbl>\n## 1             5.84                     5.81\niris |> summarize(across(where(is.numeric), mean))\n## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         5.84        3.06         3.76        1.20\niris |> summarize(across(where(is.numeric), sd))\n## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1        0.828       0.436         1.77       0.762\n\niris |> summarize(across(where(is.numeric), \n            list(min = min, median = median, max = max)))\n## # A tibble: 1 × 12\n##   Sepal.Length_min Sepal.Length_median Sepal.Length_max\n##              <dbl>               <dbl>            <dbl>\n## 1              4.3                 5.8              7.9\n## # ℹ 9 more variables: Sepal.Width_min <dbl>,\n## #   Sepal.Width_median <dbl>, Sepal.Width_max <dbl>,\n## #   Petal.Length_min <dbl>, Petal.Length_median <dbl>,\n## #   Petal.Length_max <dbl>, Petal.Width_min <dbl>,\n## #   Petal.Width_median <dbl>, Petal.Width_max <dbl>\niris |> summarize(across(where(is.numeric), mad))\n## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         1.04       0.445         1.85        1.04"},{"path":"data.html","id":"grouped-operations-and-calculations","chapter":"2 Data","heading":"2.5.2 Grouped Operations and Calculations","text":"can use nominal feature form groups calculate\ngroup-wise statistics continuous features. often use\ngroup-wise averages see differ groups.see species Virginica highest average , \nSepal.Width.statistical difference groups can tested using ANOVA\n(analysis \nvariance).summary shows significant difference \nSepal.Length groups. TukeyHDS evaluates differences\npairs groups. case, significantly different.\ndata contains two groups, t.test can used.","code":"\niris |> \n  group_by(Species) |> \n  summarize(across(Sepal.Length, mean))\n## # A tibble: 3 × 2\n##   Species    Sepal.Length\n##   <fct>             <dbl>\n## 1 setosa             5.01\n## 2 versicolor         5.94\n## 3 virginica          6.59\niris |> \n  group_by(Species) |> \n  summarize(across(where(is.numeric), mean))\n## # A tibble: 3 × 5\n##   Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <fct>             <dbl>       <dbl>        <dbl>       <dbl>\n## 1 setosa             5.01        3.43         1.46       0.246\n## 2 versicolor         5.94        2.77         4.26       1.33 \n## 3 virginica          6.59        2.97         5.55       2.03\nres.aov <- aov(Sepal.Length ~ Species, data = iris)\nsummary(res.aov)\n##              Df Sum Sq Mean Sq F value Pr(>F)    \n## Species       2   63.2   31.61     119 <2e-16 ***\n## Residuals   147   39.0    0.27                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(res.aov)\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = Sepal.Length ~ Species, data = iris)\n## \n## $Species\n##                       diff   lwr   upr p adj\n## versicolor-setosa    0.930 0.686 1.174     0\n## virginica-setosa     1.582 1.338 1.826     0\n## virginica-versicolor 0.652 0.408 0.896     0"},{"path":"data.html","id":"tabulate-data","chapter":"2 Data","heading":"2.5.3 Tabulate data","text":"can count number flowers species.base R, can also done using count(iris$Species).following examples, discretize data using cut.Cross tabulation used find two discrete features \nrelated.table contains number rows contain combination \nvalues (e.g., number flowers short Sepal.Length \nspecies Setosa 47). cells large counts\nothers low counts, might \nrelationship. iris data, see species Setosa mostly \nshort Sepal.Length, Versicolor Virginica longer sepals.Creating cross table tidyverse little involved uses\npivot operations grouping.can use statistical test determine significant\nrelationship two features. Pearson’s chi-squared\ntest independence\nperformed null hypothesis joint distribution \ncell counts 2-dimensional contingency table product \nrow column marginals. null hypothesis h0 independence \nrows columns.small p-value indicates null hypothesis independence\nneeds rejected. small counts (cells counts <5),\nFisher’s exact\ntest better.","code":"\niris |> \n  group_by(Species) |> \n  summarize(n())\n## # A tibble: 3 × 2\n##   Species    `n()`\n##   <fct>      <int>\n## 1 setosa        50\n## 2 versicolor    50\n## 3 virginica     50\niris_ord <- iris |> mutate(across(where(is.numeric),  \n  function(x) cut(x, 3, labels = c(\"short\", \"medium\", \"long\"), ordered = TRUE)))\n\niris_ord\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##    <ord>        <ord>       <ord>        <ord>       <fct>  \n##  1 short        medium      short        short       setosa \n##  2 short        medium      short        short       setosa \n##  3 short        medium      short        short       setosa \n##  4 short        medium      short        short       setosa \n##  5 short        medium      short        short       setosa \n##  6 short        long        short        short       setosa \n##  7 short        medium      short        short       setosa \n##  8 short        medium      short        short       setosa \n##  9 short        medium      short        short       setosa \n## 10 short        medium      short        short       setosa \n## # ℹ 140 more rows\nsummary(iris_ord)\n##  Sepal.Length Sepal.Width Petal.Length Petal.Width       Species  \n##  short :59    short :47   short :50    short :50   setosa    :50  \n##  medium:71    medium:88   medium:54    medium:54   versicolor:50  \n##  long  :20    long  :15   long  :46    long  :46   virginica :50\ntbl <- iris_ord |> \n  select(Sepal.Length, Species) |> \n  table()\ntbl\n##             Species\n## Sepal.Length setosa versicolor virginica\n##       short      47         11         1\n##       medium      3         36        32\n##       long        0          3        17\niris_ord |>\n  select(Species, Sepal.Length) |>\n### Relationship Between Nominal and Ordinal Features\n  pivot_longer(cols = Sepal.Length) |>\n  group_by(Species, value) |> \n  count() |> \n  ungroup() |>\n  pivot_wider(names_from = Species, values_from = n)\n## # A tibble: 3 × 4\n##   value  setosa versicolor virginica\n##   <ord>   <int>      <int>     <int>\n## 1 short      47         11         1\n## 2 medium      3         36        32\n## 3 long       NA          3        17\ntbl |> \n  chisq.test()\n## \n##  Pearson's Chi-squared test\n## \n## data:  tbl\n## X-squared = 112, df = 4, p-value <2e-16\nfisher.test(tbl)\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tbl\n## p-value <2e-16\n## alternative hypothesis: two.sided"},{"path":"data.html","id":"percentiles-quantiles","chapter":"2 Data","heading":"2.5.4 Percentiles (Quantiles)","text":"Quantiles cutting points\ndividing range probability distribution continuous\nintervals equal probability. example, median \nempirical 50% quantile dividing observations 50% \nobservations smaller median 50% \nlarger median.default quartiles calculated. 25% typically called Q1, 50% \ncalled Q2 median 75% called Q3.interquartile range measure variability robust\noutliers. defined length Q3 - Q2 covers 50%\ndata middle.","code":"\niris |> \n  pull(Petal.Length) |> \n  quantile()\n##   0%  25%  50%  75% 100% \n## 1.00 1.60 4.35 5.10 6.90\niris |> \n  summarize(IQR = \n  quantile(Petal.Length, probs = 0.75) - quantile(Petal.Length, probs = 0.25))\n## # A tibble: 1 × 1\n##     IQR\n##   <dbl>\n## 1   3.5"},{"path":"data.html","id":"correlation","chapter":"2 Data","heading":"2.5.5 Correlation","text":"","code":""},{"path":"data.html","id":"pearson-correlation","chapter":"2 Data","heading":"2.5.5.1 Pearson Correlation","text":"Correlation can used ratio/interval scaled features. typically\nthink Pearson correlation\ncoefficient\nfeatures (columns).cor calculates correlation matrix pairwise correlations \nfeatures. Correlation matrices symmetric, different \ndistances, whole matrix stored.correlation Petal.Length Petal.Width can visualized\nusing scatter plot.geom_smooth adds regression line fitting linear model (lm).\npoints close line indicating strong linear dependence\n(.e., high correlation).can calculate individual correlations specifying two vectors.Note: lets use columns using just names \n(iris, cor(Petal.Length, Petal.Width)) \ncor(iris$Petal.Length, iris$Petal.Width).Finally, can test correlation significantly different \nzero.small p-value (less 0.05) indicates observed correlation\nsignificantly different zero. can also seen fact\n95% confidence interval span zero.Sepal.Length Sepal.Width show little correlation:","code":"\ncc <- iris |> \n  select(-Species) |> \n  cor()\ncc\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length        1.000      -0.118        0.872       0.818\n## Sepal.Width        -0.118       1.000       -0.428      -0.366\n## Petal.Length        0.872      -0.428        1.000       0.963\n## Petal.Width         0.818      -0.366        0.963       1.000\nggplot(iris, aes(Petal.Length, Petal.Width)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n## `geom_smooth()` using formula = 'y ~ x'\nwith(iris, cor(Petal.Length, Petal.Width))\n## [1] 0.963\nwith(iris, cor.test(Petal.Length, Petal.Width))\n## \n##  Pearson's product-moment correlation\n## \n## data:  Petal.Length and Petal.Width\n## t = 43, df = 148, p-value <2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.949 0.973\n## sample estimates:\n##   cor \n## 0.963\nggplot(iris, aes(Sepal.Length, Sepal.Width)) + \n  geom_point() +   \n  geom_smooth(method = \"lm\") \n## `geom_smooth()` using formula = 'y ~ x'\n\nwith(iris, cor(Sepal.Length, Sepal.Width)) \n## [1] -0.118\nwith(iris, cor.test(Sepal.Length, Sepal.Width))\n## \n##  Pearson's product-moment correlation\n## \n## data:  Sepal.Length and Sepal.Width\n## t = -1, df = 148, p-value = 0.2\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.2727  0.0435\n## sample estimates:\n##    cor \n## -0.118"},{"path":"data.html","id":"rank-correlation","chapter":"2 Data","heading":"2.5.5.2 Rank Correlation","text":"Rank correlation used ordinal features correlation \nlinear. show , first convert continuous features \nIris dataset ordered factors (ordinal) three levels using\nfunction cut.Two measures rank correlation Kendall’s Tau Spearman’s Rho.Kendall’s Tau Rank Correlation\nCoefficient\nmeasures agreement two rankings (.e., ordinal features).Note: use xtfrm transform ordered factors \nranks, .e., numbers representing order.Spearman’s\nRho\nequal Pearson correlation rank values two\nfeatures.Spearman’s Rho much faster compute large datasets \nKendall’s Tau.Comparing rank correlation results Pearson correlation \noriginal data shows similar. indicates \ndiscretizing data result loss much information.","code":"\niris_ord <- iris |> \n  mutate(across(where(is.numeric), \n  function(x) cut(x, 3, labels = c(\"short\", \"medium\", \"long\"), ordered = TRUE)))\n\niris_ord\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##    <ord>        <ord>       <ord>        <ord>       <fct>  \n##  1 short        medium      short        short       setosa \n##  2 short        medium      short        short       setosa \n##  3 short        medium      short        short       setosa \n##  4 short        medium      short        short       setosa \n##  5 short        medium      short        short       setosa \n##  6 short        long        short        short       setosa \n##  7 short        medium      short        short       setosa \n##  8 short        medium      short        short       setosa \n##  9 short        medium      short        short       setosa \n## 10 short        medium      short        short       setosa \n## # ℹ 140 more rows\nsummary(iris_ord)\n##  Sepal.Length Sepal.Width Petal.Length Petal.Width       Species  \n##  short :59    short :47   short :50    short :50   setosa    :50  \n##  medium:71    medium:88   medium:54    medium:54   versicolor:50  \n##  long  :20    long  :15   long  :46    long  :46   virginica :50\niris_ord |> \n  pull(Sepal.Length)\n##   [1] short  short  short  short  short  short  short  short  short \n##  [10] short  short  short  short  short  medium medium short  short \n##  [19] medium short  short  short  short  short  short  short  short \n##  [28] short  short  short  short  short  short  short  short  short \n##  [37] short  short  short  short  short  short  short  short  short \n##  [46] short  short  short  short  short  long   medium long   short \n##  [55] medium medium medium short  medium short  short  medium medium\n##  [64] medium medium medium medium medium medium medium medium medium\n##  [73] medium medium medium medium long   medium medium medium short \n##  [82] short  medium medium short  medium medium medium medium short \n##  [91] short  medium medium short  medium medium medium medium short \n## [100] medium medium medium long   medium medium long   short  long  \n## [109] medium long   medium medium long   medium medium medium medium\n## [118] long   long   medium long   medium long   medium medium long  \n## [127] medium medium medium long   long   long   medium medium medium\n## [136] long   medium medium medium long   medium long   medium long  \n## [145] medium medium medium medium medium medium\n## Levels: short < medium < long\niris_ord |> \n  select(-Species) |> \n  sapply(xtfrm) |> \n  cor(method = \"kendall\")\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length        1.000      -0.144        0.742       0.730\n## Sepal.Width        -0.144       1.000       -0.330      -0.315\n## Petal.Length        0.742      -0.330        1.000       0.920\n## Petal.Width         0.730      -0.315        0.920       1.000\niris_ord |> \n  select(-Species) |> \n  sapply(xtfrm) |> \n  cor(method = \"spearman\")\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length        1.000      -0.157        0.794       0.784\n## Sepal.Width        -0.157       1.000       -0.366      -0.352\n## Petal.Length        0.794      -0.366        1.000       0.940\n## Petal.Width         0.784      -0.352        0.940       1.000\niris |> \n  select(-Species) |> \n  cor()\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length        1.000      -0.118        0.872       0.818\n## Sepal.Width        -0.118       1.000       -0.428      -0.366\n## Petal.Length        0.872      -0.428        1.000       0.963\n## Petal.Width         0.818      -0.366        0.963       1.000"},{"path":"data.html","id":"density","chapter":"2 Data","heading":"2.5.6 Density","text":"Density estimation\nestimate probability density function\n(distribution) continuous variable observed data.Just plotting data using points helpful single\nfeature.","code":"\nggplot(iris, aes(x = Petal.Length, y = 0)) + geom_point()"},{"path":"data.html","id":"histograms","chapter":"2 Data","heading":"2.5.6.1 Histograms","text":"histograms shows \ndistribution counting many values fall within bin \nvisualizing counts bar chart. use geom_rug place marks\noriginal data points bottom histogram.Two-dimensional distributions can visualized using 2-d binning \nhexagonal bins.","code":"\nggplot(iris, aes(x = Petal.Length)) +\n  geom_histogram() +\n  geom_rug(alpha = 1/2)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_bin2d(bins = 10) +\n  geom_jitter(color = \"red\")\n\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_hex(bins = 10) +\n  geom_jitter(color = \"red\")"},{"path":"data.html","id":"kernel-density-estimate-kde","chapter":"2 Data","heading":"2.5.6.2 Kernel Density Estimate (KDE)","text":"Kernel density\nestimation \nused estimate probability density function (distribution) \nfeature. works replacing value kernel function (often\nGaussian) adding . result estimated\nprobability density function looks like smoothed version \nhistogram. bandwidth (bw) kernel controls amount \nsmoothing.Kernel density estimates can also done two dimensions.","code":"\nggplot(iris, aes(Petal.Length)) +\n  geom_density(bw = .2) +\n  geom_rug(alpha = 1/2)\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_density_2d_filled() +\n  geom_jitter()"},{"path":"data.html","id":"visualization","chapter":"2 Data","heading":"2.6 Visualization","text":"go basic visualizations used working data.\nspace reasons, chapter moved \nprinted textbook \nData Exploration Web Chapter.","code":""},{"path":"data.html","id":"histogram","chapter":"2 Data","heading":"2.6.1 Histogram","text":"Histograms show distribution single continuous feature.data contains groups, group information can easily\nadded aesthetic histogram. also add alpha position\nmake ","code":"\nggplot(iris, aes(Petal.Width)) + geom_histogram(bins = 20)\nggplot(iris, aes(Petal.Width)) + \n         geom_histogram(bins = 20, aes(fill = Species))"},{"path":"data.html","id":"boxplot","chapter":"2 Data","heading":"2.6.2 Boxplot","text":"Boxplots used compare distribution feature \ndifferent groups. horizontal line middle boxes \ngroup-wise medians, boxes span interquartile range. whiskers\n(vertical lines) span typically 1.4 times interquartile range.\nPoints fall outside range typically outliers shown \ndots.group-wise medians can also calculated directly.compare distribution four features using ggplot boxplot,\nfirst transform data long format (.e., feature\nvalues combined single column).visualization useful features roughly \nrange. data can scaled first compare distributions.","code":"\nggplot(iris, aes(Species, Sepal.Length)) + \n  geom_boxplot()\niris |> group_by(Species) |> \n  summarize(across(where(is.numeric), median))\n## # A tibble: 3 × 5\n##   Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <fct>             <dbl>       <dbl>        <dbl>       <dbl>\n## 1 setosa              5           3.4         1.5          0.2\n## 2 versicolor          5.9         2.8         4.35         1.3\n## 3 virginica           6.5         3           5.55         2\nlibrary(tidyr)\niris_long <- iris |> \n  mutate(id = row_number()) |> \n  pivot_longer(1:4)\n\nggplot(iris_long, aes(name, value)) + \n  geom_boxplot() +\n  labs(y = \"Original value\")\nlibrary(tidyr)\niris_long_scaled <- iris |> \n  scale_numeric() |> \n  mutate(id = row_number()) |> pivot_longer(1:4)\n\nggplot(iris_long_scaled, aes(name, value)) + \n  geom_boxplot() +\n  labs(y = \"Scaled value\")"},{"path":"data.html","id":"scatter-plot","chapter":"2 Data","heading":"2.6.3 Scatter plot","text":"Scatter plots show relationship two continuous features.can add regression using geom_smooth \nlinear model method show \nlinear relationship two variables. confidence interval\nregression also shown.\ncan suppressed using se = FALSE.can also perform group-wise linear regression adding color\naesthetic also geom_smooth.can achieved using color aesthetic qqplot call,\napplies geoms.","code":"\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + \n  geom_point(aes(color = Species))\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + \n  geom_point(aes(color = Species)) +  \n  geom_smooth(method = \"lm\")\n## `geom_smooth()` using formula = 'y ~ x'\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + \n  geom_point(aes(color = Species)) +  \n  geom_smooth(method = \"lm\", aes(color = Species))\n## `geom_smooth()` using formula = 'y ~ x'"},{"path":"data.html","id":"scatter-plot-matrix","chapter":"2 Data","heading":"2.6.4 Scatter Plot Matrix","text":"scatter plot matrix show relationship pairs features\narranging panels matrix. First, lets look regular\nR-base plot.package GGally provides way sophisticated visualization.Additional plots\n(histograms, density estimates box plots) correlation\ncoefficients shown different panels. See Data Quality\nsection description interpret different panels.","code":"\npairs(iris, col = iris$Species)\nlibrary(\"GGally\")\nggpairs(iris,  aes(color = Species), progress = FALSE)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"data.html","id":"matrix-visualization","chapter":"2 Data","heading":"2.6.5 Matrix Visualization","text":"Matrix visualization shows values matrix using color scale.need long format tidyverse.Smaller values darker. Package seriation provides simpler\nplotting function.can scale features z-scores make better comparable.reveals red blue blocks. row flower flowers\nIris dataset sorted species. blue blocks top\n50 flowers show flowers smaller average \nSepal.Width red blocks show bottom 50 flowers \nlarger features.Often, reordering data matrices help visualization. reordering\ntechnique called seriation. Ir reorders rows columns place\nsimilar points closer together.see rows (flowers) organized blue red\nfeatures reordered move Sepal.Width way \nright different features.","code":"\niris_matrix <- iris |> select(-Species) |> as.matrix()\niris_long <- as_tibble(iris_matrix) |> \n  mutate(id = row_number()) |> \n  pivot_longer(1:4)\n\nhead(iris_long)\n## # A tibble: 6 × 3\n##      id name         value\n##   <int> <chr>        <dbl>\n## 1     1 Sepal.Length   5.1\n## 2     1 Sepal.Width    3.5\n## 3     1 Petal.Length   1.4\n## 4     1 Petal.Width    0.2\n## 5     2 Sepal.Length   4.9\n## 6     2 Sepal.Width    3\n\nggplot(iris_long, aes(x = name, y = id)) + \n  geom_tile(aes(fill = value))\nlibrary(seriation)\n## Registered S3 methods overwritten by 'registry':\n##   method               from \n##   print.registry_field proxy\n##   print.registry_entry proxy\n## \n## Attaching package: 'seriation'\n## The following object is masked from 'package:lattice':\n## \n##     panel.lines\nggpimage(iris_matrix, prop = FALSE)\niris_scaled <- scale(iris_matrix)\nggpimage(iris_scaled, prop = FALSE)\nggpimage(iris_scaled, order = seriate(iris_scaled), prop = FALSE)"},{"path":"data.html","id":"correlation-matrix","chapter":"2 Data","heading":"2.6.6 Correlation Matrix","text":"correlation matrix contains correlation features.Package ggcorrplot provides visualization correlation matrices.Package seriation provides reordered version plot using \nheatmap.Correlations can also calculates objects transposing \ndata matrix.Object--object correlations can used measure similarity.\ndark red blocks indicate different species.","code":"\ncm1 <- iris |> select(-Species) |> as.matrix() |> cor()\ncm1\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length        1.000      -0.118        0.872       0.818\n## Sepal.Width        -0.118       1.000       -0.428      -0.366\n## Petal.Length        0.872      -0.428        1.000       0.963\n## Petal.Width         0.818      -0.366        0.963       1.000\nlibrary(ggcorrplot)\nggcorrplot(cm1)\ngghmap(cm1, prop = TRUE)\ncm2 <- iris |> select(-Species) |> as.matrix() |> t() |> cor()\n\nggcorrplot(cm2)"},{"path":"data.html","id":"parallel-coordinates-plot","chapter":"2 Data","heading":"2.6.7 Parallel Coordinates Plot","text":"Parallel coordinate plots can visualize several features single\nplot. Lines connect values object (flower).plot can improved reordering variables place correlated\nfeatures next .","code":"\nlibrary(GGally)\nggparcoord(iris, columns = 1:4, groupColumn = 5)\no <- seriate(as.dist(1-cor(iris[,1:4])), method = \"BBURCG\")\nget_order(o)\n## Petal.Length  Petal.Width Sepal.Length  Sepal.Width \n##            3            4            1            2\nggparcoord(iris, columns = as.integer(get_order(o)), groupColumn = 5)"},{"path":"data.html","id":"more-visualizations","chapter":"2 Data","heading":"2.6.8 More Visualizations","text":"well organized collection visualizations code can found \nR Graph Gallery.","code":""},{"path":"data.html","id":"exercises","chapter":"2 Data","heading":"2.7 Exercises*","text":"R package palmerpenguins contains measurements penguin different\nspecies Palmer Archipelago, Antarctica. Install package.\nprovides CSV file can read following way:Create RStudio new R Markdown document.\nApply code sections chapter data set answer \nfollowing questions.scale measurement column?missing values data? much data missing?Compute discuss basic statistics.Group penguins species, island sex. can find ?Can identify correlations?Apply histograms, boxplots, scatterplots correlation matrix visualization.\nvisualizations show.Make sure markdown document contains now well formatted report.\nUse Knit button create HTML document.","code":"\nlibrary(\"palmerpenguins\")\npenguins <- read_csv(path_to_file(\"penguins.csv\"))\n## Rows: 344 Columns: 8\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): species, island, sex\n## dbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm\n##   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n## 1 Adelie  Torgersen           39.1          18.7               181\n## 2 Adelie  Torgersen           39.5          17.4               186\n## 3 Adelie  Torgersen           40.3          18                 195\n## 4 Adelie  Torgersen           NA            NA                  NA\n## 5 Adelie  Torgersen           36.7          19.3               193\n## 6 Adelie  Torgersen           39.3          20.6               190\n## # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"classification-basic-concepts-and-techniques.html","id":"classification-basic-concepts-and-techniques","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3 Classification: Basic Concepts and Techniques","text":"Install packages used chapter:packages used chapter : caret (M. Kuhn 2023), FSelector (Romanski, Kotthoff, Schratz 2023), lattice (Sarkar 2023), mlbench (Leisch Dimitriadou 2024), palmerpenguins (Horst, Hill, Gorman 2022), party (Hothorn et al. 2024), pROC (Robin et al. 2023), rpart (Therneau Atkinson 2023), rpart.plot (Milborrow 2024), sampling (Tillé Matei 2023), tidyverse (Wickham 2023c)","code":"\npkgs <- c(\"caret\", \"FSelector\", \"lattice\", \"mlbench\", \n          \"palmerpenguins\", \"party\", \"pROC\", \"rpart\", \"rpart.plot\", \n          \"sampling\", \"tidyverse\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"classification-basic-concepts-and-techniques.html","id":"basic-concepts","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.1 Basic Concepts","text":"Classification machine learning task goal learn predictive\nfunction form\\[y = f(\\mathbf{x}),\\]\\(\\mathbf{x}\\) called attribute set \\(y\\) class label. attribute set\nconsists feature describe object. features can measured using scale\n(.e., nominal, interval, …). class label nominal attribute. binary\nattribute, problem called binary classification problem.Classification learns classification model training data features \ncorrect class label available. called supervised learning problem.related supervised learning problem regression,\n\\(y\\) number instead label.\nLinear regression popular supervised learning model\ntaught almost introductory statistics course.\nCode examples regression available extra Chapter\nRegression*.chapter introduce decision trees, model evaluation comparison, feature selection,\nexplore methods handle class imbalance problem.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 3. Classification: Basic Concepts \nTechniques","code":""},{"path":"classification-basic-concepts-and-techniques.html","id":"general-framework-for-classification","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.2 General Framework for Classification","text":"Supervised learning two steps:Induction: Training model training data known class labels.Deduction: Predicting class labels new data.often test model predicting class data know \ncorrect label. test model test data known labels\ncan calculate error comparing prediction \nknown correct label.\ntempting measure well model learned \ntraining data, testing training data. error \ntraining data called resubstitution error. \nhelp us find model generalizes well new data \npart training.typically want \nevaluate well model generalizes new data, important \ntest data training data overlap. call \nerror proper test data generalization error.chapter builds needed concepts.\ncomplete example perform model selection estimate\ngeneralization error section Hyperparameter Tuning.","code":""},{"path":"classification-basic-concepts-and-techniques.html","id":"the-zoo-dataset","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.2.1 The Zoo Dataset","text":"demonstrate classification, use Zoo dataset included R package\nmlbench (may install ). Zoo dataset containing 17\n(mostly logical) variables 101 animals data frame \n17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator,\ntoothed, backbone, breathes, venomous, fins, legs, tail, domestic,\ncatsize, type).\nfirst 16 columns represent feature vector \\(\\mathbf{x}\\) last column\ncalled type class label \\(y\\).\nconvert data frame tidyverse tibble\n(optional).Note: data.frames R can row names. Zoo data set uses \nanimal name row names. tibbles tidyverse support\nrow names. keep animal name can add column animal\nname.remove animal column learning model! \nfollowing use data.frame.translate TRUE/FALSE values factors (nominal). \noften needed building models. Always check summary() make sure\ndata ready model learning.","code":"\ndata(Zoo, package=\"mlbench\")\nhead(Zoo)\n##           hair feathers  eggs  milk airborne aquatic predator toothed\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE     TRUE    TRUE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE    FALSE    TRUE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE     TRUE    TRUE\n## bear      TRUE    FALSE FALSE  TRUE    FALSE   FALSE     TRUE    TRUE\n## boar      TRUE    FALSE FALSE  TRUE    FALSE   FALSE     TRUE    TRUE\n## buffalo   TRUE    FALSE FALSE  TRUE    FALSE   FALSE    FALSE    TRUE\n##          backbone breathes venomous  fins legs  tail domestic catsize\n## aardvark     TRUE     TRUE    FALSE FALSE    4 FALSE    FALSE    TRUE\n## antelope     TRUE     TRUE    FALSE FALSE    4  TRUE    FALSE    TRUE\n## bass         TRUE    FALSE    FALSE  TRUE    0  TRUE    FALSE   FALSE\n## bear         TRUE     TRUE    FALSE FALSE    4 FALSE    FALSE    TRUE\n## boar         TRUE     TRUE    FALSE FALSE    4  TRUE    FALSE    TRUE\n## buffalo      TRUE     TRUE    FALSE FALSE    4  TRUE    FALSE    TRUE\n##            type\n## aardvark mammal\n## antelope mammal\n## bass       fish\n## bear     mammal\n## boar     mammal\n## buffalo  mammal\nlibrary(tidyverse)\nas_tibble(Zoo, rownames = \"animal\")\n## # A tibble: 101 × 18\n##    animal hair  feathers eggs  milk  airborne aquatic predator toothed\n##    <chr>  <lgl> <lgl>    <lgl> <lgl> <lgl>    <lgl>   <lgl>    <lgl>  \n##  1 aardv… TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE     TRUE   \n##  2 antel… TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE    TRUE   \n##  3 bass   FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE     TRUE   \n##  4 bear   TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE     TRUE   \n##  5 boar   TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE     TRUE   \n##  6 buffa… TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE    TRUE   \n##  7 calf   TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE    TRUE   \n##  8 carp   FALSE FALSE    TRUE  FALSE FALSE    TRUE    FALSE    TRUE   \n##  9 catfi… FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE     TRUE   \n## 10 cavy   TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE    TRUE   \n## # ℹ 91 more rows\n## # ℹ 9 more variables: backbone <lgl>, breathes <lgl>, venomous <lgl>,\n## #   fins <lgl>, legs <int>, tail <lgl>, domestic <lgl>,\n## #   catsize <lgl>, type <fct>\nZoo <- Zoo |>\n  mutate(across(where(is.logical), \n                function (x) factor(x, levels = c(TRUE, FALSE)))) |>\n  mutate(across(where(is.character), factor))\n\nsummary(Zoo)\n##     hair     feathers     eggs       milk     airborne   aquatic  \n##  TRUE :43   TRUE :20   TRUE :59   TRUE :41   TRUE :24   TRUE :36  \n##  FALSE:58   FALSE:81   FALSE:42   FALSE:60   FALSE:77   FALSE:65  \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##   predator   toothed    backbone   breathes   venomous     fins   \n##  TRUE :56   TRUE :61   TRUE :83   TRUE :80   TRUE : 8   TRUE :17  \n##  FALSE:45   FALSE:40   FALSE:18   FALSE:21   FALSE:93   FALSE:84  \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##       legs         tail     domestic   catsize              type   \n##  Min.   :0.00   TRUE :75   TRUE :13   TRUE :44   mammal       :41  \n##  1st Qu.:2.00   FALSE:26   FALSE:88   FALSE:57   bird         :20  \n##  Median :4.00                                    reptile      : 5  \n##  Mean   :2.84                                    fish         :13  \n##  3rd Qu.:4.00                                    amphibian    : 4  \n##  Max.   :8.00                                    insect       : 8  \n##                                                  mollusc.et.al:10"},{"path":"classification-basic-concepts-and-techniques.html","id":"decision-tree-classifiers","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3 Decision Tree Classifiers","text":"use recursive partitioning implementation follows largely\nCART uses Gini index make\nsplitting decisions early stopping (also called pre-pruning).","code":"\nlibrary(rpart)"},{"path":"classification-basic-concepts-and-techniques.html","id":"create-tree","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.1 Create Tree","text":"create first tree default settings (see ? rpart.control).Notes:|> supplies data rpart. Since data \nfirst argument rpart, syntax data = _ used specify\ndata Zoo goes. call equivalent \ntree_default <- rpart(type ~ ., data = Zoo).|> supplies data rpart. Since data \nfirst argument rpart, syntax data = _ used specify\ndata Zoo goes. call equivalent \ntree_default <- rpart(type ~ ., data = Zoo).formula models thetype variable features represented ..formula models thetype variable features represented ..class variable needs factor recognized nominal\nrpart create regression tree instead decision tree.\nUse .factor() column class label first, necessary.class variable needs factor recognized nominal\nrpart create regression tree instead decision tree.\nUse .factor() column class label first, necessary.can plot resulting decision tree.Note: extra=2 prints leaf node number correctly\nclassified objects data total number objects \ntraining data falling node (correct/total).","code":"\ntree_default <- Zoo |> \n  rpart(type ~ ., data = _)\ntree_default\n## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##  1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##    2) milk=TRUE 41  0 mammal (1 0 0 0 0 0 0) *\n##    3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##      6) feathers=TRUE 20  0 bird (0 1 0 0 0 0 0) *\n##      7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##       14) fins=TRUE 13  0 fish (0 0 0 1 0 0 0) *\n##       15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##         30) backbone=TRUE 9  4 reptile (0 0 0.56 0 0.44 0 0) *\n##         31) backbone=FALSE 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56) *\nlibrary(rpart.plot)\nrpart.plot(tree_default, extra = 2)"},{"path":"classification-basic-concepts-and-techniques.html","id":"make-predictions-for-new-data","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.2 Make Predictions for New Data","text":"make animal: lion feathered wings.data types need match original data \nchange columns factors like training set.Next, make prediction using default tree","code":"\nmy_animal <- tibble(hair = TRUE, feathers = TRUE, eggs = FALSE,\n  milk = TRUE, airborne = TRUE, aquatic = FALSE, predator = TRUE,\n  toothed = TRUE, backbone = TRUE, breathes = TRUE, venomous = FALSE,\n  fins = FALSE, legs = 4, tail = TRUE, domestic = FALSE,\n  catsize = FALSE, type = NA)\nmy_animal <- my_animal |> \n  mutate(across(where(is.logical), \n                function(x) factor(x, levels = c(TRUE, FALSE))))\nmy_animal\n## # A tibble: 1 × 17\n##   hair  feathers eggs  milk  airborne aquatic predator toothed\n##   <fct> <fct>    <fct> <fct> <fct>    <fct>   <fct>    <fct>  \n## 1 TRUE  TRUE     FALSE TRUE  TRUE     FALSE   TRUE     TRUE   \n## # ℹ 9 more variables: backbone <fct>, breathes <fct>, venomous <fct>,\n## #   fins <fct>, legs <dbl>, tail <fct>, domestic <fct>,\n## #   catsize <fct>, type <fct>\npredict(tree_default , my_animal, type = \"class\")\n##      1 \n## mammal \n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al"},{"path":"classification-basic-concepts-and-techniques.html","id":"manual-calculation-of-the-resubstitution-error","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.3 Manual Calculation of the Resubstitution Error","text":"calculate error model training data manually first,\nsee calculated.can easily tabulate true predicted labels create \nconfusion matrix.counts diagonal correct predictions. -diagonal\ncounts represent errors (.e., confusions).can summarize confusion matrix using accuracy measure.accuracy calculation simple function.","code":"\npredict(tree_default, Zoo) |> head ()\n##          mammal bird reptile fish amphibian insect mollusc.et.al\n## aardvark      1    0       0    0         0      0             0\n## antelope      1    0       0    0         0      0             0\n## bass          0    0       0    1         0      0             0\n## bear          1    0       0    0         0      0             0\n## boar          1    0       0    0         0      0             0\n## buffalo       1    0       0    0         0      0             0\n\npred <- predict(tree_default, Zoo, type=\"class\")\nhead(pred)\n## aardvark antelope     bass     bear     boar  buffalo \n##   mammal   mammal     fish   mammal   mammal   mammal \n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al\nconfusion_table <- with(Zoo, table(type, pred))\nconfusion_table\n##                pred\n## type            mammal bird reptile fish amphibian insect\n##   mammal            41    0       0    0         0      0\n##   bird               0   20       0    0         0      0\n##   reptile            0    0       5    0         0      0\n##   fish               0    0       0   13         0      0\n##   amphibian          0    0       4    0         0      0\n##   insect             0    0       0    0         0      0\n##   mollusc.et.al      0    0       0    0         0      0\n##                pred\n## type            mollusc.et.al\n##   mammal                    0\n##   bird                      0\n##   reptile                   0\n##   fish                      0\n##   amphibian                 0\n##   insect                    8\n##   mollusc.et.al            10\ncorrect <- confusion_table |> diag() |> sum()\ncorrect\n## [1] 89\nerror <- confusion_table |> sum() - correct\nerror\n## [1] 12\n\naccuracy <- correct / (correct + error)\naccuracy\n## [1] 0.8812\naccuracy <- function(truth, prediction) {\n    tbl <- table(truth, prediction)\n    sum(diag(tbl))/sum(tbl)\n}\n\naccuracy(Zoo |> pull(type), pred)\n## [1] 0.8812"},{"path":"classification-basic-concepts-and-techniques.html","id":"confusion-matrix-using-caret","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.4 Confusion Matrix using Caret","text":"package caret makes preparing\ntraining sets, building classification (regression) models \nevaluation easier. great cheat sheet can found\n.caret package provides convenient way create \nanalyze confusion table including many useful statistics.Note: Calculating accuracy training data good idea.\nKeep reading!","code":"\nlibrary(caret)\nconfusionMatrix(data = pred, \n                reference = Zoo |> pull(type))\n## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian insect\n##   mammal            41    0       0    0         0      0\n##   bird               0   20       0    0         0      0\n##   reptile            0    0       5    0         4      0\n##   fish               0    0       0   13         0      0\n##   amphibian          0    0       0    0         0      0\n##   insect             0    0       0    0         0      0\n##   mollusc.et.al      0    0       0    0         0      8\n##                Reference\n## Prediction      mollusc.et.al\n##   mammal                    0\n##   bird                      0\n##   reptile                   0\n##   fish                      0\n##   amphibian                 0\n##   insect                    0\n##   mollusc.et.al            10\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.881         \n##                  95% CI : (0.802, 0.937)\n##     No Information Rate : 0.406         \n##     P-Value [Acc > NIR] : <2e-16        \n##                                         \n##                   Kappa : 0.843         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird Class: reptile\n## Sensitivity                  1.000       1.000         1.0000\n## Specificity                  1.000       1.000         0.9583\n## Pos Pred Value               1.000       1.000         0.5556\n## Neg Pred Value               1.000       1.000         1.0000\n## Prevalence                   0.406       0.198         0.0495\n## Detection Rate               0.406       0.198         0.0495\n## Detection Prevalence         0.406       0.198         0.0891\n## Balanced Accuracy            1.000       1.000         0.9792\n##                      Class: fish Class: amphibian Class: insect\n## Sensitivity                1.000           0.0000        0.0000\n## Specificity                1.000           1.0000        1.0000\n## Pos Pred Value             1.000              NaN           NaN\n## Neg Pred Value             1.000           0.9604        0.9208\n## Prevalence                 0.129           0.0396        0.0792\n## Detection Rate             0.129           0.0000        0.0000\n## Detection Prevalence       0.129           0.0000        0.0000\n## Balanced Accuracy          1.000           0.5000        0.5000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         0.912\n## Pos Pred Value                      0.556\n## Neg Pred Value                      1.000\n## Prevalence                          0.099\n## Detection Rate                      0.099\n## Detection Prevalence                0.178\n## Balanced Accuracy                   0.956"},{"path":"classification-basic-concepts-and-techniques.html","id":"model-overfitting","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.4 Model Overfitting","text":"tempted create largest possible tree\nget accurate model. can achieved \nchanging algorithms hyperparameter (parameters \nchange algorithm works). \nset complexity parameter cp 0 (split\neven improve tree) set minimum number \nobservations node needed split smallest value 2 (see:\n?rpart.control). Note: good idea!\nsee later, full trees overfit training data!Error training set full treeWe see error smaller pruned tree. ,\nhowever, mean model better. actually overfitting\ntraining data (just memorizes ) worse generalization\nperformance new data. effect\ncalled overfitting training data needs avoided.","code":"\ntree_full <- Zoo |> \n  rpart(type ~ . , data = _, \n        control = rpart.control(minsplit = 2, cp = 0))\nrpart.plot(tree_full, extra = 2, \n           roundint=FALSE,\n            box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \n                               \"Or\", \"Rd\", \"Pu\")) # specify 7 colors\ntree_full\n## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##     2) milk=TRUE 41  0 mammal (1 0 0 0 0 0 0) *\n##     3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##       6) feathers=TRUE 20  0 bird (0 1 0 0 0 0 0) *\n##       7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##        14) fins=TRUE 13  0 fish (0 0 0 1 0 0 0) *\n##        15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##          30) backbone=TRUE 9  4 reptile (0 0 0.56 0 0.44 0 0)  \n##            60) aquatic=FALSE 4  0 reptile (0 0 1 0 0 0 0) *\n##            61) aquatic=TRUE 5  1 amphibian (0 0 0.2 0 0.8 0 0)  \n##             122) eggs=FALSE 1  0 reptile (0 0 1 0 0 0 0) *\n##             123) eggs=TRUE 4  0 amphibian (0 0 0 0 1 0 0) *\n##          31) backbone=FALSE 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56)  \n##            62) airborne=TRUE 6  0 insect (0 0 0 0 0 1 0) *\n##            63) airborne=FALSE 12  2 mollusc.et.al (0 0 0 0 0 0.17 0.83)  \n##             126) predator=FALSE 4  2 insect (0 0 0 0 0 0.5 0.5)  \n##               252) legs>=3 2  0 insect (0 0 0 0 0 1 0) *\n##               253) legs< 3 2  0 mollusc.et.al (0 0 0 0 0 0 1) *\n##             127) predator=TRUE 8  0 mollusc.et.al (0 0 0 0 0 0 1) *\npred_full <- predict(tree_full, Zoo, type = \"class\")\n\naccuracy(Zoo |> pull(type), pred_full)\n## [1] 1"},{"path":"classification-basic-concepts-and-techniques.html","id":"model-selection","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.5 Model Selection","text":"often can create many different models classification problem.\n, created decision tree using default settings also\nfull tree. question : one use. problem called\nmodel selection.order select model need split training data \nvalidation set training set actually used train model.\nerror rate validation set can used choose\nseveral models.Caret model selection build train() function. select\ndefault complexity cp = 0.01 full tree cp = 0 (see tuneGrid).\ntrControl specified validation set obtained. LGOCV\npicks randomly proportion p data train uses rest \nvalidation set. get better estimate \nerror, process repeated number times errors averaged.see case, full tree model performs slightly better.\nresult fact small dataset.","code":"\nfit <- Zoo |>\n  train(type ~ .,\n    data = _ ,\n    method = \"rpart\",\n    control = rpart.control(minsplit = 2), # we have little data\n    tuneGrid = data.frame(cp = c(0.01, 0)),\n    trControl = trainControl(method = \"LGOCV\", p = 0.8, number = 10),\n    tuneLength = 5)\n\nfit\n## CART \n## \n## 101 samples\n##  16 predictor\n##   7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Repeated Train/Test Splits Estimated (10 reps, 80%) \n## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa \n##   0.00  0.9444    0.9231\n##   0.01  0.9444    0.9231\n## \n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final value used for the model was cp = 0.01."},{"path":"classification-basic-concepts-and-techniques.html","id":"model-evaluation","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.6 Model Evaluation","text":"Models evaluated test set overlap \ntraining set. typically split data using random sampling.\nget reproducible results,\nset random number generator seed.","code":"\nset.seed(2000)"},{"path":"classification-basic-concepts-and-techniques.html","id":"holdout-method","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.6.1 Holdout Method","text":"Test data used model building process set aside purely\ntesting model. , partition data 80% training 20%\ntesting.Now can train test set get generalization error \ntest set.","code":"\ninTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]\nZoo_train <- Zoo |> slice(inTrain)\nZoo_test <- Zoo |> slice(-inTrain)"},{"path":"classification-basic-concepts-and-techniques.html","id":"cross-validation","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.6.2 Cross-Validation","text":"k-fold cross-validation splits data randomly \\(k\\) folds. \nholds one fold back testing trains \\(k-1\\) folds. \ndone fold resulting statistic (e.g., accuracy) averaged.\nmethod uses data efficiently holdout method.Cross validation can directly used train() using\ntrControl = trainControl(method = \"cv\", number = 10).\nmodel selection necessary give \ngeneralization error.Cross-validation runs independent can done faster \nparallel. enable multi-core support, caret uses package\nforeach need load backend. Linux, can use\ndoMC 4 cores. Windows needs different backend like doParallel\n(see caret cheat sheet ).","code":"\n## Linux backend\n# library(doMC)\n# registerDoMC(cores = 4)\n# getDoParWorkers()\n\n## Windows backend\n# library(doParallel)\n# cl <- makeCluster(4, type=\"SOCK\")\n# registerDoParallel(cl)"},{"path":"classification-basic-concepts-and-techniques.html","id":"hyperparameter-tuning","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7 Hyperparameter Tuning","text":"Hyperparameters parameters change \ntraining algorithm works. example complexity parameter\ncp rpart decision trees. Tuning hyperparameter means \nwant perform model selection pick best setting.typically use holdout method create test set use\ncross validation using training data model selection. Let us use\n80% training hold 20% testing.package caret combines training validation hyperparameter\ntuning train() function. internally splits \ndata training validation sets thus provide \nerror estimates different hyperparameter settings. trainControl \nused choose testing performed.rpart, train tries tune cp parameter (tree complexity)\nusing accuracy chose best model. set minsplit 2 since \nmuch data. Note: Parameters used tuning (case\ncp) need set using data.frame argument tuneGrid!\nSetting control ignored.Note: Train built 10 trees using training folds \nvalue cp reported values accuracy Kappa \naverages validation folds.model using best tuning parameters using data supplied\ntrain() available fit$finalModel.caret also computes variable importance. default uses competing\nsplits (splits runners , get chosen \ntree) rpart models (see ? varImp). Toothed runner \nmany splits, never gets chosen!variable importance without competing splits.Note: models provide variable importance function. \ncase caret might calculate variable importance ignore\nmodel (see ? varImp)!Now, can estimate generalization error best model \nheld test data.Caret’s confusionMatrix() function calculates accuracy, confidence\nintervals, kappa many evaluation metrics. need use\nseparate test data create confusion matrix based \ngeneralization error.notesMany classification algorithms train caret deal well\nmissing values. classification model can deal \nmissing values (e.g., rpart) use na.action = na.pass \ncall train predict. Otherwise, need remove\nobservations missing values na.omit use imputation \nreplace missing values train model. Make sure\nstill enough observations left.Make sure nominal variables (includes logical variables)\ncoded factors.class variable train caret level names \nkeywords R (e.g., TRUE FALSE). Rename , \nexample, “yes” “.”Make sure nominal variables (factors) examples \npossible values. methods might problems variable\nvalues without examples. can drop empty levels using\ndroplevels factor.Sampling train might create sample contain\nexamples values nominal (factor) variable. get\nerror message. likely happens variables \none rare value. may remove variable.","code":"\ninTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]\nZoo_train <- Zoo |> slice(inTrain)\nZoo_test <- Zoo |> slice(-inTrain)\nfit <- Zoo_train |>\n  train(type ~ .,\n    data = _ ,\n    method = \"rpart\",\n    control = rpart.control(minsplit = 2), # we have little data\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 5)\n\nfit\n## CART \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 73, 77, 75, 73, 75, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa \n##   0.00  0.9289    0.9058\n##   0.08  0.8603    0.8179\n##   0.16  0.7296    0.6422\n##   0.22  0.6644    0.5448\n##   0.32  0.4383    0.1136\n## \n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final value used for the model was cp = 0.\nlibrary(rpart.plot)\nrpart.plot(fit$finalModel, extra = 2,\n  box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \"Or\", \"Rd\", \"Pu\"))\nvarImp(fit)\n## rpart variable importance\n## \n##               Overall\n## toothedFALSE    100.0\n## feathersFALSE    79.5\n## eggsFALSE        67.7\n## milkFALSE        63.3\n## backboneFALSE    57.3\n## finsFALSE        53.5\n## hairFALSE        52.1\n## breathesFALSE    48.9\n## legs             41.4\n## tailFALSE        29.0\n## aquaticFALSE     27.5\n## airborneFALSE    26.5\n## predatorFALSE    10.6\n## venomousFALSE     1.8\n## catsizeFALSE      0.0\n## domesticFALSE     0.0\nimp <- varImp(fit, compete = FALSE)\nimp\n## rpart variable importance\n## \n##               Overall\n## milkFALSE      100.00\n## feathersFALSE   55.69\n## finsFALSE       39.45\n## aquaticFALSE    28.11\n## backboneFALSE   21.76\n## eggsFALSE       12.32\n## legs             7.28\n## tailFALSE        0.00\n## domesticFALSE    0.00\n## airborneFALSE    0.00\n## catsizeFALSE     0.00\n## toothedFALSE     0.00\n## venomousFALSE    0.00\n## hairFALSE        0.00\n## breathesFALSE    0.00\n## predatorFALSE    0.00\n\nggplot(imp)\npred <- predict(fit, newdata = Zoo_test)\npred\n##  [1] mammal        bird          mollusc.et.al bird         \n##  [5] mammal        mammal        insect        bird         \n##  [9] mammal        mammal        mammal        mammal       \n## [13] bird          fish          fish          reptile      \n## [17] mammal        mollusc.et.al\n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al\nconfusionMatrix(data = pred, \n                ref = Zoo_test |> pull(type))\n## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian insect\n##   mammal             8    0       0    0         0      0\n##   bird               0    4       0    0         0      0\n##   reptile            0    0       1    0         0      0\n##   fish               0    0       0    2         0      0\n##   amphibian          0    0       0    0         0      0\n##   insect             0    0       0    0         0      1\n##   mollusc.et.al      0    0       0    0         0      0\n##                Reference\n## Prediction      mollusc.et.al\n##   mammal                    0\n##   bird                      0\n##   reptile                   0\n##   fish                      0\n##   amphibian                 0\n##   insect                    0\n##   mollusc.et.al             2\n## \n## Overall Statistics\n##                                     \n##                Accuracy : 1         \n##                  95% CI : (0.815, 1)\n##     No Information Rate : 0.444     \n##     P-Value [Acc > NIR] : 4.58e-07  \n##                                     \n##                   Kappa : 1         \n##                                     \n##  Mcnemar's Test P-Value : NA        \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird Class: reptile\n## Sensitivity                  1.000       1.000         1.0000\n## Specificity                  1.000       1.000         1.0000\n## Pos Pred Value               1.000       1.000         1.0000\n## Neg Pred Value               1.000       1.000         1.0000\n## Prevalence                   0.444       0.222         0.0556\n## Detection Rate               0.444       0.222         0.0556\n## Detection Prevalence         0.444       0.222         0.0556\n## Balanced Accuracy            1.000       1.000         1.0000\n##                      Class: fish Class: amphibian Class: insect\n## Sensitivity                1.000               NA        1.0000\n## Specificity                1.000                1        1.0000\n## Pos Pred Value             1.000               NA        1.0000\n## Neg Pred Value             1.000               NA        1.0000\n## Prevalence                 0.111                0        0.0556\n## Detection Rate             0.111                0        0.0556\n## Detection Prevalence       0.111                0        0.0556\n## Balanced Accuracy          1.000               NA        1.0000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-basic-concepts-and-techniques.html","id":"pitfalls-of-model-selection-and-evaluation","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.8 Pitfalls of Model Selection and Evaluation","text":"training data test sets overlap \nevaluate generalization performance. training set can come\ncontaminated things like preprocessing data together.measure error training set \nuse validation error generalization error estimate.\nAlways use generalization error test set!","code":""},{"path":"classification-basic-concepts-and-techniques.html","id":"model-comparison","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.9 Model Comparison","text":"compare decision trees k-nearest neighbors (kNN)\nclassifier. create fixed sampling scheme (10-folds) \ncompare different models using exactly folds. \nspecified trControl training.Build modelsNote: kNN ask train scale data using\npreProcess = \"scale\". Logicals used 0-1 variables \nEuclidean distance calculation.Compare accuracy folds.caret provides visualizations. \nexample, boxplot compare accuracy kappa distribution (\n10 folds).see kNN performing consistently better folds CART\n(except outlier folds).Find one models statistically better (\ndifference accuracy zero).p-values tells probability seeing even extreme value\n(difference accuracy) given null hypothesis (difference\n= 0) true. better classifier, p-value less \n.05 0.01. diff automatically applies Bonferroni correction \nmultiple comparisons. case, kNN seems better classifiers\nperform statistically differently.","code":"\ntrain_index <- createFolds(Zoo_train$type, k = 10, returnTrain = TRUE)\nrpartFit <- Zoo_train |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        tuneLength = 10,\n        trControl = trainControl(method = \"cv\", index = train_index)\n  )\nknnFit <- Zoo_train |> \n  train(type ~ .,\n        data = _,\n        method = \"knn\",\n        preProcess = \"scale\",\n          tuneLength = 10,\n          trControl = trainControl(method = \"cv\", index = train_index)\n  )\nresamps <- resamples(list(\n        CART = rpartFit,\n        kNearestNeighbors = knnFit\n        ))\n\nsummary(resamps)\n## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: CART, kNearestNeighbors \n## Number of resamples: 10 \n## \n## Accuracy \n##                     Min. 1st Qu. Median   Mean 3rd Qu.  Max. NA's\n## CART              0.6667   0.750 0.7778 0.7890  0.8512 0.875    0\n## kNearestNeighbors 0.7500   0.875 0.8990 0.9076  1.0000 1.000    0\n## \n## Kappa \n##                     Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's\n## CART              0.5909  0.6800 0.7188 0.7227  0.7706 0.8298    0\n## kNearestNeighbors 0.6800  0.8315 0.8713 0.8816  1.0000 1.0000    0\nbwplot(resamps, layout = c(3, 1))\ndifs <- diff(resamps)\ndifs\n## \n## Call:\n## diff.resamples(x = resamps)\n## \n## Models: CART, kNearestNeighbors \n## Metrics: Accuracy, Kappa \n## Number of differences: 1 \n## p-value adjustment: bonferroni\n\nsummary(difs)\n## \n## Call:\n## summary.diff.resamples(object = difs)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## Accuracy \n##                   CART    kNearestNeighbors\n## CART                      -0.119           \n## kNearestNeighbors 0.00768                  \n## \n## Kappa \n##                   CART    kNearestNeighbors\n## CART                      -0.159           \n## kNearestNeighbors 0.00538"},{"path":"classification-basic-concepts-and-techniques.html","id":"feature-selection","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.10 Feature Selection*","text":"Decision trees implicitly select features splitting, can also\nselect features apply learning algorithm.\nSince different features lead different models, choosing\nbest set features also type \nmodel selection.Many feature selection methods implemented FSelector package.","code":"\nlibrary(FSelector)"},{"path":"classification-basic-concepts-and-techniques.html","id":"univariate-feature-importance-score","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.10.1 Univariate Feature Importance Score","text":"scores measure related feature class variable.\ndiscrete features (case), chi-square statistic can \nused derive score.can plot importance descending order (using reorder order factor\nlevels used ggplot).Picking best features called feature ranking approach.\npick 5 highest-ranked features.Use selected features build model (Fselector provides\n.simple.formula).many alternative ways calculate univariate importance\nscores (see package FSelector). (also) work continuous\nfeatures. One example information gain ratio based entropy \nused decision tree induction.","code":"\nweights <- Zoo_train |> \n  chi.squared(type ~ ., data = _) |>\n  as_tibble(rownames = \"feature\") |>\n  arrange(desc(attr_importance))\n\nweights\n## # A tibble: 16 × 2\n##    feature  attr_importance\n##    <chr>              <dbl>\n##  1 feathers           1    \n##  2 milk               1    \n##  3 backbone           1    \n##  4 toothed            0.981\n##  5 eggs               0.959\n##  6 breathes           0.917\n##  7 hair               0.906\n##  8 fins               0.845\n##  9 legs               0.834\n## 10 airborne           0.818\n## 11 tail               0.779\n## 12 aquatic            0.725\n## 13 catsize            0.602\n## 14 venomous           0.520\n## 15 predator           0.374\n## 16 domestic           0.256\nggplot(weights,\n  aes(x = attr_importance, y = reorder(feature, attr_importance))) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Importance score\") + \n  ylab(\"Feature\")\nsubset <- cutoff.k(weights |> \n                   column_to_rownames(\"feature\"), 5)\nsubset\n## [1] \"feathers\" \"milk\"     \"backbone\" \"toothed\"  \"eggs\"\nf <- as.simple.formula(subset, \"type\")\nf\n## type ~ feathers + milk + backbone + toothed + eggs\n## <environment: 0x59296aa5e5a8>\n\nm <- Zoo_train |> rpart(f, data = _)\nrpart.plot(m, extra = 2, roundint = FALSE)\nZoo_train |> \n  gain.ratio(type ~ ., data = _) |>\n  as_tibble(rownames = \"feature\") |>\n  arrange(desc(attr_importance))\n## # A tibble: 16 × 2\n##    feature  attr_importance\n##    <chr>              <dbl>\n##  1 milk               1    \n##  2 backbone           1    \n##  3 feathers           1    \n##  4 toothed            0.959\n##  5 eggs               0.907\n##  6 breathes           0.845\n##  7 hair               0.781\n##  8 fins               0.689\n##  9 legs               0.689\n## 10 airborne           0.633\n## 11 tail               0.573\n## 12 aquatic            0.474\n## 13 venomous           0.429\n## 14 catsize            0.310\n## 15 domestic           0.115\n## 16 predator           0.110"},{"path":"classification-basic-concepts-and-techniques.html","id":"feature-subset-selection-1","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.10.2 Feature Subset Selection","text":"Often, features related calculating importance feature\nindependently optimal. can use greedy search heuristics. \nexample cfs uses correlation/entropy best first search.Black-box feature selection uses evaluator function (black box)\ncalculate score maximized.\ntypically best method, since can use actual model\nselection.\nFirst, define evaluation\nfunction builds model given subset features calculates \nquality score. use average 5 bootstrap samples\n(method = \"cv\" can also used instead), tuning (faster),\naverage accuracy score.Start features (class variable type)several (greedy) search strategies available. run \ncommented . Remove comment one time\ntry type feature selection.","code":"\nZoo_train |> \n  cfs(type ~ ., data = _)\n##  [1] \"hair\"     \"feathers\" \"eggs\"     \"milk\"     \"toothed\"  \"backbone\"\n##  [7] \"breathes\" \"fins\"     \"legs\"     \"tail\"\nevaluator <- function(subset) {\n  model <- Zoo_train |> \n    train(as.simple.formula(subset, \"type\"),\n          data = _,\n          method = \"rpart\",\n          trControl = trainControl(method = \"boot\", number = 5),\n          tuneLength = 0)\n  results <- model$resample$Accuracy\n  cat(\"Trying features:\", paste(subset, collapse = \" + \"), \"\\n\")\n  m <- mean(results)\n  cat(\"Accuracy:\", round(m, 2), \"\\n\\n\")\n  m\n}\nfeatures <- Zoo_train |> colnames() |> setdiff(\"type\")\n#subset <- backward.search(features, evaluator)\n#subset <- forward.search(features, evaluator)\n#subset <- best.first.search(features, evaluator)\n#subset <- hill.climbing.search(features, evaluator)\n#\n#subset"},{"path":"classification-basic-concepts-and-techniques.html","id":"using-dummy-variables-for-factors","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.10.3 Using Dummy Variables for Factors","text":"Nominal features (factors) often encoded series 0-1 dummy\nvariables. example, let us try predict animal predator\ngiven type. First use original encoding type factor\nseveral values.Note: splits use multiple values. Building tree become\nextremely slow factor many levels (different values) since \ntree check possible splits two subsets. situation\navoided.Convert type set 0-1 dummy variables using class2ind. See\nalso ? dummyVars package caret.Using caret original factor encoding automatically translates\nfactors (type) 0-1 dummy variables (e.g., typeinsect = 0).\nreason models directly use factors caret\ntries consistently work .Note: use fixed value tuning parameter cp, \ncreate tuning grid contains value.","code":"\ntree_predator <- Zoo_train |> \n  rpart(predator ~ type, data = _)\nrpart.plot(tree_predator, extra = 2, roundint = FALSE)\nZoo_train_dummy <- as_tibble(class2ind(Zoo_train$type)) |> \n  mutate(across(everything(), as.factor)) |>\n  add_column(predator = Zoo_train$predator)\nZoo_train_dummy\n## # A tibble: 83 × 8\n##    mammal bird  reptile fish  amphibian insect mollusc.et.al predator\n##    <fct>  <fct> <fct>   <fct> <fct>     <fct>  <fct>         <fct>   \n##  1 1      0     0       0     0         0      0             TRUE    \n##  2 0      0     0       1     0         0      0             TRUE    \n##  3 1      0     0       0     0         0      0             TRUE    \n##  4 1      0     0       0     0         0      0             TRUE    \n##  5 1      0     0       0     0         0      0             FALSE   \n##  6 1      0     0       0     0         0      0             FALSE   \n##  7 0      0     0       1     0         0      0             FALSE   \n##  8 0      0     0       1     0         0      0             TRUE    \n##  9 1      0     0       0     0         0      0             FALSE   \n## 10 1      0     0       0     0         0      0             TRUE    \n## # ℹ 73 more rows\n\ntree_predator <- Zoo_train_dummy |> \n  rpart(predator ~ ., \n        data = _,\n        control = rpart.control(minsplit = 2, cp = 0.01))\nrpart.plot(tree_predator, roundint = FALSE)\nfit <- Zoo_train |> \n  train(predator ~ type, \n        data = _, \n        method = \"rpart\",\n        control = rpart.control(minsplit = 2),\n        tuneGrid = data.frame(cp = 0.01))\nfit\n## CART \n## \n## 83 samples\n##  1 predictor\n##  2 classes: 'TRUE', 'FALSE' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... \n## Resampling results:\n## \n##   Accuracy  Kappa \n##   0.5635    0.1166\n## \n## Tuning parameter 'cp' was held constant at a value of 0.01\n\nrpart.plot(fit$finalModel, extra = 2)"},{"path":"classification-basic-concepts-and-techniques.html","id":"exercises-1","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.11 Exercises*","text":"use Palmer penguin data exercises.Create R markdown file code discussion following .\nRemember, complete approach described section Hyperparameter Tuning.Split data training test set.Create rpart decision tree predict species. deal \nmissing values.Experiment setting minsplit rpart make sure tuneLength \nleast 5.\nDiscuss model selection process (hyperparameter tuning) final\nmodel chosen.Visualize tree discuss splits mean.Calculate variable importance fitted model. variables \nimportant? variables matter?Use test set evaluate generalization error accuracy.","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm\n##   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n## 1 Adelie  Torgersen           39.1          18.7               181\n## 2 Adelie  Torgersen           39.5          17.4               186\n## 3 Adelie  Torgersen           40.3          18                 195\n## 4 Adelie  Torgersen           NA            NA                  NA\n## 5 Adelie  Torgersen           36.7          19.3               193\n## 6 Adelie  Torgersen           39.3          20.6               190\n## # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"classification-alternative-techniques.html","id":"classification-alternative-techniques","chapter":"4 Classification: Alternative Techniques","heading":"4 Classification: Alternative Techniques","text":"Install packages used chapter:packages used chapter : basemodels (Chen et al. 2023), C50 (M. Kuhn Quinlan 2023), caret (M. Kuhn 2023), e1071 (Meyer et al. 2023), keras (Allaire Chollet 2024), klaR (Roever et al. 2023), lattice (Sarkar 2023), MASS (B. Ripley 2023a), mlbench (Leisch Dimitriadou 2024), nnet (B. Ripley 2023b), palmerpenguins (Horst, Hill, Gorman 2022), randomForest (Breiman et al. 2022), rpart (Therneau Atkinson 2023), RWeka (Hornik 2023), scales (Wickham, Pedersen, Seidel 2023), tidyverse (Wickham 2023c), xgboost (Chen et al. 2024)use tidyverse prepare data.Show fewer digits","code":"\npkgs <- c(\"basemodels\", \"C50\", \"caret\", \"e1071\", \"keras\", \"klaR\", \n          \"lattice\", \"MASS\", \"mlbench\", \"nnet\", \"palmerpenguins\", \n          \"randomForest\", \"rpart\", \"RWeka\", \"scales\", \"tidyverse\", \n          \"xgboost\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)\nlibrary(tidyverse)\noptions(digits=3)"},{"path":"classification-alternative-techniques.html","id":"types-of-classifiers","chapter":"4 Classification: Alternative Techniques","heading":"4.1 Types of Classifiers","text":"Many different classification algorithms\nproposed literature.\nchapter, apply popular methods.","code":""},{"path":"classification-alternative-techniques.html","id":"set-up-the-training-and-test-data","chapter":"4 Classification: Alternative Techniques","heading":"4.1.1 Set up the Training and Test Data","text":"use Zoo dataset included R package mlbench\n(may install ). Zoo dataset containing 17 (mostly\nlogical) variables different 101 animals data frame 17\ncolumns (hair, feathers, eggs, milk, airborne, aquatic, predator,\ntoothed, backbone, breathes, venomous, fins, legs, tail, domestic,\ncatsize, type). convert data frame tidyverse tibble\n(optional).use package caret \nmake preparing training sets building classification (\nregression) models easier. great cheat sheet can found\n.Multi-core support can used cross-validation. Note: \ncommented work rJava used RWeka-based\nclassifiers\n.Test data used model building process needs set\naside purely testing model completely built. \nuse 80% training.train() aware tunable hyperparameters method \nautomatically performs model selection using validation set.\nuse models validation sets \ncreating fixed sampling scheme (10-folds). help can compare fitted models later.fixed folds used train() argument\ntrControl = trainControl(method = \"cv\", index = train_index)). \ndon’t need fixed folds, remove index = train_index \ncode .help building models caret see: ? trainNote: careful many NA values data.\ntrain() cross-validation many fail cases. \ncase can remove features (columns) many NAs, omit\nNAs using na.omit() use imputation replace \nreasonable values (e.g., feature mean via kNN). Highly\nimbalanced datasets also problematic since chance \nfold contain examples class leading hard \nunderstand error message.","code":"\ndata(Zoo, package=\"mlbench\")\nZoo <- as_tibble(Zoo)\nZoo\n## # A tibble: 101 × 17\n##    hair  feathers eggs  milk  airborne aquatic predator toothed\n##    <lgl> <lgl>    <lgl> <lgl> <lgl>    <lgl>   <lgl>    <lgl>  \n##  1 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE     TRUE   \n##  2 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE    TRUE   \n##  3 FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE     TRUE   \n##  4 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE     TRUE   \n##  5 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE     TRUE   \n##  6 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE    TRUE   \n##  7 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE    TRUE   \n##  8 FALSE FALSE    TRUE  FALSE FALSE    TRUE    FALSE    TRUE   \n##  9 FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE     TRUE   \n## 10 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE    TRUE   \n## # ℹ 91 more rows\n## # ℹ 9 more variables: backbone <lgl>, breathes <lgl>, venomous <lgl>,\n## #   fins <lgl>, legs <int>, tail <lgl>, domestic <lgl>,\n## #   catsize <lgl>, type <fct>\nlibrary(caret)\n##library(doMC, quietly = TRUE)\n##registerDoMC(cores = 4)\n##getDoParWorkers()\ninTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]\nZoo_train <- Zoo |> slice(inTrain)\nZoo_test <- Zoo |> slice(-inTrain)\ntrain_index <- createFolds(Zoo_train$type, k = 10, returnTrain = TRUE)"},{"path":"classification-alternative-techniques.html","id":"rule-based-classifier-part","chapter":"4 Classification: Alternative Techniques","heading":"4.2 Rule-based classifier: PART","text":"model selection results shown table. selected model:PART returns decision list, .e., ordered rule set. example,\nfirst rule shows animal feathers milk mammal.\nordered rule sets, decision first matching rule used.","code":"\nrulesFit <- Zoo_train |> train(type ~ .,\n  method = \"PART\",\n  data = _,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\", index = train_index))\nrulesFit\n## Rule-Based Classifier \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results across tuning parameters:\n## \n##   threshold  pruned  Accuracy  Kappa\n##   0.010      yes     0.957     0.944\n##   0.010      no      0.947     0.931\n##   0.133      yes     0.957     0.944\n##   0.133      no      0.947     0.931\n##   0.255      yes     0.957     0.944\n##   0.255      no      0.947     0.931\n##   0.378      yes     0.957     0.944\n##   0.378      no      0.947     0.931\n##   0.500      yes     0.957     0.944\n##   0.500      no      0.947     0.931\n## \n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final values used for the model were threshold = 0.5 and pruned\n##  = yes.\nrulesFit$finalModel\n## PART decision list\n## ------------------\n## \n## feathersTRUE <= 0 AND\n## milkTRUE > 0: mammal (33.0)\n## \n## feathersTRUE > 0: bird (16.0)\n## \n## backboneTRUE <= 0 AND\n## airborneTRUE <= 0: mollusc.et.al (9.0/1.0)\n## \n## airborneTRUE <= 0 AND\n## finsTRUE > 0: fish (11.0)\n## \n## airborneTRUE > 0: insect (6.0)\n## \n## aquaticTRUE > 0: amphibian (5.0/1.0)\n## \n## : reptile (3.0)\n## \n## Number of Rules  :   7"},{"path":"classification-alternative-techniques.html","id":"nearest-neighbor-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.3 Nearest Neighbor Classifier","text":"K-Nearest neighbor classifiers classify new data point looking \nmajority class labels k nearest neighbors training data set.\nused kNN implementation uses Euclidean distance determine data points\nnear , data needs standardized\n(scaled) first. legs measured 0 6 \nvariables 0 1. Scaling z-scores can directly performed \npreprocessing train using parameter preProcess = \"scale\".\\(k\\) value typically choose odd number get clear majority.kNN classifiers lazy models meaning instead learning, just\nkeep complete dataset. final model just gives us summary statistic class labels training data.","code":"\nknnFit <- Zoo_train |> train(type ~ .,\n  method = \"knn\",\n  data = _,\n  preProcess = \"scale\",\n  tuneGrid = data.frame(k = c(1, 3, 5, 7, 9)),\n    trControl = trainControl(method = \"cv\", index = train_index))\nknnFit\n## k-Nearest Neighbors \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## Pre-processing: scaled (16) \n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results across tuning parameters:\n## \n##   k  Accuracy  Kappa\n##   1  0.934     0.916\n##   3  0.933     0.914\n##   5  0.923     0.901\n##   7  0.898     0.871\n##   9  0.899     0.868\n## \n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final value used for the model was k = 1.\nknnFit$finalModel\n## 1-nearest neighbor model\n## Training set outcome distribution:\n## \n##        mammal          bird       reptile          fish     amphibian \n##            33            16             4            11             4 \n##        insect mollusc.et.al \n##             7             8"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.4 Naive Bayes Classifier","text":"Caret’s train formula interface translates logicals factors dummy\nvariables classifier interprets numbers used Gaussian naive Bayes estimation. avoid , directly specify x y.final model contains prior probabilities class.conditional probabilities table feature. brevity,\nshow tables first three features. example,\ncondition probability\n\\(P(\\text{hair} = \\text{TRUE} | \\text{class} = \\text{mammal})\\) \n0.964.","code":"\nNBFit <- train(x = as.data.frame(Zoo_train[, -ncol(Zoo_train)]), \n               y = pull(Zoo_train, \"type\"),\n               method = \"nb\",\n               tuneGrid = data.frame(fL = c(.2, .5, 1, 5), \n                                     usekernel = TRUE, adjust = 1),\n               trControl = trainControl(method = \"cv\", index = train_index))\nNBFit\n## Naive Bayes \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results across tuning parameters:\n## \n##   fL   Accuracy  Kappa\n##   0.2  0.945     0.929\n##   0.5  0.945     0.929\n##   1.0  0.934     0.916\n##   5.0  0.898     0.870\n## \n## Tuning parameter 'usekernel' was held constant at a value of\n##  TRUE\n## Tuning parameter 'adjust' was held constant at a value of 1\n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final values used for the model were fL = 0.2, usekernel =\n##  TRUE and adjust = 1.\nNBFit$finalModel$apriori\n## grouping\n##        mammal          bird       reptile          fish     amphibian \n##        0.3976        0.1928        0.0482        0.1325        0.0482 \n##        insect mollusc.et.al \n##        0.0843        0.0964\nNBFit$finalModel$tables[1:3]\n## $hair\n##                var\n## grouping         FALSE   TRUE\n##   mammal        0.0359 0.9641\n##   bird          0.9878 0.0122\n##   reptile       0.9545 0.0455\n##   fish          0.9825 0.0175\n##   amphibian     0.9545 0.0455\n##   insect        0.4324 0.5676\n##   mollusc.et.al 0.9762 0.0238\n## \n## $feathers\n##                var\n## grouping          FALSE    TRUE\n##   mammal        0.99401 0.00599\n##   bird          0.01220 0.98780\n##   reptile       0.95455 0.04545\n##   fish          0.98246 0.01754\n##   amphibian     0.95455 0.04545\n##   insect        0.97297 0.02703\n##   mollusc.et.al 0.97619 0.02381\n## \n## $eggs\n##                var\n## grouping         FALSE   TRUE\n##   mammal        0.9641 0.0359\n##   bird          0.0122 0.9878\n##   reptile       0.2727 0.7273\n##   fish          0.0175 0.9825\n##   amphibian     0.0455 0.9545\n##   insect        0.0270 0.9730\n##   mollusc.et.al 0.1429 0.8571"},{"path":"classification-alternative-techniques.html","id":"bayesian-network","chapter":"4 Classification: Alternative Techniques","heading":"4.5 Bayesian Network","text":"Bayesian networks covered . R good\nsupport modeling Bayesian Networks. example \npackage bnlearn.","code":""},{"path":"classification-alternative-techniques.html","id":"logistic-regression","chapter":"4 Classification: Alternative Techniques","heading":"4.6 Logistic regression","text":"Logistic regression powerful classification method\nalways tried \none first models.\ndetailed discussion code available Section\nLogistic Regression*.Regular logistic regression predicts one outcome coded \nbinary variable. Since data several\nclasses, use multinomial logistic regression,\nalso called log-linear model\nextension logistic regresses multi-class problems.coefficients log-odds ratios. negative log-odds ratio means odds go increase \nvalue predictor. predictor \npositive log-odds ratio increases odds. example,\nmodel , hair=TRUE negative coefficient \nbird feathers=TRUE large positive coefficient.","code":"\nlogRegFit <- Zoo_train |> train(type ~ .,\n  method = \"multinom\",\n  data = _,\n  trace = FALSE, # suppress some output\n    trControl = trainControl(method = \"cv\", index = train_index))\nlogRegFit\n## Penalized Multinomial Regression \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results across tuning parameters:\n## \n##   decay  Accuracy  Kappa\n##   0e+00  0.906     0.877\n##   1e-04  0.884     0.849\n##   1e-01  0.923     0.901\n## \n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final value used for the model was decay = 0.1.\nlogRegFit$finalModel\n## Call:\n## nnet::multinom(formula = .outcome ~ ., data = dat, decay = param$decay, \n##     trace = FALSE)\n## \n## Coefficients:\n##               (Intercept) hairTRUE feathersTRUE eggsTRUE milkTRUE\n## bird               -0.333   -1.047        2.970    0.828   -1.252\n## reptile             0.013   -2.081       -1.089    0.673   -2.180\n## fish               -0.175   -0.276       -0.114    1.882   -1.357\n## amphibian          -1.283   -1.516       -0.270    0.680   -1.601\n## insect             -0.753   -0.390       -0.145    0.898   -1.013\n## mollusc.et.al       1.521   -1.229       -0.249    0.932   -0.904\n##               airborneTRUE aquaticTRUE predatorTRUE toothedTRUE\n## bird                1.1731      -0.159       0.2231      -1.785\n## reptile            -0.5180      -1.089       0.0417      -0.200\n## fish               -0.0901       0.509      -0.3309       0.412\n## amphibian          -0.3665       1.627      -0.1399       0.740\n## insect              1.3740      -1.075      -1.1174      -1.185\n## mollusc.et.al      -1.1788       0.716       0.8307      -1.739\n##               backboneTRUE breathesTRUE venomousTRUE finsTRUE   legs\n## bird                 0.474        0.134       -0.328 -0.54598 -0.599\n## reptile              0.897       -0.504        1.278 -1.19220 -0.242\n## fish                 0.277       -1.971       -0.420  1.47242 -1.158\n## amphibian            0.256        0.459        0.161 -0.62875  0.093\n## insect              -1.572        0.134       -0.257 -0.00253  0.591\n## mollusc.et.al       -2.605       -0.629        0.841 -0.20610  0.121\n##               tailTRUE domesticTRUE catsizeTRUE\n## bird             0.595       0.1418      -0.118\n## reptile          1.186      -0.4089      -0.430\n## fish             0.323       0.0864      -0.313\n## amphibian       -1.353      -0.4055      -1.358\n## insect          -1.691      -0.2492      -1.042\n## mollusc.et.al   -0.735      -0.2260      -0.708\n## \n## Residual Deviance: 35.5 \n## AIC: 239"},{"path":"classification-alternative-techniques.html","id":"artificial-neural-network-ann","chapter":"4 Classification: Alternative Techniques","heading":"4.7 Artificial Neural Network (ANN)","text":"Standard networks input layer, output layer \nsingle hidden layer.input layer size 16, one input feature \noutput layer size 7 representing 7 classes.\nModel selection chose network architecture hidden\nlayer 5 units\nresulting 127 learned weights.\nSince model considered black-box model network architecture\nused variables shown summary.","code":"\nnnetFit <- Zoo_train |> train(type ~ .,\n  method = \"nnet\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", index = train_index),\n  trace = FALSE # no progress output\n  )\nnnetFit\n## Neural Network \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results across tuning parameters:\n## \n##   size  decay  Accuracy  Kappa\n##   1     0e+00  0.747     0.654\n##   1     1e-04  0.713     0.584\n##   1     1e-03  0.876     0.836\n##   1     1e-02  0.804     0.746\n##   1     1e-01  0.734     0.644\n##   3     0e+00  0.848     0.800\n##   3     1e-04  0.838     0.788\n##   3     1e-03  0.911     0.886\n##   3     1e-02  0.923     0.898\n##   3     1e-01  0.876     0.840\n##   5     0e+00  0.897     0.864\n##   5     1e-04  0.911     0.885\n##   5     1e-03  0.933     0.914\n##   5     1e-02  0.946     0.929\n##   5     1e-01  0.911     0.886\n##   7     0e+00  0.911     0.885\n##   7     1e-04  0.895     0.864\n##   7     1e-03  0.912     0.887\n##   7     1e-02  0.933     0.914\n##   7     1e-01  0.933     0.914\n##   9     0e+00  0.917     0.891\n##   9     1e-04  0.905     0.873\n##   9     1e-03  0.922     0.901\n##   9     1e-02  0.923     0.901\n##   9     1e-01  0.923     0.901\n## \n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final values used for the model were size = 5 and decay = 0.01.\nnnetFit$finalModel\n## a 16-5-7 network with 127 weights\n## inputs: hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \n## output(s): .outcome \n## options were - softmax modelling  decay=0.01"},{"path":"classification-alternative-techniques.html","id":"deep-learning-with-kerastensorflow","chapter":"4 Classification: Alternative Techniques","heading":"4.8 Deep Learning with keras/tensorflow","text":"Using deep learning requires extra setup. helpful\ndataset use, included demonstrate set \ntrain simple model architecture.keras package needs packages reticulate tensorflow. \ninstall keras need tohave working Python installation,install keras R package install.packages(\"keras\"), andinstall tensorflow/keras Python modules \nlibrary(keras); install_keras()Prepare data. data needs matrix \nnumeric/integer values. class variable needs one-hot encoded \nkeras function to_categorical() expects class index starting 0\ninstead 1.Deep learning uses large set hyper-parameters. Choices \nactivation function, number layers, number units per layer, \noptimizer, type regularization used.define simple network single dense hidden layer.\nNote keras, specify input size first dense layer.\noutput classifier categorical class value, therefore\nuse output layer softmax activation function, \ntraining specify\ncategorical cross-entropy loss function, accuracy\nadditional metric.\nAlso, L2 regularization used weights hidden layer \nreduce overfitting.model training, need specify batch size number \ntraining epochs. fitting process can also use fraction \ntraining data validation provide generalization loss/accuracy.create predictions model, convert one-hot\nencoding back class labels.","code":"\nlibrary(keras)\nX <- Zoo_train |> \n  select(!type) |> \n  mutate(across(everything(), as.integer)) |> \n  as.matrix()\nhead(X)\n##      hair feathers eggs milk airborne aquatic predator toothed\n## [1,]    1        0    0    1        0       0        0       1\n## [2,]    0        0    1    0        0       1        1       1\n## [3,]    1        0    0    1        0       0        1       1\n## [4,]    1        0    0    1        0       0        1       1\n## [5,]    1        0    0    1        0       0        0       1\n## [6,]    1        0    0    1        0       0        0       1\n##      backbone breathes venomous fins legs tail domestic catsize\n## [1,]        1        1        0    0    4    1        0       1\n## [2,]        1        0        0    1    0    1        0       0\n## [3,]        1        1        0    0    4    0        0       1\n## [4,]        1        1        0    0    4    1        0       1\n## [5,]        1        1        0    0    4    1        0       1\n## [6,]        1        1        0    0    4    1        1       1\n\ny <- Zoo_train |> \n  pull(\"type\") |> \n  { function(x)(as.integer(x) - 1L) }() |>   ## make index start with 0\n  to_categorical()\nhead(y)\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [1,]    1    0    0    0    0    0    0\n## [2,]    0    0    0    1    0    0    0\n## [3,]    1    0    0    0    0    0    0\n## [4,]    1    0    0    0    0    0    0\n## [5,]    1    0    0    0    0    0    0\n## [6,]    1    0    0    0    0    0    0\n\nX_test <- Zoo_test |> \n  select(!type) |> \n  mutate(across(everything(), as.integer)) |> \n  as.matrix()\ny_test <- Zoo_test |> \n  pull(\"type\") |> \n  { function(x)(as.integer(x) - 1L) }() |>\n  to_categorical()\nmodel <- keras_model_sequential(input_shape = c(ncol(X)), \n              name = \"single_hidden_layer_classifier\") |>\n  layer_dense(units = 10, activation = 'relu', \n              kernel_regularizer=regularizer_l2(l=0.01), name = \"hidden1\") |>\n  layer_dense(units = ncol(y), activation = 'softmax', name = \"output\")\n\nmodel\n## Model: \"single_hidden_layer_classifier\"\n## ______________________________________________________________________\n##  Layer (type)                  Output Shape                Param #    \n## ======================================================================\n##  hidden1 (Dense)               (None, 10)                  170        \n##  output (Dense)                (None, 7)                   77         \n## ======================================================================\n## Total params: 247\n## Trainable params: 247\n## Non-trainable params: 0\n## ______________________________________________________________________\nmodel <- model |>  \n  compile(loss = 'categorical_crossentropy', \n          optimizer = 'adam', \n          metrics = 'accuracy')\nhistory <- model |>\n  fit(\n    X, \n    y,\n    batch_size = 10,\n    epochs = 100,\n    validation_split = .2\n  )\n\nplot(history)\nclass_labels <- levels(Zoo_train |> pull(type))\n\npr <- predict(model, X_test) |> \n  apply(MARGIN = 1, FUN = which.max)\n## 1/1 - 0s - 51ms/epoch - 51ms/step\npr <- factor(pr, labels = class_labels, levels = seq_along(class_labels))\n\npr\n##  [1] mammal fish   bird   fish   mammal insect mammal mammal mammal\n## [10] mammal bird   mammal mammal bird   fish   bird   bird   bird  \n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al"},{"path":"classification-alternative-techniques.html","id":"support-vector-machines","chapter":"4 Classification: Alternative Techniques","heading":"4.9 Support Vector Machines","text":"use linear support vector machine.\nSupport vector machines can use kernels create non-linear decision boundaries.\nmethod can changed \"svmPoly\" \"svmRadial\" \nuse kernels. choice kernel typically make experimentation.support vectors determining decision boundary stored model.","code":"\nsvmFit <- Zoo_train |> train(type ~.,\n  method = \"svmLinear\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", index = train_index))\nsvmFit\n## Support Vector Machines with Linear Kernel \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.936     0.917\n## \n## Tuning parameter 'C' was held constant at a value of 1\nsvmFit$finalModel\n## Support Vector Machine object of class \"ksvm\" \n## \n## SV type: C-svc  (classification) \n##  parameter : cost C = 1 \n## \n## Linear (vanilla) kernel function. \n## \n## Number of Support Vectors : 42 \n## \n## Objective Function Value : -0.143 -0.22 -0.15 -0.176 -0.0943 -0.105 -0.28 -0.0808 -0.154 -0.0902 -0.114 -0.173 -0.589 -0.13 -0.185 -0.116 -0.0472 -0.0803 -0.125 -0.15 -0.57 \n## Training error : 0"},{"path":"classification-alternative-techniques.html","id":"ensemble-methods","chapter":"4 Classification: Alternative Techniques","heading":"4.10 Ensemble Methods","text":"Many ensemble methods available R. cover code two\npopular methods.","code":""},{"path":"classification-alternative-techniques.html","id":"random-forest","chapter":"4 Classification: Alternative Techniques","heading":"4.10.1 Random Forest","text":"default number trees 500 \nmtry determines number variables randomly sampled candidates\nsplit. number tradeoff larger number allows tree\npick better splits, smaller\nnumber increases independence trees.model set 500 trees prediction made applying trees \nusing majority vote.Since random forests use bagging (bootstrap sampling train trees),\nremaining data can used like test set. resulting error\ncalled --bag (OOB) error gives estimate \ngeneralization error. model also shows confusion matrix based \nOOB error.","code":"\nrandomForestFit <- Zoo_train |> train(type ~ .,\n  method = \"rf\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", index = train_index))\nrandomForestFit\n## Random Forest \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results across tuning parameters:\n## \n##   mtry  Accuracy  Kappa\n##    2    0.933     0.914\n##    5    0.936     0.917\n##    9    0.947     0.931\n##   12    0.947     0.932\n##   16    0.947     0.931\n## \n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final value used for the model was mtry = 9.\nrandomForestFit$finalModel\n## \n## Call:\n##  randomForest(x = x, y = y, mtry = param$mtry) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 9\n## \n##         OOB estimate of  error rate: 7.23%\n## Confusion matrix:\n##               mammal bird reptile fish amphibian insect mollusc.et.al\n## mammal            33    0       0    0         0      0             0\n## bird               0   16       0    0         0      0             0\n## reptile            0    0       2    1         1      0             0\n## fish               0    0       0   11         0      0             0\n## amphibian          0    0       1    0         3      0             0\n## insect             0    0       0    0         0      6             1\n## mollusc.et.al      0    0       1    0         0      1             6\n##               class.error\n## mammal              0.000\n## bird                0.000\n## reptile             0.500\n## fish                0.000\n## amphibian           0.250\n## insect              0.143\n## mollusc.et.al       0.250"},{"path":"classification-alternative-techniques.html","id":"gradient-boosted-decision-trees-xgboost","chapter":"4 Classification: Alternative Techniques","heading":"4.10.2 Gradient Boosted Decision Trees (xgboost)","text":"idea gradient boosting learn base model learn\nsuccessive models predict correct error previous models.\nTypically, tree models used.final model complicated set trees, summary information\nshown.","code":"\nxgboostFit <- Zoo_train |> train(type ~ .,\n  method = \"xgbTree\",\n  data = _,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\", index = train_index),\n  tuneGrid = expand.grid(\n    nrounds = 20,\n    max_depth = 3,\n    colsample_bytree = .6,\n    eta = 0.1,\n    gamma=0,\n    min_child_weight = 1,\n    subsample = .5\n  ))\nxgboostFit\n## eXtreme Gradient Boosting \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.944     0.927\n## \n## Tuning parameter 'nrounds' was held constant at a value of 20\n##  a value of 1\n## Tuning parameter 'subsample' was held constant at\n##  a value of 0.5\nxgboostFit$finalModel\n## ##### xgb.Booster\n## raw: 111.6 Kb \n## call:\n##   xgboost::xgb.train(params = list(eta = param$eta, max_depth = param$max_depth, \n##     gamma = param$gamma, colsample_bytree = param$colsample_bytree, \n##     min_child_weight = param$min_child_weight, subsample = param$subsample), \n##     data = x, nrounds = param$nrounds, num_class = length(lev), \n##     objective = \"multi:softprob\")\n## params (as set within xgb.train):\n##   eta = \"0.1\", max_depth = \"3\", gamma = \"0\", colsample_bytree = \"0.6\", min_child_weight = \"1\", subsample = \"0.5\", num_class = \"7\", objective = \"multi:softprob\", validate_parameters = \"TRUE\"\n## xgb.attributes:\n##   niter\n## callbacks:\n##   cb.print.evaluation(period = print_every_n)\n## # of features: 16 \n## niter: 20\n## nfeatures : 16 \n## xNames : hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \n## problemType : Classification \n## tuneValue :\n##    nrounds max_depth eta gamma colsample_bytree min_child_weight\n## 1      20         3 0.1     0              0.6                1\n##   subsample\n## 1       0.5\n## obsLevels : mammal bird reptile fish amphibian insect mollusc.et.al \n## param :\n##  list()"},{"path":"classification-alternative-techniques.html","id":"class-imbalance","chapter":"4 Classification: Alternative Techniques","heading":"4.11 Class Imbalance","text":"Classifiers hard time learn data much \nobservations one class (called majority class). called\nclass imbalance problem.good article problem \nsolutions.Class distributionTo create imbalanced problem, want decide animal \nreptile. First, change class variable make binary\nreptile/reptile classification problem. Note: use \ntraining data testing. use separate testing data set!forget make class variable factor (nominal variable)\nget regression tree instead classification tree.See class imbalance problem.Create test training data. use 50/50 split make sure\ntest set samples rare reptile class.new class variable clearly balanced. problem \nbuilding tree!","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\ndata(Zoo, package = \"mlbench\")\nggplot(Zoo, aes(y = type)) + geom_bar()\nZoo_reptile <- Zoo |> \n  mutate(type = factor(Zoo$type == \"reptile\", \n                       levels = c(FALSE, TRUE),\n                       labels = c(\"nonreptile\", \"reptile\")))\nsummary(Zoo_reptile)\n##     hair          feathers          eggs            milk        \n##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:58        FALSE:81        FALSE:42        FALSE:60       \n##  TRUE :43        TRUE :20        TRUE :59        TRUE :41       \n##                                                                 \n##                                                                 \n##                                                                 \n##   airborne        aquatic         predator        toothed       \n##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:77        FALSE:65        FALSE:45        FALSE:40       \n##  TRUE :24        TRUE :36        TRUE :56        TRUE :61       \n##                                                                 \n##                                                                 \n##                                                                 \n##   backbone        breathes        venomous          fins        \n##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:18        FALSE:21        FALSE:93        FALSE:84       \n##  TRUE :83        TRUE :80        TRUE :8         TRUE :17       \n##                                                                 \n##                                                                 \n##                                                                 \n##       legs         tail          domestic        catsize       \n##  Min.   :0.00   Mode :logical   Mode :logical   Mode :logical  \n##  1st Qu.:2.00   FALSE:26        FALSE:88        FALSE:57       \n##  Median :4.00   TRUE :75        TRUE :13        TRUE :44       \n##  Mean   :2.84                                                  \n##  3rd Qu.:4.00                                                  \n##  Max.   :8.00                                                  \n##          type   \n##  nonreptile:96  \n##  reptile   : 5  \n##                 \n##                 \n##                 \n## \nggplot(Zoo_reptile, aes(y = type)) + geom_bar()\nset.seed(1234)\n\ninTrain <- createDataPartition(y = Zoo_reptile$type, p = .5)[[1]]\ntraining_reptile <- Zoo_reptile |> slice(inTrain)\ntesting_reptile <- Zoo_reptile |> slice(-inTrain)"},{"path":"classification-alternative-techniques.html","id":"option-1-use-the-data-as-is-and-hope-for-the-best","chapter":"4 Classification: Alternative Techniques","heading":"4.11.1 Option 1: Use the Data As Is and Hope For The Best","text":"Warnings: “missing values resampled performance\nmeasures.” means test folds contain examples \nclasses. likely class imbalance small datasets.tree predicts everything non-reptile. look error \ntest set.Accuracy high, exactly -information rate\nkappa zero. Sensitivity also zero, meaning \nidentify positive (reptile). cost missing positive \nmuch larger cost associated misclassifying negative,\naccuracy good measure! dealing imbalance, \nconcerned accuracy, want increase \nsensitivity, .e., chance identify positive examples.Note: positive class value (one want detect) \nset manually reptile using positive = \"reptile\". Otherwise\nsensitivity/specificity correctly calculated.","code":"\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"))\n## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info =\n## trainInfo, : There were missing values in resampled performance\n## measures.\nfit\n## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 47, 46, 46, 45, 46, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.947     0    \n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         48       2\n##   reptile             0       0\n##                                         \n##                Accuracy : 0.96          \n##                  95% CI : (0.863, 0.995)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.677         \n##                                         \n##                   Kappa : 0             \n##                                         \n##  Mcnemar's Test P-Value : 0.480         \n##                                         \n##             Sensitivity : 0.00          \n##             Specificity : 1.00          \n##          Pos Pred Value :  NaN          \n##          Neg Pred Value : 0.96          \n##              Prevalence : 0.04          \n##          Detection Rate : 0.00          \n##    Detection Prevalence : 0.00          \n##       Balanced Accuracy : 0.50          \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"option-2-balance-data-with-resampling","chapter":"4 Classification: Alternative Techniques","heading":"4.11.2 Option 2: Balance Data With Resampling","text":"use stratified sampling replacement (oversample \nminority/positive class). also use SMOTE (package DMwR)\nsampling strategies (e.g., package unbalanced). use\n50+50 observations (Note: many samples chosen several\ntimes).Check unbalanced testing data.Note accuracy information rate! However,\nkappa (improvement accuracy randomness) sensitivity (\nability identify reptiles) increased.tradeoff sensitivity specificity (many \nidentified animals really reptiles) tradeoff can controlled\nusing sample proportions. can sample reptiles increase\nsensitivity cost lower specificity (effect seen\ndata since test set reptiles).","code":"\nlibrary(sampling)\nset.seed(1000) # for repeatability\n\nid <- strata(training_reptile, stratanames = \"type\", \n             size = c(50, 50), method = \"srswr\")\ntraining_reptile_balanced <- training_reptile |> \n  slice(id$ID_unit)\ntable(training_reptile_balanced$type)\n## \n## nonreptile    reptile \n##         50         50\n\nfit <- training_reptile_balanced |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"),\n        control = rpart.control(minsplit = 5))\n\nfit\n## CART \n## \n## 100 samples\n##  16 predictor\n##   2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 90, 90, 90, 90, 90, 90, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa\n##   0.18  0.81      0.62 \n##   0.30  0.63      0.26 \n##   0.34  0.53      0.06 \n## \n## Accuracy was used to select the optimal model using the\n##  largest value.\n## The final value used for the model was cp = 0.18.\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         19       0\n##   reptile            29       2\n##                                         \n##                Accuracy : 0.42          \n##                  95% CI : (0.282, 0.568)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1             \n##                                         \n##                   Kappa : 0.05          \n##                                         \n##  Mcnemar's Test P-Value : 2e-07         \n##                                         \n##             Sensitivity : 1.0000        \n##             Specificity : 0.3958        \n##          Pos Pred Value : 0.0645        \n##          Neg Pred Value : 1.0000        \n##              Prevalence : 0.0400        \n##          Detection Rate : 0.0400        \n##    Detection Prevalence : 0.6200        \n##       Balanced Accuracy : 0.6979        \n##                                         \n##        'Positive' Class : reptile       \n## \nid <- strata(training_reptile, stratanames = \"type\", \n             size = c(50, 100), method = \"srswr\")\ntraining_reptile_balanced <- training_reptile |> \n  slice(id$ID_unit)\ntable(training_reptile_balanced$type)\n## \n## nonreptile    reptile \n##         50        100\n\nfit <- training_reptile_balanced |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"),\n        control = rpart.control(minsplit = 5))\n\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         33       0\n##   reptile            15       2\n##                                         \n##                Accuracy : 0.7           \n##                  95% CI : (0.554, 0.821)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1.000000      \n##                                         \n##                   Kappa : 0.15          \n##                                         \n##  Mcnemar's Test P-Value : 0.000301      \n##                                         \n##             Sensitivity : 1.000         \n##             Specificity : 0.688         \n##          Pos Pred Value : 0.118         \n##          Neg Pred Value : 1.000         \n##              Prevalence : 0.040         \n##          Detection Rate : 0.040         \n##    Detection Prevalence : 0.340         \n##       Balanced Accuracy : 0.844         \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"option-3-build-a-larger-tree-and-use-predicted-probabilities","chapter":"4 Classification: Alternative Techniques","heading":"4.11.3 Option 3: Build A Larger Tree and use Predicted Probabilities","text":"Increase complexity require less data splitting node. \nalso use AUC (area ROC) tuning metric. need \nspecify two class summary function. Note tree still trying\nimprove accuracy data AUC! also enable class\nprobabilities since want predict probabilities later.Note: Accuracy high, close \n-information rate!","code":"\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        tuneLength = 10,\n        trControl = trainControl(method = \"cv\",\n        classProbs = TRUE,  ## necessary for predict with type=\"prob\"\n        summaryFunction=twoClassSummary),  ## necessary for ROC\n        metric = \"ROC\",\n        control = rpart.control(minsplit = 3))\n## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info =\n## trainInfo, : There were missing values in resampled performance\n## measures.\nfit\n## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 47, 46, 46, 46, 45, ... \n## Resampling results:\n## \n##   ROC    Sens   Spec\n##   0.358  0.975  0   \n## \n## Tuning parameter 'cp' was held constant at a value of 0\n\nrpart.plot(fit$finalModel, extra = 2)\n\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         48       2\n##   reptile             0       0\n##                                         \n##                Accuracy : 0.96          \n##                  95% CI : (0.863, 0.995)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.677         \n##                                         \n##                   Kappa : 0             \n##                                         \n##  Mcnemar's Test P-Value : 0.480         \n##                                         \n##             Sensitivity : 0.00          \n##             Specificity : 1.00          \n##          Pos Pred Value :  NaN          \n##          Neg Pred Value : 0.96          \n##              Prevalence : 0.04          \n##          Detection Rate : 0.00          \n##    Detection Prevalence : 0.00          \n##       Balanced Accuracy : 0.50          \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"create-a-biased-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.11.3.1 Create A Biased Classifier","text":"can create classifier detect reptiles \nexpense misclassifying non-reptiles. equivalent increasing\ncost misclassifying reptile non-reptile. usual rule \npredict node majority class test data \nnode. binary classification problem means probability \n>50%. following, reduce threshold 1% . \nmeans new observation ends leaf node 1% \nreptiles training observation classified \nreptile. data set small works better data.Note accuracy goes information rate.\nHowever, measures based idea errors \ncost. important now able find \nreptiles.","code":"\nprob <- predict(fit, testing_reptile, type = \"prob\")\ntail(prob)\n##      nonreptile reptile\n## tuna      1.000  0.0000\n## vole      0.962  0.0385\n## wasp      0.500  0.5000\n## wolf      0.962  0.0385\n## worm      1.000  0.0000\n## wren      0.962  0.0385\npred <- ifelse(prob[,\"reptile\"]>=0.01, \"reptile\", \"nonreptile\") |> \n  as.factor()\n\nconfusionMatrix(data = pred,\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         13       0\n##   reptile            35       2\n##                                         \n##                Accuracy : 0.3           \n##                  95% CI : (0.179, 0.446)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1             \n##                                         \n##                   Kappa : 0.029         \n##                                         \n##  Mcnemar's Test P-Value : 9.08e-09      \n##                                         \n##             Sensitivity : 1.0000        \n##             Specificity : 0.2708        \n##          Pos Pred Value : 0.0541        \n##          Neg Pred Value : 1.0000        \n##              Prevalence : 0.0400        \n##          Detection Rate : 0.0400        \n##    Detection Prevalence : 0.7400        \n##       Balanced Accuracy : 0.6354        \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"plot-the-roc-curve","chapter":"4 Classification: Alternative Techniques","heading":"4.11.3.2 Plot the ROC Curve","text":"Since binary classification problem classifier \npredicts probability observation reptile, can also\nuse receiver operating characteristic\n(ROC)\ncurve. ROC curve different cutoff thresholds \nprobability used connected line. area \ncurve represents single number well classifier works (\ncloser one, better).","code":"\nlibrary(\"pROC\")\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\nr <- roc(testing_reptile$type == \"reptile\", prob[,\"reptile\"])\n## Setting levels: control = FALSE, case = TRUE\n## Setting direction: controls < cases\nr\n## \n## Call:\n## roc.default(response = testing_reptile$type == \"reptile\", predictor = prob[,     \"reptile\"])\n## \n## Data: prob[, \"reptile\"] in 48 controls (testing_reptile$type == \"reptile\" FALSE) < 2 cases (testing_reptile$type == \"reptile\" TRUE).\n## Area under the curve: 0.766\n\nggroc(r) + geom_abline(intercept = 1, slope = 1, color = \"darkgrey\")"},{"path":"classification-alternative-techniques.html","id":"option-4-use-a-cost-sensitive-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.11.4 Option 4: Use a Cost-Sensitive Classifier","text":"implementation CART rpart can use cost matrix making\nsplitting decisions (parameter loss). matrix formTP FP FN TNTP TN 0. make FN expensive (100).warning “missing values resampled performance\nmeasures” means folds contain reptiles (\nclass imbalance) thus performance measures \ncalculates.high cost false negatives results classifier \nmiss reptile.Note: Using cost-sensitive classifier often best option.\nUnfortunately, classification algorithms (\nimplementation) ability consider misclassification\ncost.","code":"\ncost <- matrix(c(\n  0,   1,\n  100, 0\n), byrow = TRUE, nrow = 2)\ncost\n##      [,1] [,2]\n## [1,]    0    1\n## [2,]  100    0\n\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        parms = list(loss = cost),\n        trControl = trainControl(method = \"cv\"))\nfit\n## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 46, 46, 45, 46, 45, ... \n## Resampling results:\n## \n##   Accuracy  Kappa  \n##   0.477     -0.0304\n## \n## Tuning parameter 'cp' was held constant at a value of 0\n\nrpart.plot(fit$finalModel, extra = 2)\n\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         39       0\n##   reptile             9       2\n##                                         \n##                Accuracy : 0.82          \n##                  95% CI : (0.686, 0.914)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.99998       \n##                                         \n##                   Kappa : 0.257         \n##                                         \n##  Mcnemar's Test P-Value : 0.00766       \n##                                         \n##             Sensitivity : 1.000         \n##             Specificity : 0.812         \n##          Pos Pred Value : 0.182         \n##          Neg Pred Value : 1.000         \n##              Prevalence : 0.040         \n##          Detection Rate : 0.040         \n##    Detection Prevalence : 0.220         \n##       Balanced Accuracy : 0.906         \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"model-comparison-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12 Model Comparison","text":"first create weak baseline model always predicts majority\nclass mammal.kappa 0 clearly indicates baseline model power.collect performance metrics models trained \ndata.summary statistics shows performance. see \nmethods well easy data set, baseline model performs\nexpected poorly.Perform inference differences models. metric, \npair-wise differences computed tested assess \ndifference equal zero. default Bonferroni correction \nmultiple comparison used. Differences shown upper triangle\np-values lower triangle.perform similarly well except baseline model (differences first row\nnegative p-values first column <.05 indicating\nnull-hypothesis difference 0 can rejected).models similarly well data. choose random\nforest model evaluate generalization performance held-\ntest set.Calculate confusion matrix held-test data.","code":"\nbaselineFit <- Zoo_train |> train(type ~ .,\n  method = basemodels::dummyClassifier,\n  data = _,\n  strategy = \"constant\",\n  constant = \"mammal\",\n  trControl = trainControl(method = \"cv\", index = train_index))\nbaselineFit\n## dummyClassifier \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 75, 75, 75, 74, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.401     0\nresamps <- resamples(list(\n  baseline = baselineFit,\n  SVM = svmFit,\n  KNN = knnFit,\n  rules = rulesFit,\n  randomForest = randomForestFit,\n  xgboost = xgboostFit,\n  NeuralNet = nnetFit\n    ))\nresamps\n## \n## Call:\n## resamples.default(x = list(baseline = baselineFit, SVM = svmFit,\n##  KNN = knnFit, rules = rulesFit, randomForest =\n##  randomForestFit, xgboost = xgboostFit, NeuralNet = nnetFit))\n## \n## Models: baseline, SVM, KNN, rules, randomForest, xgboost, NeuralNet \n## Number of resamples: 10 \n## Performance metrics: Accuracy, Kappa \n## Time estimates for: everything, final model fit\nsummary(resamps)\n## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: baseline, SVM, KNN, rules, randomForest, xgboost, NeuralNet \n## Number of resamples: 10 \n## \n## Accuracy \n##               Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n## baseline     0.333   0.375  0.375 0.401   0.433  0.5    0\n## SVM          0.778   0.889  1.000 0.936   1.000  1.0    0\n## KNN          0.778   0.878  1.000 0.934   1.000  1.0    0\n## rules        0.778   0.925  1.000 0.957   1.000  1.0    0\n## randomForest 0.800   0.889  1.000 0.947   1.000  1.0    0\n## xgboost      0.778   0.892  1.000 0.944   1.000  1.0    0\n## NeuralNet    0.778   0.892  1.000 0.946   1.000  1.0    0\n## \n## Kappa \n##               Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n## baseline     0.000   0.000      0 0.000       0    0    0\n## SVM          0.719   0.853      1 0.917       1    1    0\n## KNN          0.714   0.846      1 0.916       1    1    0\n## rules        0.719   0.903      1 0.944       1    1    0\n## randomForest 0.740   0.860      1 0.931       1    1    0\n## xgboost      0.714   0.849      1 0.927       1    1    0\n## NeuralNet    0.714   0.864      1 0.929       1    1    0\n\nlibrary(lattice)\nbwplot(resamps, layout = c(3, 1))\ndifs <- diff(resamps)\nsummary(difs)\n## \n## Call:\n## summary.diff.resamples(object = difs)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## Accuracy \n##              baseline SVM      KNN      rules    randomForest\n## baseline              -0.53444 -0.53306 -0.55556 -0.54556    \n## SVM          2.76e-07           0.00139 -0.02111 -0.01111    \n## KNN          9.97e-08 1                 -0.02250 -0.01250    \n## rules        1.48e-07 1        1                  0.01000    \n## randomForest 1.26e-07 1        1        1                    \n## xgboost      1.44e-07 1        1        1        1           \n## NeuralNet    6.35e-08 1        1        1        1           \n##              xgboost  NeuralNet\n## baseline     -0.54306 -0.54444 \n## SVM          -0.00861 -0.01000 \n## KNN          -0.01000 -0.01139 \n## rules         0.01250  0.01111 \n## randomForest  0.00250  0.00111 \n## xgboost               -0.00139 \n## NeuralNet    1                 \n## \n## Kappa \n##              baseline SVM      KNN      rules    randomForest\n## baseline              -0.91726 -0.91582 -0.94389 -0.93133    \n## SVM          2.46e-08           0.00145 -0.02662 -0.01406    \n## KNN          2.81e-08 1                 -0.02807 -0.01551    \n## rules        4.69e-09 1        1                  0.01256    \n## randomForest 3.88e-09 1        1        1                    \n## xgboost      8.27e-09 1        1        1        1           \n## NeuralNet    6.49e-09 1        1        1        1           \n##              xgboost  NeuralNet\n## baseline     -0.92665 -0.92946 \n## SVM          -0.00939 -0.01219 \n## KNN          -0.01083 -0.01364 \n## rules         0.01724  0.01443 \n## randomForest  0.00468  0.00187 \n## xgboost               -0.00281 \n## NeuralNet    1\npr <- predict(randomForestFit, Zoo_test)\npr\n##  [1] mammal        fish          mollusc.et.al fish         \n##  [5] mammal        insect        mammal        mammal       \n##  [9] mammal        mammal        bird          mammal       \n## [13] mammal        bird          reptile       bird         \n## [17] mollusc.et.al bird         \n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al\nconfusionMatrix(pr, reference = Zoo_test$type)\n## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian insect\n##   mammal             8    0       0    0         0      0\n##   bird               0    4       0    0         0      0\n##   reptile            0    0       1    0         0      0\n##   fish               0    0       0    2         0      0\n##   amphibian          0    0       0    0         0      0\n##   insect             0    0       0    0         0      1\n##   mollusc.et.al      0    0       0    0         0      0\n##                Reference\n## Prediction      mollusc.et.al\n##   mammal                    0\n##   bird                      0\n##   reptile                   0\n##   fish                      0\n##   amphibian                 0\n##   insect                    0\n##   mollusc.et.al             2\n## \n## Overall Statistics\n##                                     \n##                Accuracy : 1         \n##                  95% CI : (0.815, 1)\n##     No Information Rate : 0.444     \n##     P-Value [Acc > NIR] : 4.58e-07  \n##                                     \n##                   Kappa : 1         \n##                                     \n##  Mcnemar's Test P-Value : NA        \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird Class: reptile\n## Sensitivity                  1.000       1.000         1.0000\n## Specificity                  1.000       1.000         1.0000\n## Pos Pred Value               1.000       1.000         1.0000\n## Neg Pred Value               1.000       1.000         1.0000\n## Prevalence                   0.444       0.222         0.0556\n## Detection Rate               0.444       0.222         0.0556\n## Detection Prevalence         0.444       0.222         0.0556\n## Balanced Accuracy            1.000       1.000         1.0000\n##                      Class: fish Class: amphibian Class: insect\n## Sensitivity                1.000               NA        1.0000\n## Specificity                1.000                1        1.0000\n## Pos Pred Value             1.000               NA        1.0000\n## Neg Pred Value             1.000               NA        1.0000\n## Prevalence                 0.111                0        0.0556\n## Detection Rate             0.111                0        0.0556\n## Detection Prevalence       0.111                0        0.0556\n## Balanced Accuracy          1.000               NA        1.0000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-alternative-techniques.html","id":"comparing-decision-boundaries-of-popular-classification-techniques","chapter":"4 Classification: Alternative Techniques","heading":"4.13 Comparing Decision Boundaries of Popular Classification Techniques*","text":"Classifiers create decision boundaries discriminate classes.\nDifferent classifiers able create different shapes decision\nboundaries (e.g., strictly linear) thus classifiers\nmay perform better certain datasets. page visualizes \ndecision boundaries found several popular classification methods.following plot adds decision boundary (black lines) \nclassification confidence (color intensity) evaluating classifier\nevenly spaced grid points. Note low resolution (make\nevaluation faster) make decision boundary look like \nsmall steps even (straight) line.","code":"\nlibrary(scales)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\n\ndecisionplot <- function(model, data, class_var, \n  predict_type = c(\"class\", \"prob\"), resolution = 3 * 72) {\n  # resolution is set to 72 dpi if the image is rendered  3 inches wide. \n  \n  y <- data |> pull(class_var)\n  x <- data |> dplyr::select(-all_of(class_var))\n  \n  # resubstitution accuracy\n  prediction <- predict(model, x, type = predict_type[1])\n  # LDA returns a list\n  if(is.list(prediction)) prediction <- prediction$class\n  prediction <- factor(prediction, levels = levels(y))\n  \n  cm <- confusionMatrix(data = prediction, \n                        reference = y)\n  acc <- cm$overall[\"Accuracy\"]\n  \n  # evaluate model on a grid\n  r <- sapply(x[, 1:2], range, na.rm = TRUE)\n  xs <- seq(r[1,1], r[2,1], length.out = resolution)\n  ys <- seq(r[1,2], r[2,2], length.out = resolution)\n  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))\n  colnames(g) <- colnames(r)\n  g <- as_tibble(g)\n  \n  ### guess how to get class labels from predict\n  ### (unfortunately not very consistent between models)\n  cl <- predict(model, g, type = predict_type[1])\n  \n  # LDA returns a list\n  prob <- NULL\n  if(is.list(cl)) { \n    prob <- cl$posterior\n    cl <- cl$class\n  } else\n    if(!is.na(predict_type[2]))\n      try(prob <- predict(model, g, type = predict_type[2]))\n  \n  # we visualize the difference in probability/score between the \n  # winning class and the second best class.\n  # don't use probability if predict for the classifier does not support it.\n  max_prob <- 1\n  if(!is.null(prob))\n    try({\n      max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))\n      max_prob <- max_prob[,1] - max_prob[,2]\n    }, silent = TRUE) \n  \n  cl <- factor(cl, levels = levels(y))\n  \n  g <- g |> add_column(prediction = cl, probability = max_prob)\n  \n  ggplot(g, mapping = aes(\n    x = .data[[colnames(g)[1]]], y = .data[[colnames(g)[2]]])) +\n    geom_raster(mapping = aes(fill = prediction, alpha = probability)) +\n    geom_contour(mapping = aes(z = as.numeric(prediction)), \n      bins = length(levels(cl)), linewidth = .5, color = \"black\") +\n    geom_point(data = data, mapping =  aes(\n      x = .data[[colnames(data)[1]]], \n      y = .data[[colnames(data)[2]]],\n      shape = .data[[class_var]]), alpha = .7) + \n    scale_alpha_continuous(range = c(0,1), limits = c(0,1), guide = \"none\") +  \n    labs(subtitle = paste(\"Training accuracy:\", round(acc, 2)))\n}"},{"path":"classification-alternative-techniques.html","id":"iris-dataset","chapter":"4 Classification: Alternative Techniques","heading":"4.13.1 Iris Dataset","text":"easier visualization, use two dimensions Iris dataset.\nContour lines visualize density like mountains map.Note: overplotting use geom_jitter()\ninstead geom_point().","code":"\nset.seed(1000)\ndata(iris)\niris <- as_tibble(iris)\n\n### Three classes \n### (note: MASS also has a select function which hides dplyr's select)\nx <- iris |> dplyr::select(Sepal.Length, Sepal.Width, Species)\nx\n## # A tibble: 150 × 3\n##    Sepal.Length Sepal.Width Species\n##           <dbl>       <dbl> <fct>  \n##  1          5.1         3.5 setosa \n##  2          4.9         3   setosa \n##  3          4.7         3.2 setosa \n##  4          4.6         3.1 setosa \n##  5          5           3.6 setosa \n##  6          5.4         3.9 setosa \n##  7          4.6         3.4 setosa \n##  8          5           3.4 setosa \n##  9          4.4         2.9 setosa \n## 10          4.9         3.1 setosa \n## # ℹ 140 more rows\n\nggplot(x, aes(x = Sepal.Length, y = Sepal.Width, fill = Species)) +  \n  stat_density_2d(geom = \"polygon\", aes(alpha = after_stat(level))) +\n  geom_point()"},{"path":"classification-alternative-techniques.html","id":"nearest-neighbor-classifier-1","chapter":"4 Classification: Alternative Techniques","heading":"4.13.1.1 Nearest Neighbor Classifier","text":"Increasing \\(k\\) smooths decision boundary. \\(k=1\\), see white\nareas around points flowers two classes spot.\n, algorithm randomly chooses class prediction resulting\nmeandering decision boundary. predictions area \nstable every time ask class, may get different\nclass.","code":"\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 1)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (1 neighbor)\")\n\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 3)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (3 neighbor)\")\n\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 9)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (9 neighbor)\")"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier-1","chapter":"4 Classification: Alternative Techniques","heading":"4.13.1.2 Naive Bayes Classifier","text":"","code":"\nmodel <- x |> e1071::naiveBayes(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"Naive Bayes\")"},{"path":"classification-alternative-techniques.html","id":"linear-discriminant-analysis","chapter":"4 Classification: Alternative Techniques","heading":"4.13.1.3 Linear Discriminant Analysis","text":"","code":"\nmodel <- x |> MASS::lda(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"LDA\")"},{"path":"classification-alternative-techniques.html","id":"multinomial-logistic-regression-implemented-in-nnet","chapter":"4 Classification: Alternative Techniques","heading":"4.13.1.4 Multinomial Logistic Regression (implemented in nnet)","text":"Multinomial logistic regression extension logistic regression\nproblems two classes.","code":"\nmodel <- x |> nnet::multinom(Species ~., data = _)\n## # weights:  12 (6 variable)\n## initial  value 164.791843 \n## iter  10 value 62.715967\n## iter  20 value 59.808291\n## iter  30 value 55.445984\n## iter  40 value 55.375704\n## iter  50 value 55.346472\n## iter  60 value 55.301707\n## iter  70 value 55.253532\n## iter  80 value 55.243230\n## iter  90 value 55.230241\n## iter 100 value 55.212479\n## final  value 55.212479 \n## stopped after 100 iterations\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(titel = \"Multinomial Logistic Regression\")"},{"path":"classification-alternative-techniques.html","id":"decision-trees","chapter":"4 Classification: Alternative Techniques","heading":"4.13.1.5 Decision Trees","text":"","code":"\nmodel <- x |> rpart::rpart(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"CART\")\n\nmodel <- x |> rpart::rpart(Species ~ ., data = _,\n  control = rpart::rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"CART (overfitting)\")\n\nmodel <- x |> C50::C5.0(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"C5.0\")\n\nmodel <- x |> randomForest::randomForest(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"Random Forest\")"},{"path":"classification-alternative-techniques.html","id":"svm","chapter":"4 Classification: Alternative Techniques","heading":"4.13.1.6 SVM","text":"","code":"\nmodel <- x |> e1071::svm(Species ~ ., data = _, kernel = \"linear\")\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (linear kernel)\")\n\nmodel <- x |> e1071::svm(Species ~ ., data = _, kernel = \"radial\")\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (radial kernel)\")\n\nmodel <- x |> e1071::svm(Species ~ ., data = _, kernel = \"polynomial\")\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (polynomial kernel)\")\n\nmodel <- x |> e1071::svm(Species ~ ., data = _, kernel = \"sigmoid\")\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (sigmoid kernel)\")"},{"path":"classification-alternative-techniques.html","id":"single-layer-feed-forward-neural-networks","chapter":"4 Classification: Alternative Techniques","heading":"4.13.1.7 Single Layer Feed-forward Neural Networks","text":"","code":"\nmodel <-x |> nnet::nnet(Species ~ ., data = _, size = 1, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (1 neuron)\")\n\nmodel <-x |> nnet::nnet(Species ~ ., data = _, size = 2, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (2 neurons)\")\n\nmodel <-x |> nnet::nnet(Species ~ ., data = _, size = 4, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (4 neurons)\")\n\nmodel <-x |> nnet::nnet(Species ~ ., data = _, size = 10, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"NN (10 neurons)\")"},{"path":"classification-alternative-techniques.html","id":"circle-dataset","chapter":"4 Classification: Alternative Techniques","heading":"4.13.2 Circle Dataset","text":"set linearly separable!","code":"\nset.seed(1000)\n\nx <- mlbench::mlbench.circle(500)\n###x <- mlbench::mlbench.cassini(500)\n###x <- mlbench::mlbench.spirals(500, sd = .1)\n###x <- mlbench::mlbench.smiley(500)\nx <- cbind(as.data.frame(x$x), factor(x$classes))\ncolnames(x) <- c(\"x\", \"y\", \"class\")\nx <- as_tibble(x)\nx\n## # A tibble: 500 × 3\n##          x       y class\n##      <dbl>   <dbl> <fct>\n##  1 -0.344   0.448  1    \n##  2  0.518   0.915  2    \n##  3 -0.772  -0.0913 1    \n##  4  0.382   0.412  1    \n##  5  0.0328  0.438  1    \n##  6 -0.865  -0.354  2    \n##  7  0.477   0.640  2    \n##  8  0.167  -0.809  2    \n##  9 -0.568  -0.281  1    \n## 10 -0.488   0.638  2    \n## # ℹ 490 more rows\n\nggplot(x, aes(x = x, y = y, color = class)) + \n  geom_point()"},{"path":"classification-alternative-techniques.html","id":"nearest-neighbor-classifier-2","chapter":"4 Classification: Alternative Techniques","heading":"4.13.2.1 Nearest Neighbor Classifier","text":"","code":"\nmodel <- x |> caret::knn3(class ~ ., data = _, k = 1)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"kNN (1 neighbor)\")\n\nmodel <- x |> caret::knn3(class ~ ., data = _, k = 10)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"kNN (10 neighbor)\")"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier-2","chapter":"4 Classification: Alternative Techniques","heading":"4.13.2.2 Naive Bayes Classifier","text":"","code":"\nmodel <- x |> e1071::naiveBayes(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"naive Bayes\")"},{"path":"classification-alternative-techniques.html","id":"linear-discriminant-analysis-1","chapter":"4 Classification: Alternative Techniques","heading":"4.13.2.3 Linear Discriminant Analysis","text":"LDA find good model since true decision boundary \nlinear.","code":"\nmodel <- x |> MASS::lda(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + labs(title = \"LDA\")"},{"path":"classification-alternative-techniques.html","id":"logistic-regression-implemented-in-nnet","chapter":"4 Classification: Alternative Techniques","heading":"4.13.2.4 Logistic Regression (implemented in nnet)","text":"Multinomial logistic regression extension logistic regression\nproblems two classes. also tries find linear\ndecision boundary.","code":"\nmodel <- x |> nnet::multinom(class ~., data = _)\n## # weights:  4 (3 variable)\n## initial  value 346.573590 \n## final  value 346.308371 \n## converged\ndecisionplot(model, x, class_var = \"class\") + \n  labs(titel = \"Multinomial Logistic Regression\")"},{"path":"classification-alternative-techniques.html","id":"decision-trees-1","chapter":"4 Classification: Alternative Techniques","heading":"4.13.2.5 Decision Trees","text":"","code":"\nmodel <- x |> rpart::rpart(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"CART\")\n\nmodel <- x |> rpart::rpart(class ~ ., data = _,\n  control = rpart::rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"CART (overfitting)\")\n\nmodel <- x |> C50::C5.0(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"C5.0\")\n\nlibrary(randomForest)\n## randomForest 4.7-1.1\n## Type rfNews() to see new features/changes/bug fixes.\n## \n## Attaching package: 'randomForest'\n## The following object is masked from 'package:dplyr':\n## \n##     combine\n## The following object is masked from 'package:ggplot2':\n## \n##     margin\nmodel <- x |> randomForest(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"Random Forest\")"},{"path":"classification-alternative-techniques.html","id":"svm-1","chapter":"4 Classification: Alternative Techniques","heading":"4.13.2.6 SVM","text":"Linear SVM work data.","code":"\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"linear\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (linear kernel)\")\n## Warning: Computation failed in `stat_contour()`.\n## Caused by error in `if (zero_range(range)) ...`:\n## ! missing value where TRUE/FALSE needed\n\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"radial\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (radial kernel)\")\n\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"polynomial\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (polynomial kernel)\")\n\nmodel <- x |> e1071::svm(class ~ ., data = _, kernel = \"sigmoid\")\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (sigmoid kernel)\")"},{"path":"classification-alternative-techniques.html","id":"single-layer-feed-forward-neural-networks-1","chapter":"4 Classification: Alternative Techniques","heading":"4.13.2.7 Single Layer Feed-forward Neural Networks","text":"","code":"\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 1, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + labs(title = \"NN (1 neuron)\")\n\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 2, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + labs(title = \"NN (2 neurons)\")\n\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 4, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + labs(title = \"NN (4 neurons)\")\n\nmodel <-x |> nnet::nnet(class ~ ., data = _, size = 10, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\")) + labs(title = \"NN (10 neurons)\")"},{"path":"classification-alternative-techniques.html","id":"more-information-on-classification-with-r","chapter":"4 Classification: Alternative Techniques","heading":"4.14 More Information on Classification with R","text":"Package caret: http://topepo.github.io/caret/index.htmlTidymodels (machine learning tidyverse):\nhttps://www.tidymodels.org/R taskview machine learning:\nhttp://cran.r-project.org/web/views/MachineLearning.html","code":""},{"path":"classification-alternative-techniques.html","id":"exercises-2","chapter":"4 Classification: Alternative Techniques","heading":"4.15 Exercises*","text":"use Palmer penguin data exercises.Create R markdown file code following .Apply least 3 different classification models data.Compare models simple baseline model. model\nperforms best? perform significantly better \nmodels?","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm\n##   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n## 1 Adelie  Torgersen           39.1          18.7               181\n## 2 Adelie  Torgersen           39.5          17.4               186\n## 3 Adelie  Torgersen           40.3          18                 195\n## 4 Adelie  Torgersen           NA            NA                  NA\n## 5 Adelie  Torgersen           36.7          19.3               193\n## 6 Adelie  Torgersen           39.3          20.6               190\n## # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"association-analysis-basic-concepts-and-algorithms","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5 Association Analysis: Basic Concepts and Algorithms","text":"Install packages used chapter:packages used chapter : arules (Hahsler et al. 2023), arulesViz (Hahsler 2024), mlbench (Leisch Dimitriadou 2024), palmerpenguins (Horst, Hill, Gorman 2022), tidyverse (Wickham 2023c)","code":"\npkgs <- c(\"arules\", \"arulesViz\", \"mlbench\", \"palmerpenguins\", \"tidyverse\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"preliminaries","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.1 Preliminaries","text":"Association rule mining\nplays vital role discovering hidden patterns relationships within large\ntransactional datasets. Applications range exploratory data analysis marketing building rule-based\nclassifiers.\nAgrawal, Imielinski, Swami (1993) introduced problem\nmining association rules transaction data follows (definition taken Hahsler, Grün, Hornik (2005)):Let \\(= \\{i_1,i_2,...,i_n\\}\\) set \\(n\\) binary attributes called items. Let \\(D = \\{t_1,t_2,...,t_m\\}\\) \nset transactions called database. transaction \\(D\\) unique transaction ID \ncontains subset items \\(\\). rule defined implication form \\(X \\Rightarrow Y\\) \n\\(X,Y \\subseteq \\) \\(X \\cap Y = \\emptyset\\) called itemsets. itemsets rules several quality measures can\ndefined. important measures support confidence. support \\(supp(X)\\) \nitemset \\(X\\) defined proportion transactions data set contain itemset.\nItemsets support surpasses user-defined threshold \\(\\sigma\\) called frequent itemsets. \nconfidence rule defined \\(conf(X \\Rightarrow Y) = supp(X \\cup Y)/supp(X)\\). Association rules rules\n\\(supp(X \\cup Y) \\ge \\sigma\\) \\(conf(X) \\ge \\delta\\) \\(\\sigma\\) \\(\\delta\\) user-defined thresholds.\nfound set association rules used reason data.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 5. Association Analysis: Basic Concepts \nAlgorithms","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"the-arules-package","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.1.1 The arules Package","text":"Association rule mining R implemented package arules.information arules package try: help(package=\"arules\")\nvignette(\"arules\") (also available \nCRAN)arules uses S4 object system implement classes methods.\nStandard R objects use S3 object\nsystem use formal class\ndefinitions usually implemented list class\nattribute. arules many R packages use S4 object\nsystem based formal class\ndefinitions member variables methods (similar \nobject-oriented programming languages like Java C++). important\ndifferences using S4 objects compared usual S3 objects :coercion (casting): (, \"class_name\")help classes: class? class_name","code":"\nlibrary(tidyverse)\nlibrary(arules)\nlibrary(arulesViz)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.1.2 Transactions","text":"","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"create-transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.1.2.1 Create Transactions","text":"use Zoo dataset mlbench.data data.frame need converted set \ntransactions row represents transaction column \ntranslated items. done using constructor\ntransactions(). Zoo data set means consider\nanimals transactions different traits (features) become\nitems animal . example animal antelope \nitem hair transaction.conversion gives warning discrete features (factor\nlogical) can directly translated items. Continuous\nfeatures need discretized first.column 13?Possible solution: Make legs /legsAlternatives:use unique value item:discretize (see\n? discretize\ndiscretization code Chapter\n2):Convert data set transactions","code":"\ndata(Zoo, package = \"mlbench\")\nhead(Zoo)\n##           hair feathers  eggs  milk airborne aquatic predator toothed\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE     TRUE    TRUE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE    FALSE    TRUE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE     TRUE    TRUE\n## bear      TRUE    FALSE FALSE  TRUE    FALSE   FALSE     TRUE    TRUE\n## boar      TRUE    FALSE FALSE  TRUE    FALSE   FALSE     TRUE    TRUE\n## buffalo   TRUE    FALSE FALSE  TRUE    FALSE   FALSE    FALSE    TRUE\n##          backbone breathes venomous  fins legs  tail domestic catsize\n## aardvark     TRUE     TRUE    FALSE FALSE    4 FALSE    FALSE    TRUE\n## antelope     TRUE     TRUE    FALSE FALSE    4  TRUE    FALSE    TRUE\n## bass         TRUE    FALSE    FALSE  TRUE    0  TRUE    FALSE   FALSE\n## bear         TRUE     TRUE    FALSE FALSE    4 FALSE    FALSE    TRUE\n## boar         TRUE     TRUE    FALSE FALSE    4  TRUE    FALSE    TRUE\n## buffalo      TRUE     TRUE    FALSE FALSE    4  TRUE    FALSE    TRUE\n##            type\n## aardvark mammal\n## antelope mammal\n## bass       fish\n## bear     mammal\n## boar     mammal\n## buffalo  mammal\ntrans <- transactions(Zoo)\n## Warning: Column(s) 13 not logical or factor. Applying default\n## discretization (see '? discretizeDF').\nsummary(Zoo[13])\n##       legs     \n##  Min.   :0.00  \n##  1st Qu.:2.00  \n##  Median :4.00  \n##  Mean   :2.84  \n##  3rd Qu.:4.00  \n##  Max.   :8.00\nggplot(Zoo, aes(legs)) + geom_bar()\nZoo$legs |> table()\n## \n##  0  2  4  5  6  8 \n## 23 27 38  1 10  2\nZoo_has_legs <- Zoo |> mutate(legs = legs > 0)\nggplot(Zoo_has_legs, aes(legs)) + geom_bar()\nZoo_has_legs$legs |> table()\n## \n## FALSE  TRUE \n##    23    78\nZoo_unique_leg_values <- Zoo |> mutate(legs = factor(legs))\nZoo_unique_leg_values$legs |> head()\n## [1] 4 4 0 4 4 4\n## Levels: 0 2 4 5 6 8\nZoo_discretized_legs <- Zoo |> mutate(\n  legs = discretize(legs, breaks = 2, method=\"interval\")\n)\ntable(Zoo_discretized_legs$legs)\n## \n## [0,4) [4,8] \n##    50    51\ntrans <- transactions(Zoo_has_legs)\ntrans\n## transactions in sparse format with\n##  101 transactions (rows) and\n##  23 items (columns)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"inspect-transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.1.2.2 Inspect Transactions","text":"Look created items. still called column names since \ntransactions actually stored large sparse logical matrix (see\n).Compare original features (column names) ZooLook (first) transactions matrix. 1 indicates presence\nitem.Look transactions sets itemsPlot binary matrix. Dark dots represent 1s.Look relative frequency (=support) items data set. \nlook 10 frequent items.Alternative encoding: Also create items FALSE (use factor)","code":"\nsummary(trans)\n## transactions as itemMatrix in sparse format with\n##  101 rows (elements/itemsets/transactions) and\n##  23 columns (items) and a density of 0.3612 \n## \n## most frequent items:\n## backbone breathes     legs     tail  toothed  (Other) \n##       83       80       78       75       61      462 \n## \n## element (itemset/transaction) length distribution:\n## sizes\n##  3  4  5  6  7  8  9 10 11 12 \n##  3  2  6  5  8 21 27 25  3  1 \n## \n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    3.00    8.00    9.00    8.31   10.00   12.00 \n## \n## includes extended item information - examples:\n##     labels variables levels\n## 1     hair      hair   TRUE\n## 2 feathers  feathers   TRUE\n## 3     eggs      eggs   TRUE\n## \n## includes extended transaction information - examples:\n##   transactionID\n## 1      aardvark\n## 2      antelope\n## 3          bass\ncolnames(trans)\n##  [1] \"hair\"               \"feathers\"           \"eggs\"              \n##  [4] \"milk\"               \"airborne\"           \"aquatic\"           \n##  [7] \"predator\"           \"toothed\"            \"backbone\"          \n## [10] \"breathes\"           \"venomous\"           \"fins\"              \n## [13] \"legs\"               \"tail\"               \"domestic\"          \n## [16] \"catsize\"            \"type=mammal\"        \"type=bird\"         \n## [19] \"type=reptile\"       \"type=fish\"          \"type=amphibian\"    \n## [22] \"type=insect\"        \"type=mollusc.et.al\"\ncolnames(Zoo)\n##  [1] \"hair\"     \"feathers\" \"eggs\"     \"milk\"     \"airborne\" \"aquatic\" \n##  [7] \"predator\" \"toothed\"  \"backbone\" \"breathes\" \"venomous\" \"fins\"    \n## [13] \"legs\"     \"tail\"     \"domestic\" \"catsize\"  \"type\"\nas(trans, \"matrix\")[1:3,]\n##           hair feathers  eggs  milk airborne aquatic predator toothed\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE     TRUE    TRUE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE    FALSE    TRUE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE     TRUE    TRUE\n##          backbone breathes venomous  fins  legs  tail domestic\n## aardvark     TRUE     TRUE    FALSE FALSE  TRUE FALSE    FALSE\n## antelope     TRUE     TRUE    FALSE FALSE  TRUE  TRUE    FALSE\n## bass         TRUE    FALSE    FALSE  TRUE FALSE  TRUE    FALSE\n##          catsize type=mammal type=bird type=reptile type=fish\n## aardvark    TRUE        TRUE     FALSE        FALSE     FALSE\n## antelope    TRUE        TRUE     FALSE        FALSE     FALSE\n## bass       FALSE       FALSE     FALSE        FALSE      TRUE\n##          type=amphibian type=insect type=mollusc.et.al\n## aardvark          FALSE       FALSE              FALSE\n## antelope          FALSE       FALSE              FALSE\n## bass              FALSE       FALSE              FALSE\ninspect(trans[1:3])\n##     items         transactionID\n## [1] {hair,                     \n##      milk,                     \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      catsize,                  \n##      type=mammal}      aardvark\n## [2] {hair,                     \n##      milk,                     \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      tail,                     \n##      catsize,                  \n##      type=mammal}      antelope\n## [3] {eggs,                     \n##      aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      type=fish}        bass\nimage(trans)\nitemFrequencyPlot(trans,topN = 20)\n\nggplot(\n  tibble(\n    Support = sort(itemFrequency(trans, type = \"absolute\"), decreasing = TRUE),\n    Item = seq_len(ncol(trans))\n  ), aes(x = Item, y = Support)) + geom_line()\nsapply(Zoo_has_legs, class)\n##      hair  feathers      eggs      milk  airborne   aquatic  predator \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##   toothed  backbone  breathes  venomous      fins      legs      tail \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##  domestic   catsize      type \n## \"logical\" \"logical\"  \"factor\"\nZoo_factors <- Zoo_has_legs |> mutate(across(where(is.logical), factor))\nsapply(Zoo_factors, class)\n##     hair feathers     eggs     milk airborne  aquatic predator \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \n##  toothed backbone breathes venomous     fins     legs     tail \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \n## domestic  catsize     type \n## \"factor\" \"factor\" \"factor\"\nsummary(Zoo_factors)\n##     hair     feathers     eggs       milk     airborne   aquatic  \n##  FALSE:58   FALSE:81   FALSE:42   FALSE:60   FALSE:77   FALSE:65  \n##  TRUE :43   TRUE :20   TRUE :59   TRUE :41   TRUE :24   TRUE :36  \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##   predator   toothed    backbone   breathes   venomous     fins   \n##  FALSE:45   FALSE:40   FALSE:18   FALSE:21   FALSE:93   FALSE:84  \n##  TRUE :56   TRUE :61   TRUE :83   TRUE :80   TRUE : 8   TRUE :17  \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##     legs       tail     domestic   catsize              type   \n##  FALSE:23   FALSE:26   FALSE:88   FALSE:57   mammal       :41  \n##  TRUE :78   TRUE :75   TRUE :13   TRUE :44   bird         :20  \n##                                              reptile      : 5  \n##                                              fish         :13  \n##                                              amphibian    : 4  \n##                                              insect       : 8  \n##                                              mollusc.et.al:10\n\ntrans_factors <- transactions(Zoo_factors)\ntrans_factors\n## transactions in sparse format with\n##  101 transactions (rows) and\n##  39 items (columns)\n\nitemFrequencyPlot(trans_factors, topN = 20)\n\n## Select transactions that contain a certain item\ntrans_insects <- trans_factors[trans %in% \"type=insect\"]\ntrans_insects\n## transactions in sparse format with\n##  8 transactions (rows) and\n##  39 items (columns)\ninspect(trans_insects)\n##     items             transactionID\n## [1] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=FALSE,               \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          flea    \n## [2] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          gnat    \n## [3] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=TRUE,                \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=TRUE,                \n##      catsize=FALSE,                \n##      type=insect}          honeybee\n## [4] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          housefly\n## [5] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=TRUE,                \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          ladybird\n## [6] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          moth    \n## [7] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=FALSE,               \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          termite \n## [8] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=TRUE,                \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          wasp"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"vertical-layout-transaction-id-lists","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.1.2.3 Vertical Layout (Transaction ID Lists)","text":"default layout transactions horizontal layout (.e. \ntransaction row). vertical layout represents transaction data\nlist transaction IDs item (= transaction ID lists).","code":"\nvertical <- as(trans, \"tidLists\")\nas(vertical, \"matrix\")[1:10, 1:5]\n##          aardvark antelope  bass  bear  boar\n## hair         TRUE     TRUE FALSE  TRUE  TRUE\n## feathers    FALSE    FALSE FALSE FALSE FALSE\n## eggs        FALSE    FALSE  TRUE FALSE FALSE\n## milk         TRUE     TRUE FALSE  TRUE  TRUE\n## airborne    FALSE    FALSE FALSE FALSE FALSE\n## aquatic     FALSE    FALSE  TRUE FALSE FALSE\n## predator     TRUE    FALSE  TRUE  TRUE  TRUE\n## toothed      TRUE     TRUE  TRUE  TRUE  TRUE\n## backbone     TRUE     TRUE  TRUE  TRUE  TRUE\n## breathes     TRUE     TRUE FALSE  TRUE  TRUE"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"frequent-itemset-generation","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.2 Frequent Itemset Generation","text":"dataset already huge number possible itemsetsFind frequent itemsets (target=“frequent”) default settings.Default minimum support .1 (10%). Note: use small\ndata set. larger datasets default minimum support might \nlow may run memory. probably want start \nhigher minimum support like .5 (50%) work way .order find itemsets effect 5 animals need go \nsupport 5%.Sort supportLook frequent itemsets many items (set breaks manually since\nAutomatically chosen breaks look bad)","code":"\n2^ncol(trans)\n## [1] 8388608\nits <- apriori(trans, parameter=list(target = \"frequent\"))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support\n##          NA    0.1    1 none FALSE            TRUE       5     0.1\n##  minlen maxlen            target  ext\n##       1     10 frequent itemsets TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [18 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, parameter = list(target = \"frequent\")):\n## Mining stopped (maxlen reached). Only patterns up to a length of 10\n## returned!\n##  done [0.00s].\n## sorting transactions ... done [0.00s].\n## writing ... [1465 set(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nits\n## set of 1465 itemsets\n5/nrow(trans)\n## [1] 0.0495\nits <- apriori(trans, parameter=list(target = \"frequent\", support = 0.05))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support\n##          NA    0.1    1 none FALSE            TRUE       5    0.05\n##  minlen maxlen            target  ext\n##       1     10 frequent itemsets TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, parameter = list(target = \"frequent\",\n## support = 0.05)): Mining stopped (maxlen reached). Only patterns up\n## to a length of 10 returned!\n##  done [0.00s].\n## sorting transactions ... done [0.00s].\n## writing ... [2537 set(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nits\n## set of 2537 itemsets\nits <- sort(its, by = \"support\")\nits |> head(n = 10) |> inspect()\n##      items                      support count\n## [1]  {backbone}                 0.8218  83   \n## [2]  {breathes}                 0.7921  80   \n## [3]  {legs}                     0.7723  78   \n## [4]  {tail}                     0.7426  75   \n## [5]  {backbone, tail}           0.7327  74   \n## [6]  {breathes, legs}           0.7228  73   \n## [7]  {backbone, breathes}       0.6832  69   \n## [8]  {backbone, legs}           0.6337  64   \n## [9]  {backbone, breathes, legs} 0.6337  64   \n## [10] {toothed}                  0.6040  61\nggplot(tibble(`Itemset Size` = factor(size(its))), aes(`Itemset Size`)) + \n  geom_bar()\nits[size(its) > 8] |> inspect()\n##      items         support count\n## [1]  {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.23762    24\n## [2]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       catsize,                  \n##       type=mammal} 0.15842    16\n## [3]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       type=mammal} 0.14851    15\n## [4]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.13861    14\n## [5]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [6]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [7]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [8]  {milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [9]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize}     0.12871    13\n## [10] {hair,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [11] {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [12] {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       domestic,                 \n##       catsize,                  \n##       type=mammal} 0.05941     6\n## [13] {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       domestic,                 \n##       type=mammal} 0.05941     6\n## [14] {feathers,                 \n##       eggs,                     \n##       airborne,                 \n##       predator,                 \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       type=bird}   0.05941     6"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"rules-generation","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3 Rules Generation","text":"use APRIORI algorithm (see\n? apriori)Look rules highest liftCreate rules using alternative encoding (“FALSE” item)","code":"\nrules <- apriori(trans, parameter = list(support = 0.05, confidence = 0.9))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support\n##         0.9    0.1    1 none FALSE            TRUE       5    0.05\n##  minlen maxlen target  ext\n##       1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, parameter = list(support = 0.05, confidence\n## = 0.9)): Mining stopped (maxlen reached). Only patterns up to a\n## length of 10 returned!\n##  done [0.00s].\n## writing ... [7174 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nlength(rules)\n## [1] 7174\n\nrules |> head() |> inspect()\n##     lhs                     rhs        support confidence coverage\n## [1] {type=insect}        => {eggs}     0.07921 1.0        0.07921 \n## [2] {type=insect}        => {legs}     0.07921 1.0        0.07921 \n## [3] {type=insect}        => {breathes} 0.07921 1.0        0.07921 \n## [4] {type=mollusc.et.al} => {eggs}     0.08911 0.9        0.09901 \n## [5] {type=fish}          => {fins}     0.12871 1.0        0.12871 \n## [6] {type=fish}          => {aquatic}  0.12871 1.0        0.12871 \n##     lift  count\n## [1] 1.712  8   \n## [2] 1.295  8   \n## [3] 1.262  8   \n## [4] 1.541  9   \n## [5] 5.941 13   \n## [6] 2.806 13\nrules |> head() |> quality()\n##   support confidence coverage  lift count\n## 1 0.07921        1.0  0.07921 1.712     8\n## 2 0.07921        1.0  0.07921 1.295     8\n## 3 0.07921        1.0  0.07921 1.262     8\n## 4 0.08911        0.9  0.09901 1.541     9\n## 5 0.12871        1.0  0.12871 5.941    13\n## 6 0.12871        1.0  0.12871 2.806    13\nrules <- sort(rules, by = \"lift\")\nrules |> head(n = 10) |> inspect()\n##      lhs                                rhs         support\n## [1]  {eggs, fins}                    => {type=fish} 0.12871\n## [2]  {eggs, aquatic, fins}           => {type=fish} 0.12871\n## [3]  {eggs, predator, fins}          => {type=fish} 0.08911\n## [4]  {eggs, toothed, fins}           => {type=fish} 0.12871\n## [5]  {eggs, fins, tail}              => {type=fish} 0.12871\n## [6]  {eggs, backbone, fins}          => {type=fish} 0.12871\n## [7]  {eggs, aquatic, predator, fins} => {type=fish} 0.08911\n## [8]  {eggs, aquatic, toothed, fins}  => {type=fish} 0.12871\n## [9]  {eggs, aquatic, fins, tail}     => {type=fish} 0.12871\n## [10] {eggs, aquatic, backbone, fins} => {type=fish} 0.12871\n##      confidence coverage lift  count\n## [1]  1          0.12871  7.769 13   \n## [2]  1          0.12871  7.769 13   \n## [3]  1          0.08911  7.769  9   \n## [4]  1          0.12871  7.769 13   \n## [5]  1          0.12871  7.769 13   \n## [6]  1          0.12871  7.769 13   \n## [7]  1          0.08911  7.769  9   \n## [8]  1          0.12871  7.769 13   \n## [9]  1          0.12871  7.769 13   \n## [10] 1          0.12871  7.769 13\nr <- apriori(trans_factors)\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support\n##         0.8    0.1    1 none FALSE            TRUE       5     0.1\n##  minlen maxlen target  ext\n##       1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[39 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [34 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans_factors): Mining stopped (maxlen reached).\n## Only patterns up to a length of 10 returned!\n##  done [0.06s].\n## writing ... [1517191 rule(s)] done [0.17s].\n## creating S4 object  ... done [0.60s].\nr\n## set of 1517191 rules\nprint(object.size(r), unit = \"Mb\")\n## 110.2 Mb\n\ninspect(r[1:10])\n##      lhs                rhs              support confidence coverage\n## [1]  {}              => {feathers=FALSE} 0.8020  0.8020     1.0000  \n## [2]  {}              => {backbone=TRUE}  0.8218  0.8218     1.0000  \n## [3]  {}              => {fins=FALSE}     0.8317  0.8317     1.0000  \n## [4]  {}              => {domestic=FALSE} 0.8713  0.8713     1.0000  \n## [5]  {}              => {venomous=FALSE} 0.9208  0.9208     1.0000  \n## [6]  {domestic=TRUE} => {predator=FALSE} 0.1089  0.8462     0.1287  \n## [7]  {domestic=TRUE} => {aquatic=FALSE}  0.1188  0.9231     0.1287  \n## [8]  {domestic=TRUE} => {legs=TRUE}      0.1188  0.9231     0.1287  \n## [9]  {domestic=TRUE} => {breathes=TRUE}  0.1188  0.9231     0.1287  \n## [10] {domestic=TRUE} => {backbone=TRUE}  0.1188  0.9231     0.1287  \n##      lift  count\n## [1]  1.000 81   \n## [2]  1.000 83   \n## [3]  1.000 84   \n## [4]  1.000 88   \n## [5]  1.000 93   \n## [6]  1.899 11   \n## [7]  1.434 12   \n## [8]  1.195 12   \n## [9]  1.165 12   \n## [10] 1.123 12\nr |> head(n = 10, by = \"lift\") |> inspect()\n##      lhs                  rhs         support confidence coverage  lift count\n## [1]  {breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [2]  {eggs=TRUE,                                                             \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [3]  {milk=FALSE,                                                            \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [4]  {breathes=FALSE,                                                        \n##       fins=TRUE,                                                             \n##       legs=FALSE}      => {type=fish}  0.1287          1   0.1287 7.769    13\n## [5]  {aquatic=TRUE,                                                          \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [6]  {hair=FALSE,                                                            \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [7]  {eggs=TRUE,                                                             \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [8]  {milk=FALSE,                                                            \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [9]  {toothed=TRUE,                                                          \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [10] {breathes=FALSE,                                                        \n##       fins=TRUE,                                                             \n##       tail=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"calculate-additional-interest-measures","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3.1 Calculate Additional Interest Measures","text":"Add measures rulesFind rules score high Phi correlation","code":"\ninterestMeasure(rules[1:10], measure = c(\"phi\", \"gini\"),\n  trans = trans)\n##       phi   gini\n## 1  1.0000 0.2243\n## 2  1.0000 0.2243\n## 3  0.8138 0.1485\n## 4  1.0000 0.2243\n## 5  1.0000 0.2243\n## 6  1.0000 0.2243\n## 7  0.8138 0.1485\n## 8  1.0000 0.2243\n## 9  1.0000 0.2243\n## 10 1.0000 0.2243\nquality(rules) <- cbind(quality(rules),\n  interestMeasure(rules, measure = c(\"phi\", \"gini\"),\n    trans = trans))\nrules |> head(by = \"phi\") |> inspect()\n##     lhs                               rhs         support confidence\n## [1] {eggs, fins}                   => {type=fish} 0.1287  1         \n## [2] {eggs, aquatic, fins}          => {type=fish} 0.1287  1         \n## [3] {eggs, toothed, fins}          => {type=fish} 0.1287  1         \n## [4] {eggs, fins, tail}             => {type=fish} 0.1287  1         \n## [5] {eggs, backbone, fins}         => {type=fish} 0.1287  1         \n## [6] {eggs, aquatic, toothed, fins} => {type=fish} 0.1287  1         \n##     coverage lift  count phi gini  \n## [1] 0.1287   7.769 13    1   0.2243\n## [2] 0.1287   7.769 13    1   0.2243\n## [3] 0.1287   7.769 13    1   0.2243\n## [4] 0.1287   7.769 13    1   0.2243\n## [5] 0.1287   7.769 13    1   0.2243\n## [6] 0.1287   7.769 13    1   0.2243"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"mine-using-templates","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3.2 Mine Using Templates","text":"Sometimes beneficial specify items \nrule. apriori can use parameter appearance specify \n(see\n? APappearance).\nfollowing restrict rules animal type RHS \nitem LHS.Saving rules CSV-file opened Excel tools.write(rules, file = \"rules.csv\", quote = TRUE)","code":"\ntype <- grep(\"type=\", itemLabels(trans), value = TRUE)\ntype\n## [1] \"type=mammal\"        \"type=bird\"          \"type=reptile\"      \n## [4] \"type=fish\"          \"type=amphibian\"     \"type=insect\"       \n## [7] \"type=mollusc.et.al\"\n\nrules_type <- apriori(trans, appearance= list(rhs = type))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support\n##         0.8    0.1    1 none FALSE            TRUE       5     0.1\n##  minlen maxlen target  ext\n##       1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[7 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [18 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, appearance = list(rhs = type)): Mining\n## stopped (maxlen reached). Only patterns up to a length of 10\n## returned!\n##  done [0.00s].\n## writing ... [571 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\n\nrules_type |> sort(by = \"lift\") |> head() |> inspect()\n##     lhs                               rhs         support confidence\n## [1] {eggs, fins}                   => {type=fish} 0.1287  1         \n## [2] {eggs, aquatic, fins}          => {type=fish} 0.1287  1         \n## [3] {eggs, toothed, fins}          => {type=fish} 0.1287  1         \n## [4] {eggs, fins, tail}             => {type=fish} 0.1287  1         \n## [5] {eggs, backbone, fins}         => {type=fish} 0.1287  1         \n## [6] {eggs, aquatic, toothed, fins} => {type=fish} 0.1287  1         \n##     coverage lift  count\n## [1] 0.1287   7.769 13   \n## [2] 0.1287   7.769 13   \n## [3] 0.1287   7.769 13   \n## [4] 0.1287   7.769 13   \n## [5] 0.1287   7.769 13   \n## [6] 0.1287   7.769 13"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"compact-representation-of-frequent-itemsets","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.4 Compact Representation of Frequent Itemsets","text":"Find maximal frequent itemsets (superset frequent)Find closed frequent itemsets (superset frequent)","code":"\nits_max <- its[is.maximal(its)]\nits_max\n## set of 22 itemsets\nits_max |> head(by = \"support\") |> inspect()\n##     items         support count\n## [1] {hair,                     \n##      milk,                     \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      tail,                     \n##      catsize,                  \n##      type=mammal} 0.12871    13\n## [2] {eggs,                     \n##      aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      type=fish}   0.08911     9\n## [3] {aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes}    0.07921     8\n## [4] {aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      catsize}     0.06931     7\n## [5] {eggs,                     \n##      venomous}    0.05941     6\n## [6] {predator,                 \n##      venomous}    0.05941     6\nits_closed <- its[is.closed(its)]\nits_closed\n## set of 230 itemsets\n\nits_closed |> head(by = \"support\") |> inspect()\n##     items            support count\n## [1] {backbone}       0.8218  83   \n## [2] {breathes}       0.7921  80   \n## [3] {legs}           0.7723  78   \n## [4] {tail}           0.7426  75   \n## [5] {backbone, tail} 0.7327  74   \n## [6] {breathes, legs} 0.7228  73\n\ncounts <- c(\n  frequent=length(its),\n  closed=length(its_closed),\n  maximal=length(its_max)\n)\n\nggplot(as_tibble(counts, rownames = \"Itemsets\"),\n  aes(Itemsets, counts)) + geom_bar(stat = \"identity\")"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"association-rule-visualization","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5 Association Rule Visualization*","text":"Visualization powerful approach analyse large sets \nmined association rules frequent itemsets. present options\ncreate static visualizations inspect rule sets interactively.","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"static-visualizations","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.1 Static Visualizations","text":"Default scatterplotNote jitter (randomly move points) added show many\nrules confidence support value. Without jitter:Grouped plotAs graph","code":"\nlibrary(arulesViz)\nplot(rules)\n## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\nplot(rules, control = list(jitter = 0))\n\nplot(rules, shading = \"order\")\n## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n##plot(rules, interactive = TRUE)\nplot(rules, method = \"grouped\")\n##plot(rules, method = \"grouped\", engine = \"interactive\")\nplot(rules, method = \"graph\")\n## Warning: Too many rules supplied. Only plotting the best 100 using\n## 'lift' (change control parameter max if needed).\nplot(rules |> head(by = \"phi\", n = 100), method = \"graph\")"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-visualizations","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.2 Interactive Visualizations","text":"use association rules mined Iris dataset \nfollowing examples.Convert data transactions.Note conversion gives warning indicate potentially\nunwanted conversion happens. features numeric \nneed discretized. conversion automatically applies\nfrequency-based discretization 3 classes numeric feature,\nhowever, use may want use different discretization strategy.Next, mine association rules.","code":"\ndata(iris)\nsummary(iris)\n##   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  \n##  Median :5.80   Median :3.00   Median :4.35   Median :1.3  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \niris_trans <- transactions(iris)\n## Warning: Column(s) 1, 2, 3, 4 not logical or factor. Applying default\n## discretization (see '? discretizeDF').\niris_trans |> head() |> inspect()\n##     items                      transactionID\n## [1] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       1\n## [2] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[2.9,3.2),                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       2\n## [3] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       3\n## [4] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[2.9,3.2),                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       4\n## [5] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       5\n## [6] {Sepal.Length=[5.4,6.3),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       6\nrules <- apriori(iris_trans, parameter = list(support = 0.1, confidence = 0.8))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime support\n##         0.8    0.1    1 none FALSE            TRUE       5     0.1\n##  minlen maxlen target  ext\n##       1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 15 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[15 item(s), 150 transaction(s)] done [0.00s].\n## sorting and recoding items ... [15 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 done [0.00s].\n## writing ... [144 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nrules\n## set of 144 rules"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-inspect-with-sorting-filtering-and-paging","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.2.1 Interactive Inspect With Sorting, Filtering and Paging","text":"","code":"\ninspectDT(rules,options = list(scrollX = TRUE))"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"scatter-plot-1","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.2.2 Scatter Plot","text":"Plot rules scatter plot using interactive html widget. avoid\noverplotting, jitter added automatically. Set jitter = 0 disable\njitter. Hovering rules shows rule information. Note:\nplotly/javascript well many points, plot selects\ntop 1000 rules warning rules supplied.","code":"\nplot(rules, engine = \"html\")\n## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter."},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"matrix-visualization-1","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.2.3 Matrix Visualization","text":"Plot rules matrix using interactive html widget.","code":"\nplot(rules, method = \"matrix\", engine = \"html\") "},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"visualization-as-graph","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.2.4 Visualization as Graph","text":"Plot rules graph using interactive html widget. Note: used\njavascript library well many graph nodes, plot\nselects top 100 rules (warning).","code":"\nplot(rules, method = \"graph\", engine = \"html\")\n## Warning: Too many rules supplied. Only plotting the best 100 using\n## 'lift' (change control parameter max if needed)."},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-rule-explorer","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5.2.5 Interactive Rule Explorer","text":"can specify rule set dataset. explore rules can \nmined iris, use: ruleExplorer(iris)rule explorer creates interactive Shiny application can \nused locally deployed server sharing. deployed version \nruleExplorer available\n(using\nshinyapps.io).","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"exercises-3","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.6 Exercises*","text":"use Palmer penguin data exercises.Translate penguin data transaction data :conversion report warnings?following first three transactions mean?Next, use ruleExplorer() function analyze association rules\ncreated transaction data set.Use default settings parameters.\nUsing Data Table, association rule highest lift. LHS, RHS,\nsupport, confidence lift mean?Use default settings parameters.\nUsing Data Table, association rule highest lift. LHS, RHS,\nsupport, confidence lift mean?Use Graph visualization. Use select id highlight different species \ndifferent islands hover rules. see?Use Graph visualization. Use select id highlight different species \ndifferent islands hover rules. see?","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm\n##   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n## 1 Adelie  Torgersen           39.1          18.7               181\n## 2 Adelie  Torgersen           39.5          17.4               186\n## 3 Adelie  Torgersen           40.3          18                 195\n## 4 Adelie  Torgersen           NA            NA                  NA\n## 5 Adelie  Torgersen           36.7          19.3               193\n## 6 Adelie  Torgersen           39.3          20.6               190\n## # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\ntrans <- transactions(penguins)\n## Warning: Column(s) 1, 2, 3, 4, 5, 6, 7, 8 not logical or factor.\n## Applying default discretization (see '? discretizeDF').\n## Warning in discretize(x = c(2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, : The calculated breaks are: 2007, 2008, 2009, 2009\n##   Only unique breaks are used reducing the number of intervals. Look at ? discretize for details.\ntrans\n## transactions in sparse format with\n##  344 transactions (rows) and\n##  22 items (columns)\ninspect(trans[1:3])\n##     items                             transactionID\n## [1] {species=Adelie,                               \n##      island=Torgersen,                             \n##      bill_length_mm=[32.1,40.8),                   \n##      bill_depth_mm=[18.3,21.5],                    \n##      flipper_length_mm=[172,192),                  \n##      body_mass_g=[3.7e+03,4.55e+03),               \n##      sex=male,                                     \n##      year=[2007,2008)}                            1\n## [2] {species=Adelie,                               \n##      island=Torgersen,                             \n##      bill_length_mm=[32.1,40.8),                   \n##      bill_depth_mm=[16.2,18.3),                    \n##      flipper_length_mm=[172,192),                  \n##      body_mass_g=[3.7e+03,4.55e+03),               \n##      sex=female,                                   \n##      year=[2007,2008)}                            2\n## [3] {species=Adelie,                               \n##      island=Torgersen,                             \n##      bill_length_mm=[32.1,40.8),                   \n##      bill_depth_mm=[16.2,18.3),                    \n##      flipper_length_mm=[192,209),                  \n##      body_mass_g=[2.7e+03,3.7e+03),                \n##      sex=female,                                   \n##      year=[2007,2008)}                            3"},{"path":"association-analysis-advanced-concepts.html","id":"association-analysis-advanced-concepts","chapter":"6 Association Analysis: Advanced Concepts","heading":"6 Association Analysis: Advanced Concepts","text":"code available Chapter. topics already covered \ncode previous chapter.","code":""},{"path":"cluster-analysis.html","id":"cluster-analysis","chapter":"7 Cluster Analysis","heading":"7 Cluster Analysis","text":"packages used chapter : cluster (Maechler et al. 2023), dbscan (Hahsler Piekenbrock 2024), e1071 (Meyer et al. 2023), factoextra (Kassambara Mundt 2020), fpc (Hennig 2024), GGally (Schloerke et al. 2024), kernlab (Karatzoglou, Smola, Hornik 2023), mclust (Fraley, Raftery, Scrucca 2024), mlbench (Leisch Dimitriadou 2024), scatterpie (Yu 2024), seriation (Hahsler, Buchta, Hornik 2024), tidyverse (Wickham 2023c)","code":"\npkgs <- c(\"cluster\", \"dbscan\", \"e1071\", \"factoextra\", \"fpc\", \n          \"GGally\", \"kernlab\", \"mclust\", \"mlbench\", \"scatterpie\", \n          \"seriation\", \"tidyverse\")\n  \npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"cluster-analysis.html","id":"overview","chapter":"7 Cluster Analysis","heading":"7.1 Overview","text":"Cluster analysis clustering\ntask grouping set objects way objects group (called cluster) similar (sense) groups (clusters).Clustering also called unsupervised learning, tries directly learns structure data\nrely availability correct answer class label supervised learning .\nClustering often used exploratory analysis preprocess data grouping.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 7. Cluster Analysis: Basic Concepts \nAlgorithms","code":""},{"path":"cluster-analysis.html","id":"data-preparation","chapter":"7 Cluster Analysis","heading":"7.1.1 Data Preparation","text":"use small clean toy dataset called Ruspini \nincluded R package cluster.Ruspini data set, consisting 75 points four groups \npopular illustrating clustering techniques. simple data\nset well separated clusters. original dataset points\nordered group. can shuffle data (rows) using sample_frac\nsamples default 100%.","code":"\nlibrary(tidyverse)\ndata(ruspini, package = \"cluster\")\nruspini <- as_tibble(ruspini) |> \n  sample_frac()\nruspini\n## # A tibble: 75 × 2\n##        x     y\n##    <int> <int>\n##  1   115   117\n##  2   111   126\n##  3    85    96\n##  4    12    88\n##  5    36    72\n##  6    28   147\n##  7    24    58\n##  8    99   128\n##  9    46   142\n## 10    38   151\n## # ℹ 65 more rows"},{"path":"cluster-analysis.html","id":"data-cleaning","chapter":"7 Cluster Analysis","heading":"7.1.2 Data cleaning","text":"clustering algorithms necessary handle missing values\noutliers (e.g., remove observations). details see Section\n“Outlier removal” . data set missing values strong\noutlier looks like clear groups.","code":"\nggplot(ruspini, aes(x = x, y = y)) + geom_point()\n\nsummary(ruspini)\n##        x               y        \n##  Min.   :  4.0   Min.   :  4.0  \n##  1st Qu.: 31.5   1st Qu.: 56.5  \n##  Median : 52.0   Median : 96.0  \n##  Mean   : 54.9   Mean   : 92.0  \n##  3rd Qu.: 76.5   3rd Qu.:141.5  \n##  Max.   :117.0   Max.   :156.0"},{"path":"cluster-analysis.html","id":"scale-data","chapter":"7 Cluster Analysis","heading":"7.1.3 Scale data","text":"Clustering algorithms use distances variables largest\nnumber range dominate distance calculation. summary shows\nissue Ruspini dataset , x y,\nroughly 0 150. data analysts still scale\ncolumn data zero mean unit standard deviation\n(z-scores).Note: standard scale() function scales whole data\nmatrix implement function single vector apply \nnumeric columns.scaling, z-scores fall range \\([-3,3]\\) (z-scores\nmeasured standard deviations mean), \\(0\\) means\naverage.","code":"\n## I use this till tidyverse implements a scale function\nscale_numeric <- function(x) {\n  x |> mutate(across(where(is.numeric), function(y) as.vector(scale(y))))\n}\nruspini_scaled <- ruspini |> \n  scale_numeric()\nsummary(ruspini_scaled)\n##        x                 y          \n##  Min.   :-1.6681   Min.   :-1.8074  \n##  1st Qu.:-0.7665   1st Qu.:-0.7295  \n##  Median :-0.0944   Median : 0.0816  \n##  Mean   : 0.0000   Mean   : 0.0000  \n##  3rd Qu.: 0.7088   3rd Qu.: 1.0158  \n##  Max.   : 2.0366   Max.   : 1.3136"},{"path":"cluster-analysis.html","id":"k-means","chapter":"7 Cluster Analysis","heading":"7.2 K-means","text":"k-means implicitly\nassumes Euclidean distances. use \\(k = 4\\) clusters run \nalgorithm 10 times random initialized centroids. best result \nreturned.km R object implemented list. clustering vector\ncontains cluster assignment data row can accessed\nusing km$cluster. add cluster assignment column \nscaled dataset (make factor since represents nominal label).Add centroids plot. Note second geom_points uses\noriginal data specifies centroids dataset.factoextra package provides also good visualization object labels\nellipses clusters.","code":"\nkm <- kmeans(ruspini_scaled, centers = 4, nstart = 10)\nkm\n## K-means clustering with 4 clusters of sizes 23, 17, 15, 20\n## \n## Cluster means:\n##         x       y\n## 1 -0.3595  1.1091\n## 2  1.4194  0.4693\n## 3  0.4607 -1.4912\n## 4 -1.1386 -0.5560\n## \n## Clustering vector:\n##  [1] 2 2 2 4 4 1 4 2 1 1 1 1 4 3 3 2 4 3 2 1 1 1 2 2 3 2 1 3 3 2 1 3 3\n## [34] 4 2 1 3 4 4 3 1 4 1 3 2 4 3 1 2 1 1 2 2 4 4 2 1 4 3 4 1 1 2 4 4 4\n## [67] 1 1 3 3 4 1 1 4 4\n## \n## Within cluster sum of squares by cluster:\n## [1] 2.659 3.641 1.082 2.705\n##  (between_SS / total_SS =  93.2 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"    \n## [5] \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\"        \n## [9] \"ifault\"\nruspini_clustered <- ruspini_scaled |> \n  add_column(cluster = factor(km$cluster))\nruspini_clustered\n## # A tibble: 75 × 3\n##         x       y cluster\n##     <dbl>   <dbl> <fct>  \n##  1  1.97   0.513  2      \n##  2  1.84   0.698  2      \n##  3  0.987  0.0816 2      \n##  4 -1.41  -0.0827 4      \n##  5 -0.619 -0.411  4      \n##  6 -0.881  1.13   1      \n##  7 -1.01  -0.699  4      \n##  8  1.45   0.739  2      \n##  9 -0.291  1.03   1      \n## 10 -0.553  1.21   1      \n## # ℹ 65 more rows\n\nggplot(ruspini_clustered, aes(x = x, y = y)) + \n  geom_point(aes(color = cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\ncentroids\n## # A tibble: 4 × 3\n##   cluster      x      y\n##   <chr>    <dbl>  <dbl>\n## 1 1       -0.360  1.11 \n## 2 2        1.42   0.469\n## 3 3        0.461 -1.49 \n## 4 4       -1.14  -0.556\n\nggplot(ruspini_clustered, aes(x = x, y = y)) + \n  geom_point(aes(color = cluster)) +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), \n             shape = 3, size = 10)\nlibrary(factoextra)\nfviz_cluster(km, data = ruspini_scaled, centroids = TRUE, \n             repel = TRUE, ellipse.type = \"norm\")"},{"path":"cluster-analysis.html","id":"inspect-clusters","chapter":"7 Cluster Analysis","heading":"7.2.0.1 Inspect clusters","text":"inspect clusters created 4-cluster k-means solution. \nfollowing code can adapted used clustering methods.","code":""},{"path":"cluster-analysis.html","id":"cluster-profiles","chapter":"7 Cluster Analysis","heading":"7.2.0.1.1 Cluster Profiles","text":"Inspect centroids horizontal bar charts organized cluster.\ngroup plots cluster, change data format \n“long”-format using pivot operation. use colors match \nclusters scatter plots.","code":"\nggplot(pivot_longer(centroids, cols = c(x, y), names_to = \"feature\"),\n  aes(x = value, y = feature, fill = cluster)) +\n  geom_bar(stat = \"identity\") +\n  facet_grid(rows = vars(cluster))"},{"path":"cluster-analysis.html","id":"extract-a-single-cluster","chapter":"7 Cluster Analysis","heading":"7.2.0.1.2 Extract a single cluster","text":"need filter rows corresponding cluster index. \nnext example calculates summary statistics plots data\npoints cluster 1.happens try cluster 8 centers?","code":"\ncluster1 <- ruspini_clustered |> \n  filter(cluster == 1)\ncluster1\n## # A tibble: 23 × 3\n##           x     y cluster\n##       <dbl> <dbl> <fct>  \n##  1 -0.881   1.13  1      \n##  2 -0.291   1.03  1      \n##  3 -0.553   1.21  1      \n##  4 -0.553   1.09  1      \n##  5 -0.160   1.03  1      \n##  6 -0.717   1.27  1      \n##  7  0.00393 1.29  1      \n##  8 -0.0944  1.23  1      \n##  9 -0.750   1.17  1      \n## 10 -0.0289  0.657 1      \n## # ℹ 13 more rows\nsummary(cluster1)\n##        x                y         cluster\n##  Min.   :-0.881   Min.   :0.656   1:23   \n##  1st Qu.:-0.603   1st Qu.:1.036   2: 0   \n##  Median :-0.357   Median :1.129   3: 0   \n##  Mean   :-0.359   Mean   :1.109   4: 0   \n##  3rd Qu.:-0.127   3rd Qu.:1.221          \n##  Max.   : 0.266   Max.   :1.314\n\nggplot(cluster1, aes(x = x, y = y)) + geom_point() +\n  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2))\nfviz_cluster(kmeans(ruspini_scaled, centers = 8), data = ruspini_scaled,\n  centroids = TRUE,  geom = \"point\", ellipse.type = \"norm\")"},{"path":"cluster-analysis.html","id":"agglomerative-hierarchical-clustering","chapter":"7 Cluster Analysis","heading":"7.3 Agglomerative Hierarchical Clustering","text":"Hierarchical clustering starts distance matrix. dist() defaults\nmethod=“Euclidean”. Note: Distance matrices become large\nquickly (size time complexity \\(O(n^2)\\) \\(n\\) number \ndata points). possible calculate store matrix \nsmall data sets (maybe hundred thousand data points) main\nmemory. data large can use sampling.hclust() implements agglomerative hierarchical\nclustering. \ncluster using complete link.Hierarchical clustering return cluster assignments \ndendrogram. standard plot function plots dendrogram.Use factoextra (ggplot version). can specify number clusters\nvisualize dendrogram cut clusters.plotting options dendrograms, including plotting parts large\ndendrograms can found .Extract cluster assignments cutting dendrogram four parts\nadd cluster id data.Try 8 clusters (Note: fviz_cluster needs list data \ncluster labels hclust)Clustering single link","code":"\nd <- dist(ruspini_scaled)\nhc <- hclust(d, method = \"complete\")\nplot(hc)\nfviz_dend(hc, k = 4)\nclusters <- cutree(hc, k = 4)\ncluster_complete <- ruspini_scaled |>\n  add_column(cluster = factor(clusters))\ncluster_complete\n## # A tibble: 75 × 3\n##         x       y cluster\n##     <dbl>   <dbl> <fct>  \n##  1  1.97   0.513  1      \n##  2  1.84   0.698  1      \n##  3  0.987  0.0816 1      \n##  4 -1.41  -0.0827 2      \n##  5 -0.619 -0.411  2      \n##  6 -0.881  1.13   3      \n##  7 -1.01  -0.699  2      \n##  8  1.45   0.739  1      \n##  9 -0.291  1.03   3      \n## 10 -0.553  1.21   3      \n## # ℹ 65 more rows\n\nggplot(cluster_complete, aes(x, y, color = cluster)) +\n  geom_point()\nfviz_cluster(list(data = ruspini_scaled, \n                  cluster = cutree(hc, k = 8)), \n             geom = \"point\")\nhc_single <- hclust(d, method = \"single\")\nfviz_dend(hc_single, k = 4)\n\nfviz_cluster(list(data = ruspini_scaled, \n                  cluster = cutree(hc_single, k = 4)), \n             geom = \"point\")"},{"path":"cluster-analysis.html","id":"dbscan","chapter":"7 Cluster Analysis","heading":"7.4 DBSCAN","text":"DBSCAN stands “Density-Based\nSpatial Clustering Applications Noise.” groups together\npoints closely packed together treats points low-density\nregions outliers.Parameters: minPts defines many points epsilon\nneighborhood needed make point core point. often chosen\nsmoothing parameter. use minPts = 4.decide epsilon, knee kNN distance plot often used.\nNote minPts contains point , k-nearest neighbor\n. therefore use k = minPts - 1! knee around\neps = .32.run dbscanNote: Cluster 0 represents outliers).Play eps (neighborhood size) MinPts (minimum points needed\ncore cluster)","code":"\nlibrary(dbscan)\n## \n## Attaching package: 'dbscan'\n## The following object is masked from 'package:stats':\n## \n##     as.dendrogram\nkNNdistplot(ruspini_scaled, k = 3)\nabline(h = .32, col = \"red\")\ndb <- dbscan(ruspini_scaled, eps = .32, minPts = 4)\ndb\n## DBSCAN clustering for 75 objects.\n## Parameters: eps = 0.32, minPts = 4\n## Using euclidean distances and borderpoints = TRUE\n## The clustering contains 4 cluster(s) and 5 noise points.\n## \n##  0  1  2  3  4 \n##  5 12 20 23 15 \n## \n## Available fields: cluster, eps, minPts, dist, borderPoints\nstr(db)\n## List of 5\n##  $ cluster     : int [1:75] 1 1 0 2 2 3 2 1 3 3 ...\n##  $ eps         : num 0.32\n##  $ minPts      : num 4\n##  $ dist        : chr \"euclidean\"\n##  $ borderPoints: logi TRUE\n##  - attr(*, \"class\")= chr [1:2] \"dbscan_fast\" \"dbscan\"\n\nggplot(ruspini_scaled |> add_column(cluster = factor(db$cluster)),\n  aes(x, y, color = cluster)) + geom_point()\nfviz_cluster(db, ruspini_scaled, geom = \"point\")"},{"path":"cluster-analysis.html","id":"cluster-evaluation","chapter":"7 Cluster Analysis","heading":"7.5 Cluster Evaluation","text":"","code":""},{"path":"cluster-analysis.html","id":"unsupervized-cluster-evaluation","chapter":"7 Cluster Analysis","heading":"7.5.1 Unsupervized Cluster Evaluation","text":"two popular quality metrics within-cluster sum \nsquares (WCSS) used optimization objective \n\\(k\\)-means \naverage silhouette\nwidth. Look \nwithin.cluster.ss avg.silwidth .Notes:\n* load fpc since NAMESPACE overwrites dbscan.\n* clustering (second argument ) supplied vector\nnumbers (cluster IDs) factor (use .integer() \nconvert factor ID).Read ? cluster.stats explanation available indices.Next, look silhouette using \nsilhouette plot.Note: silhouette plot show correctly R Studio \nmany objects (bars missing). work open \nnew plotting device windows(), x11() quartz().ggplot visualization using factoextra","code":"\n##library(fpc)\nfpc::cluster.stats(d, km$cluster)\n## $n\n## [1] 75\n## \n## $cluster.number\n## [1] 4\n## \n## $cluster.size\n## [1] 23 17 15 20\n## \n## $min.cluster.size\n## [1] 15\n## \n## $noisen\n## [1] 0\n## \n## $diameter\n## [1] 1.1591 1.4627 0.8359 1.1193\n## \n## $average.distance\n## [1] 0.4286 0.5806 0.3564 0.4824\n## \n## $median.distance\n## [1] 0.3934 0.5024 0.3380 0.4492\n## \n## $separation\n## [1] 0.7676 0.7676 1.1577 1.1577\n## \n## $average.toother\n## [1] 2.149 2.293 2.308 2.157\n## \n## $separation.matrix\n##        [,1]   [,2]  [,3]  [,4]\n## [1,] 0.0000 0.7676 1.958 1.220\n## [2,] 0.7676 0.0000 1.308 1.340\n## [3,] 1.9577 1.3084 0.000 1.158\n## [4,] 1.2199 1.3397 1.158 0.000\n## \n## $ave.between.matrix\n##       [,1]  [,2]  [,3]  [,4]\n## [1,] 0.000 1.925 2.750 1.887\n## [2,] 1.925 0.000 2.220 2.772\n## [3,] 2.750 2.220 0.000 1.874\n## [4,] 1.887 2.772 1.874 0.000\n## \n## $average.between\n## [1] 2.219\n## \n## $average.within\n## [1] 0.463\n## \n## $n.between\n## [1] 2091\n## \n## $n.within\n## [1] 684\n## \n## $max.diameter\n## [1] 1.463\n## \n## $min.separation\n## [1] 0.7676\n## \n## $within.cluster.ss\n## [1] 10.09\n## \n## $clus.avg.silwidths\n##      1      2      3      4 \n## 0.7455 0.6813 0.8074 0.7211 \n## \n## $avg.silwidth\n## [1] 0.7368\n## \n## $g2\n## NULL\n## \n## $g3\n## NULL\n## \n## $pearsongamma\n## [1] 0.8416\n## \n## $dunn\n## [1] 0.5248\n## \n## $dunn2\n## [1] 3.228\n## \n## $entropy\n## [1] 1.373\n## \n## $wb.ratio\n## [1] 0.2086\n## \n## $ch\n## [1] 323.6\n## \n## $cwidegap\n## [1] 0.3153 0.4150 0.2352 0.2612\n## \n## $widestgap\n## [1] 0.415\n## \n## $sindex\n## [1] 0.8583\n## \n## $corrected.rand\n## NULL\n## \n## $vi\n## NULL\nsapply(\n  list(\n    km = km$cluster,\n    hc_compl = cutree(hc, k = 4),\n    hc_single = cutree(hc_single, k = 4)\n  ),\n  FUN = function(x)\n    fpc::cluster.stats(d, x))[c(\"within.cluster.ss\", \"avg.silwidth\"), ]\n##                   km     hc_compl hc_single\n## within.cluster.ss 10.09  10.09    10.09    \n## avg.silwidth      0.7368 0.7368   0.7368\nlibrary(cluster)\n## \n## Attaching package: 'cluster'\n## The following object is masked _by_ '.GlobalEnv':\n## \n##     ruspini\nplot(silhouette(km$cluster, d))\nfviz_silhouette(silhouette(km$cluster, d))\n##   cluster size ave.sil.width\n## 1       1   23          0.75\n## 2       2   17          0.68\n## 3       3   15          0.81\n## 4       4   20          0.72"},{"path":"cluster-analysis.html","id":"unsupervised-cluster-evaluation-using-the-proximity-matrix","chapter":"7 Cluster Analysis","heading":"7.5.2 Unsupervised Cluster Evaluation using the Proximity Matrix","text":"Inspect distance matrix first 5 objects.false-color image visualizes value matrix pixel \ncolor representing value.Rows columns objects ordered data set.\ndiagonal represents distance object \ndefinition distance 0 (dark line). Visualizing unordered\ndistance matrix show much structure, can reorder \nmatrix (rows columns) using k-means cluster labels cluster\n1 4. clear block structure representing clusters becomes\nvisible.Plot function dissplot package seriation rearranges matrix\nadds lines cluster labels. lower half plot, \nshows average dissimilarities clusters. function organizes\nobjects cluster reorders clusters objects within\nclusters similar objects closer together.reordering dissplot makes misspecification k visible \nblocks.Using factoextra","code":"\nggplot(ruspini_scaled, \n       aes(x, y, color = factor(km$cluster))) + \n  geom_point()\n\nd <- dist(ruspini_scaled)\nas.matrix(d)[1:5, 1:5]\n##        1      2     3      4      5\n## 1 0.0000 0.2266 1.074 3.4289 2.7498\n## 2 0.2266 0.0000 1.052 3.3381 2.6972\n## 3 1.0739 1.0517 0.000 2.3989 1.6803\n## 4 3.4289 3.3381 2.399 0.0000 0.8527\n## 5 2.7498 2.6972 1.680 0.8527 0.0000\nlibrary(seriation)\npimage(d, col = bluered(100))\npimage(d, order=order(km$cluster), col = bluered(100))\ndissplot(d, labels = km$cluster, \n         options = list(main = \"k-means with k=4\"))\ndissplot(d, labels = kmeans(ruspini_scaled, centers = 3)$cluster, \n         col = bluered(100))\ndissplot(d, labels = kmeans(ruspini_scaled, centers = 9)$cluster, \n         col = bluered(100))\nfviz_dist(d)"},{"path":"cluster-analysis.html","id":"determining-the-correct-number-of-clusters","chapter":"7 Cluster Analysis","heading":"7.5.3 Determining the Correct Number of Clusters","text":"","code":"\nggplot(ruspini_scaled, aes(x, y)) + geom_point()\n\n## We will use different methods and try 1-10 clusters.\nset.seed(1234)\nks <- 2:10"},{"path":"cluster-analysis.html","id":"elbow-method-within-cluster-sum-of-squares","chapter":"7 Cluster Analysis","heading":"7.5.3.1 Elbow Method: Within-Cluster Sum of Squares","text":"Calculate within-cluster sum squares different numbers \nclusters look knee \nelbow \nplot. (nstart = 5 just repeats k-means 5 times returns best\nsolution)","code":"\nWCSS <- sapply(ks, FUN = function(k) {\n  kmeans(ruspini_scaled, centers = k, nstart = 5)$tot.withinss\n  })\n\nggplot(tibble(ks, WCSS), aes(ks, WCSS)) + \n  geom_line() +\n  geom_vline(xintercept = 4, color = \"red\", linetype = 2)"},{"path":"cluster-analysis.html","id":"average-silhouette-width","chapter":"7 Cluster Analysis","heading":"7.5.3.2 Average Silhouette Width","text":"Plot average silhouette width different number clusters \nlook maximum plot.","code":"\nASW <- sapply(ks, FUN=function(k) {\n  fpc::cluster.stats(d, \n                     kmeans(ruspini_scaled, \n                            centers = k, \n                            nstart = 5)$cluster)$avg.silwidth\n  })\n\nbest_k <- ks[which.max(ASW)]\nbest_k\n## [1] 4\n\nggplot(tibble(ks, ASW), aes(ks, ASW)) + \n  geom_line() +\n  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"},{"path":"cluster-analysis.html","id":"dunn-index","chapter":"7 Cluster Analysis","heading":"7.5.3.3 Dunn Index","text":"Use Dunn index (another\ninternal measure given min. separation/ max. diameter)","code":"\nDI <- sapply(ks, FUN = function(k) {\n  fpc::cluster.stats(d, \n                     kmeans(ruspini_scaled, centers = k, \n                            nstart = 5)$cluster)$dunn\n})\n\nbest_k <- ks[which.max(DI)]\nggplot(tibble(ks, DI), aes(ks, DI)) + \n  geom_line() +\n  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"},{"path":"cluster-analysis.html","id":"gap-statistic","chapter":"7 Cluster Analysis","heading":"7.5.3.4 Gap Statistic","text":"Compares change within-cluster dispersion expected \nnull model (see ? clusGap). default method choose \nsmallest k value Gap(k) 1 standard error\naway first local maximum.Note: methods can also used hierarchical clustering.many methods indices proposed determine \nnumber clusters. See, e.g., package\nNbClust.","code":"\nlibrary(cluster)\nk <- clusGap(ruspini_scaled, FUN = kmeans,  nstart = 10, K.max = 10)\nk\n## Clustering Gap statistic [\"clusGap\"] from call:\n## clusGap(x = ruspini_scaled, FUNcluster = kmeans, K.max = 10, nstart = 10)\n## B=100 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n##  --> Number of clusters (method 'firstSEmax', SE.factor=1): 4\n##        logW E.logW      gap  SE.sim\n##  [1,] 3.498  3.467 -0.03080 0.03570\n##  [2,] 3.074  3.150  0.07625 0.03744\n##  [3,] 2.678  2.903  0.22466 0.03796\n##  [4,] 2.106  2.704  0.59710 0.03633\n##  [5,] 1.987  2.570  0.58271 0.03474\n##  [6,] 1.864  2.451  0.58713 0.03651\n##  [7,] 1.732  2.348  0.61558 0.03954\n##  [8,] 1.640  2.256  0.61570 0.04132\n##  [9,] 1.561  2.172  0.61129 0.04090\n## [10,] 1.513  2.093  0.57987 0.03934\nplot(k)"},{"path":"cluster-analysis.html","id":"clustering-tendency","chapter":"7 Cluster Analysis","heading":"7.5.4 Clustering Tendency","text":"clustering algorithms always produce clustering, even \ndata contain cluster structure. typically good check\ncluster tendency attempting cluster data.use smiley data.","code":"\nlibrary(mlbench)\nshapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)$x\ncolnames(shapes) <- c(\"x\", \"y\")\nshapes <- as_tibble(shapes)"},{"path":"cluster-analysis.html","id":"scatter-plots","chapter":"7 Cluster Analysis","heading":"7.5.4.1 Scatter plots","text":"first step visual inspection using scatter plots.Cluster tendency typically indicated several separated point\nclouds. Often appropriate number clusters can also visually\nobtained counting number point clouds. see four clusters,\nmouth convex/spherical thus pose problems \nalgorithms like k-means.data two features can use pairs plot\n(scatterplot matrix) look scatterplot first two principal\ncomponents using PCA.","code":"\nggplot(shapes, aes(x = x, y = y)) + geom_point()"},{"path":"cluster-analysis.html","id":"visual-analysis-for-cluster-tendency-assessment-vat","chapter":"7 Cluster Analysis","heading":"7.5.4.2 Visual Analysis for Cluster Tendency Assessment (VAT)","text":"VAT reorders objects show potential clustering tendency \nblock structure (dark blocks along main diagonal). scale data\nusing Euclidean distance.iVAT uses largest distances possible paths two\nobjects instead direct distances make block structure\nbetter visible.","code":"\nlibrary(seriation)\n\nd_shapes <- dist(scale(shapes))\nVAT(d_shapes, col = bluered(100))\niVAT(d_shapes, col = bluered(100))"},{"path":"cluster-analysis.html","id":"hopkins-statistic","chapter":"7 Cluster Analysis","heading":"7.5.4.3 Hopkins statistic","text":"factoextra can also create VAT plot calculate Hopkins\nstatistic assess\nclustering tendency. Hopkins statistic, sample size \\(n\\) \ndrawn data compares nearest neighbor distribution\nsimulated dataset drawn random uniform distribution (see\ndetailed\nexplanation).\nvalues >.5 indicates usually clustering tendency.plots show strong cluster structure 4 clusters.","code":"\nget_clust_tendency(shapes, n = 10)\n## $hopkins_stat\n## [1] 0.9074\n## \n## $plot"},{"path":"cluster-analysis.html","id":"data-without-clustering-tendency","chapter":"7 Cluster Analysis","heading":"7.5.4.4 Data Without Clustering Tendency","text":"point clouds visible, just noise.little clustering structure visible indicating low\nclustering tendency clustering performed data.\nHowever, k-means can used partition data \\(k\\) regions \nroughly equivalent size. can used data-driven\ndiscretization space.","code":"\ndata_random <- tibble(x = runif(500), y = runif(500))\nggplot(data_random, aes(x, y)) + geom_point()\nd_random <- dist(data_random)\nVAT(d_random, col = bluered(100))\niVAT(d_random, col = bluered(100))\nget_clust_tendency(data_random, n = 10, graph = FALSE)\n## $hopkins_stat\n## [1] 0.4642\n## \n## $plot\n## NULL"},{"path":"cluster-analysis.html","id":"k-means-on-data-without-clustering-tendency","chapter":"7 Cluster Analysis","heading":"7.5.4.5 k-means on Data Without Clustering Tendency","text":"happens perform k-means data inherent\nclustering structure?k-means discretizes space similarly sized regions.","code":"\nkm <- kmeans(data_random, centers = 4)\n\nrandom_clustered<- data_random |> \n  add_column(cluster = factor(km$cluster))\nggplot(random_clustered, aes(x = x, y = y, color = cluster)) + \n  geom_point()"},{"path":"cluster-analysis.html","id":"supervised-measures-of-cluster-validity","chapter":"7 Cluster Analysis","heading":"7.5.5 Supervised Measures of Cluster Validity","text":"Also called external cluster validation since uses ground truth information.\n, \nuser idea data grouped. known\nclass label provided clustering algorithm.use artificial data set known groups.Prepare dataFind optimal number Clusters k-meansUse within sum squares (look knee)Looks like 7 clustersHierarchical clustering: use single-link mouth \nnon-convex chaining may help.Find optimal number clustersThe maximum clearly 4 clusters.Compare ground truth corrected (=adjusted) Rand index\n(ARI),\nvariation information (VI)\nindex,\nentropy\n\npurity.cluster_stats computes ARI VI comparative measures. define\nfunctions entropy purity :calculate measures (comparison also use random “clusterings” \n4 6 clusters)Notes:Hierarchical clustering found perfect clustering.Entropy purity heavily impacted number clusters\n(clusters improve metric).corrected rand index shows clearly random clusterings\nrelationship ground truth (close 0). \nhelpful property.Read ? cluster.stats explanation available indices.","code":"\nlibrary(mlbench)\nset.seed(1234)\nshapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)\nplot(shapes)\ntruth <- as.integer(shapes$class)\nshapes <- shapes$x\ncolnames(shapes) <- c(\"x\", \"y\")\n\nshapes <- shapes |> scale() |> as_tibble()\n\nggplot(shapes, aes(x, y)) + geom_point()\nks <- 2:20\nWCSS <- sapply(ks, FUN = function(k) {\n  kmeans(shapes, centers = k, nstart = 10)$tot.withinss\n})\n\nggplot(tibble(ks, WCSS), aes(ks, WCSS)) + geom_line()\nkm <- kmeans(shapes, centers = 7, nstart = 10)\n\nggplot(shapes |> add_column(cluster = factor(km$cluster)), \n       aes(x, y, color = cluster)) +\n  geom_point()\nd <- dist(shapes)\nhc <- hclust(d, method = \"single\")\nASW <- sapply(ks, FUN = function(k) {\n  fpc::cluster.stats(d, cutree(hc, k))$avg.silwidth\n})\n\nggplot(tibble(ks, ASW), aes(ks, ASW)) + geom_line()\nhc_4 <- cutree(hc, 4)\n\nggplot(shapes |> add_column(cluster = factor(hc_4)), \n       aes(x, y, color = cluster)) +\n  geom_point()\nentropy <- function(cluster, truth) {\n  k <- max(cluster, truth)\n  cluster <- factor(cluster, levels = 1:k)\n  truth <- factor(truth, levels = 1:k)\n  w <- table(cluster)/length(cluster)\n\n  cnts <- sapply(split(truth, cluster), table)\n  p <- sweep(cnts, 1, rowSums(cnts), \"/\")\n  p[is.nan(p)] <- 0\n  e <- -p * log(p, 2)\n\n  sum(w * rowSums(e, na.rm = TRUE))\n}\n\npurity <- function(cluster, truth) {\n  k <- max(cluster, truth)\n  cluster <- factor(cluster, levels = 1:k)\n  truth <- factor(truth, levels = 1:k)\n  w <- table(cluster)/length(cluster)\n\n  cnts <- sapply(split(truth, cluster), table)\n  p <- sweep(cnts, 1, rowSums(cnts), \"/\")\n  p[is.nan(p)] <- 0\n\n  sum(w * apply(p, 1, max))\n}\nrandom_4 <- sample(1:4, nrow(shapes), replace = TRUE)\nrandom_6 <- sample(1:6, nrow(shapes), replace = TRUE)\n\nr <- rbind(\n  kmeans_7 = c(\n    unlist(fpc::cluster.stats(d, km$cluster, truth, compareonly = TRUE)),\n    entropy = entropy(km$cluster, truth),\n    purity = purity(km$cluster, truth)\n    ),\n  hc_4 = c(\n    unlist(fpc::cluster.stats(d, hc_4, truth, compareonly = TRUE)),\n    entropy = entropy(hc_4, truth),\n    purity = purity(hc_4, truth)\n    ),\n  random_4 = c(\n    unlist(fpc::cluster.stats(d, random_4, truth, compareonly = TRUE)),\n    entropy = entropy(random_4, truth),\n    purity = purity(random_4, truth)\n    ),\n  random_6 = c(\n    unlist(fpc::cluster.stats(d, random_6, truth, compareonly = TRUE)),\n    entropy = entropy(random_6, truth),\n    purity = purity(random_6, truth)\n    )\n  )\nr\n##          corrected.rand     vi entropy purity\n## kmeans_7       0.638229 0.5709  0.2286 0.4639\n## hc_4           1.000000 0.0000  0.0000 1.0000\n## random_4      -0.003235 2.6832  1.9883 0.2878\n## random_6      -0.002125 3.0763  1.7281 0.1436"},{"path":"cluster-analysis.html","id":"more-clustering-algorithms","chapter":"7 Cluster Analysis","heading":"7.6 More Clustering Algorithms*","text":"Note: methods covered Chapter 8 textbook.","code":""},{"path":"cluster-analysis.html","id":"partitioning-around-medoids-pam","chapter":"7 Cluster Analysis","heading":"7.6.1 Partitioning Around Medoids (PAM)","text":"PAM tries solve \n\\(k\\)-medoids problem. problem similar \\(k\\)-means, uses\nmedoids instead centroids represent clusters. Like hierarchical\nclustering, typically works precomputed distance matrix. \nadvantage can use distance metric just Euclidean\ndistances. Note: medoid central data point \nmiddle cluster.","code":"\nlibrary(cluster)\n\nd <- dist(ruspini_scaled)\nstr(d)\n##  'dist' num [1:2775] 0.227 1.074 3.429 2.75 2.918 ...\n##  - attr(*, \"Size\")= int 75\n##  - attr(*, \"Diag\")= logi FALSE\n##  - attr(*, \"Upper\")= logi FALSE\n##  - attr(*, \"method\")= chr \"Euclidean\"\n##  - attr(*, \"call\")= language dist(x = ruspini_scaled)\n\np <- pam(d, k = 4)\np\n## Medoids:\n##      ID   \n## [1,] 49 49\n## [2,] 46 46\n## [3,] 41 41\n## [4,] 44 44\n## Clustering vector:\n##  [1] 1 1 1 2 2 3 2 1 3 3 3 3 2 4 4 1 2 4 1 3 3 3 1 1 4 1 3 4 4 1 3 4 4\n## [34] 2 1 3 4 2 2 4 3 2 3 4 1 2 4 3 1 3 3 1 1 2 2 1 3 2 4 2 3 3 1 2 2 2\n## [67] 3 3 4 4 2 3 3 2 2\n## Objective function:\n##  build   swap \n## 0.4423 0.3187 \n## \n## Available components:\n## [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n## [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"\n\nruspini_clustered <- ruspini_scaled |> \n  add_column(cluster = factor(p$cluster))\n\nmedoids <- as_tibble(ruspini_scaled[p$medoids, ], rownames = \"cluster\")\nmedoids\n## # A tibble: 4 × 3\n##   cluster      x      y\n##   <chr>    <dbl>  <dbl>\n## 1 1        1.45   0.554\n## 2 2       -1.18  -0.555\n## 3 3       -0.357  1.17 \n## 4 4        0.463 -1.46\n\nggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + \n  geom_point() +\n  geom_point(data = medoids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)\n\n## __Note:__ `fviz_cluster` needs the original data.\nfviz_cluster(c(p, list(data = ruspini_scaled)), geom = \"point\", ellipse.type = \"norm\")"},{"path":"cluster-analysis.html","id":"gaussian-mixture-models","chapter":"7 Cluster Analysis","heading":"7.6.2 Gaussian Mixture Models","text":"Gaussian mixture\nmodels\nassume data set result drawing data set \nGaussian distributions distribution represents cluster.\nEstimation algorithms try identify location parameters \ndistributions thus can used find clusters. Mclust() uses\nBayesian Information Criterion (BIC) find number clusters\n(model selection). BIC uses likelihood penalty term guard\noverfitting.Rerun fixed number 4 clusters","code":"\nlibrary(mclust)\n## Package 'mclust' version 6.1.1\n## Type 'citation(\"mclust\")' for citing this R package in publications.\n## \n## Attaching package: 'mclust'\n## The following object is masked from 'package:purrr':\n## \n##     map\nm <- Mclust(ruspini_scaled)\nsummary(m)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEI (diagonal, equal volume and shape) model with 5\n## components: \n## \n##  log-likelihood  n df    BIC    ICL\n##          -91.26 75 16 -251.6 -251.7\n## \n## Clustering table:\n##  1  2  3  4  5 \n## 14  3 20 23 15\nplot(m, what = \"classification\")\nm <- Mclust(ruspini_scaled, G=4)\nsummary(m)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEI (diagonal, equal volume and shape) model with 4\n## components: \n## \n##  log-likelihood  n df    BIC    ICL\n##          -101.6 75 13 -259.3 -259.3\n## \n## Clustering table:\n##  1  2  3  4 \n## 17 20 23 15\nplot(m, what = \"classification\")"},{"path":"cluster-analysis.html","id":"spectral-clustering","chapter":"7 Cluster Analysis","heading":"7.6.3 Spectral clustering","text":"Spectral clustering\nworks embedding data points partitioning problem \nsubspace k largest eigenvectors normalized affinity/kernel\nmatrix. uses simple clustering method like k-means.","code":"\nlibrary(\"kernlab\")\n## \n## Attaching package: 'kernlab'\n## The following object is masked from 'package:scales':\n## \n##     alpha\n## The following object is masked from 'package:arules':\n## \n##     size\n## The following object is masked from 'package:purrr':\n## \n##     cross\n## The following object is masked from 'package:ggplot2':\n## \n##     alpha\n\ncluster_spec <- specc(as.matrix(ruspini_scaled), centers = 4)\ncluster_spec\n## Spectral Clustering object of class \"specc\" \n## \n##  Cluster memberships: \n##  \n## 3 3 3 1 1 2 1 3 2 2 2 2 4 2 2 3 1 2 3 2 2 2 3 3 2 3 2 2 2 3 2 2 2 4 3 2 2 4 1 2 2 1 2 2 3 1 2 2 3 2 2 3 3 1 4 3 2 1 2 1 2 2 3 1 1 1 2 2 2 2 1 2 2 1 1 \n##  \n## Gaussian Radial Basis kernel function. \n##  Hyperparameter : sigma =  90.5409637815977 \n## \n## Centers:  \n##          [,1]     [,2]\n## [1,] -1.03901 -0.51002\n## [2,] -0.03575  0.08266\n## [3,]  1.41944  0.46929\n## [4,] -1.53692 -0.73973\n## \n## Cluster size:  \n## [1] 16 38 17  4\n## \n## Within-cluster sum of squares:  \n## [1]  5.184 72.301 15.880  2.544\n\nggplot(ruspini_scaled |> \n         add_column(cluster = factor(cluster_spec)),\n       aes(x, y, color = cluster)) + \n  geom_point()"},{"path":"cluster-analysis.html","id":"fuzzy-c-means-clustering","chapter":"7 Cluster Analysis","heading":"7.6.4 Fuzzy C-Means Clustering","text":"fuzzy clustering\nversion k-means clustering problem. data point degree\nmembership cluster.Plot membership (shown small pie charts)","code":"\nlibrary(\"e1071\")\n\ncluster_cmeans <- cmeans(as.matrix(ruspini_scaled), centers = 4)\ncluster_cmeans\n## Fuzzy c-means clustering with 4 clusters\n## \n## Cluster centers:\n##         x       y\n## 1 -0.3763  1.1143\n## 2 -1.1371 -0.5550\n## 3  0.4552 -1.4760\n## 4  1.5048  0.5161\n## \n## Memberships:\n##               1         2         3         4\n##  [1,] 3.390e-02 0.0184315 0.0318371 9.158e-01\n##  [2,] 2.683e-02 0.0130798 0.0205431 9.395e-01\n##  [3,] 1.100e-01 0.0654667 0.1188610 7.056e-01\n##  [4,] 9.817e-02 0.8288408 0.0452756 2.771e-02\n##  [5,] 9.312e-02 0.7683796 0.0971244 4.138e-02\n##  [6,] 8.622e-01 0.0758548 0.0256694 3.626e-02\n##  [7,] 9.546e-03 0.9731667 0.0127762 4.511e-03\n##  [8,] 1.483e-02 0.0061528 0.0087250 9.703e-01\n##  [9,] 9.889e-01 0.0046259 0.0021818 4.268e-03\n## [10,] 9.753e-01 0.0114717 0.0048175 8.409e-03\n## [11,] 9.787e-01 0.0103280 0.0041355 6.879e-03\n## [12,] 9.597e-01 0.0151615 0.0078892 1.728e-02\n## [13,] 2.687e-02 0.9343464 0.0272600 1.153e-02\n## [14,] 2.343e-02 0.0384835 0.8921799 4.591e-02\n## [15,] 1.016e-02 0.0183426 0.9537436 1.775e-02\n## [16,] 2.457e-03 0.0011603 0.0018407 9.945e-01\n## [17,] 3.663e-02 0.8900877 0.0549646 1.831e-02\n## [18,] 1.136e-02 0.0249347 0.9472522 1.646e-02\n## [19,] 1.739e-01 0.1075081 0.1773901 5.412e-01\n## [20,] 9.245e-01 0.0371563 0.0146304 2.371e-02\n## [21,] 8.916e-01 0.0333605 0.0199925 5.510e-02\n## [22,] 9.396e-01 0.0204596 0.0114691 2.852e-02\n## [23,] 4.091e-02 0.0229225 0.0405208 8.957e-01\n## [24,] 7.444e-03 0.0032213 0.0047480 9.846e-01\n## [25,] 9.526e-03 0.0254625 0.9531437 1.187e-02\n## [26,] 2.177e-01 0.1283064 0.1837577 4.703e-01\n## [27,] 9.187e-01 0.0419859 0.0155199 2.380e-02\n## [28,] 1.087e-02 0.0205930 0.9503614 1.818e-02\n## [29,] 1.347e-03 0.0031668 0.9936328 1.853e-03\n## [30,] 5.072e-04 0.0002500 0.0004109 9.988e-01\n## [31,] 7.514e-01 0.0920641 0.0519039 1.047e-01\n## [32,] 9.017e-03 0.0173387 0.9591007 1.454e-02\n## [33,] 1.664e-03 0.0038610 0.9921769 2.298e-03\n## [34,] 3.001e-02 0.9148709 0.0405049 1.461e-02\n## [35,] 1.130e-02 0.0058690 0.0099612 9.729e-01\n## [36,] 9.103e-01 0.0483879 0.0168066 2.448e-02\n## [37,] 1.727e-02 0.0498112 0.9125435 2.038e-02\n## [38,] 5.521e-02 0.8604295 0.0593827 2.497e-02\n## [39,] 5.307e-02 0.8953172 0.0336329 1.798e-02\n## [40,] 1.083e-02 0.0287381 0.9470194 1.341e-02\n## [41,] 9.977e-01 0.0009642 0.0004512 8.878e-04\n## [42,] 2.538e-02 0.9426475 0.0220730 9.897e-03\n## [43,] 7.551e-01 0.0672204 0.0448198 1.329e-01\n## [44,] 5.053e-05 0.0001096 0.9997656 7.424e-05\n## [45,] 9.619e-02 0.0392619 0.0536219 8.109e-01\n## [46,] 4.470e-04 0.9989329 0.0004363 1.838e-04\n## [47,] 3.346e-03 0.0082407 0.9839904 4.422e-03\n## [48,] 9.923e-01 0.0033856 0.0014976 2.768e-03\n## [49,] 1.324e-03 0.0006092 0.0009437 9.971e-01\n## [50,] 7.016e-01 0.0713856 0.0509694 1.760e-01\n## [51,] 9.467e-01 0.0256411 0.0103562 1.729e-02\n## [52,] 1.387e-02 0.0075739 0.0135384 9.650e-01\n## [53,] 8.074e-03 0.0034561 0.0050418 9.834e-01\n## [54,] 2.248e-02 0.9302629 0.0358208 1.143e-02\n## [55,] 4.917e-02 0.8877345 0.0431078 1.999e-02\n## [56,] 1.267e-01 0.0394178 0.0461292 7.877e-01\n## [57,] 9.754e-01 0.0120663 0.0047505 7.762e-03\n## [58,] 2.256e-02 0.9540764 0.0155193 7.844e-03\n## [59,] 1.396e-03 0.0030791 0.9934965 2.029e-03\n## [60,] 1.357e-02 0.9712826 0.0102384 4.907e-03\n## [61,] 9.293e-01 0.0376080 0.0133145 1.973e-02\n## [62,] 9.272e-01 0.0247817 0.0139384 3.405e-02\n## [63,] 1.929e-02 0.0106791 0.0192351 9.508e-01\n## [64,] 3.837e-02 0.8665080 0.0740350 2.108e-02\n## [65,] 3.143e-03 0.9920954 0.0034025 1.359e-03\n## [66,] 1.710e-02 0.9626089 0.0138085 6.485e-03\n## [67,] 9.763e-01 0.0095410 0.0046349 9.540e-03\n## [68,] 9.885e-01 0.0044831 0.0022370 4.751e-03\n## [69,] 1.011e-02 0.0241498 0.9523630 1.337e-02\n## [70,] 1.111e-02 0.0203874 0.9492340 1.926e-02\n## [71,] 4.507e-02 0.9045022 0.0339790 1.645e-02\n## [72,] 9.964e-01 0.0015597 0.0007050 1.321e-03\n## [73,] 9.693e-01 0.0112508 0.0059273 1.354e-02\n## [74,] 4.260e-02 0.8727568 0.0633457 2.130e-02\n## [75,] 2.104e-02 0.9397664 0.0290866 1.010e-02\n## \n## Closest hard clustering:\n##  [1] 4 4 4 2 2 1 2 4 1 1 1 1 2 3 3 4 2 3 4 1 1 1 4 4 3 4 1 3 3 4 1 3 3\n## [34] 2 4 1 3 2 2 3 1 2 1 3 4 2 3 1 4 1 1 4 4 2 2 4 1 2 3 2 1 1 4 2 2 2\n## [67] 1 1 3 3 2 1 1 2 2\n## \n## Available components:\n## [1] \"centers\"     \"size\"        \"cluster\"     \"membership\" \n## [5] \"iter\"        \"withinerror\" \"call\"\nlibrary(\"scatterpie\")\nggplot()  +\n  geom_scatterpie(\n    data = cbind(ruspini_scaled, cluster_cmeans$membership),\n    aes(x = x, y = y), \n    cols = colnames(cluster_cmeans$membership), \n    legend_name = \"Membership\") + \n  coord_equal()"},{"path":"cluster-analysis.html","id":"outliers-in-clustering","chapter":"7 Cluster Analysis","heading":"7.7 Outliers in Clustering*","text":"clustering algorithms perform complete assignment (.e., data\npoints need assigned cluster). Outliers affect \nclustering. useful identify outliers remove strong outliers\nprior clustering. density based method identify outlier \nLOF (Local Outlier\nFactor). related dbscan compares density around point\ndensities around neighbors (specify \nneighborhood size \\(k\\)). LOF value regular data point 1. \nlarger LOF value gets, likely point outlier.Add clear outlier scaled Ruspini dataset 10 standard\ndeviations average x axis.","code":"\nlibrary(dbscan)\nruspini_scaled_outlier <- ruspini_scaled |> add_case(x=10,y=0)"},{"path":"cluster-analysis.html","id":"visual-inspection-of-the-data","chapter":"7 Cluster Analysis","heading":"7.7.1 Visual inspection of the data","text":"Outliers can identified using summary statistics, histograms,\nscatterplots (pairs plots), boxplots, etc. use pairs plot\n(diagonal contains smoothed histograms). outlier visible \nsingle separate point scatter plot long tail \nsmoothed histogram x (expect observations \nfall range \\[-3,3\\] normalized data).outlier problem k-meansThis problem can fixed increasing number clusters \nremoving small clusters post-processing step identifying \nremoving outliers clustering.","code":"\nlibrary(\"GGally\")\nggpairs(ruspini_scaled_outlier, progress = FALSE)\nkm <- kmeans(ruspini_scaled_outlier, centers = 4, nstart = 10)\nruspini_scaled_outlier_km <- ruspini_scaled_outlier|>\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"},{"path":"cluster-analysis.html","id":"local-outlier-factor-lof","chapter":"7 Cluster Analysis","heading":"7.7.2 Local Outlier Factor (LOF)","text":"Local Outlier\nFactor related\nconcepts DBSCAN can help identify potential outliers. Calculate\nLOF (choose local neighborhood size 10 density estimation),Plot points sorted increasing LOF look knee.Choose threshold 1.Analyze found outliers (might interesting data points) cluster data without .many outlier removal strategies available. See, e.g.,\npackage outliers.","code":"\nlof <- lof(ruspini_scaled_outlier, minPts= 10)\nlof\n##  [1]  1.0842  1.0062  1.3682  1.2124  1.0830  1.1750  0.9778  0.9982\n##  [9]  0.9716  0.9653  0.9903  1.0409  1.0509  1.2518  1.0262  0.9377\n## [17]  1.0198  1.0238  1.5716  1.0680  1.2088  1.0702  1.1375  1.0022\n## [25]  0.9957  1.5976  1.0340  1.0708  1.0145  0.9385  1.6258  0.9758\n## [33]  0.9972  1.0828  1.0206  1.0396  1.0214  1.2090  1.0881  0.9809\n## [41]  0.9319  1.0127  1.3973  0.9939  1.1603  0.9411  0.9928  0.9427\n## [49]  0.9329  1.4943  1.0213  1.0055  0.9920  0.9536  1.0852  1.1429\n## [57]  0.9815  1.0200  0.9864  0.9175  1.0340  1.1045  1.0041  1.0413\n## [65]  0.9825  1.0393  0.9850  1.0187  0.9625  1.0593  1.0008  0.9336\n## [73]  1.0515  1.0234  0.9767 17.8995\n\nggplot(ruspini_scaled_outlier |> add_column(lof = lof), aes(x, y, color = lof)) +\n    geom_point() + scale_color_gradient(low = \"gray\", high = \"red\")\nggplot(tibble(index = seq_len(length(lof)), lof = sort(lof)), aes(index, lof)) +\n  geom_line() +\n  geom_hline(yintercept = 1, color = \"red\", linetype = 2)\nggplot(ruspini_scaled_outlier |> add_column(outlier = lof >= 2), aes(x, y, color = outlier)) +\n  geom_point()\nruspini_scaled_clean <- ruspini_scaled_outlier  |> filter(lof < 2)\n\nkm <- kmeans(ruspini_scaled_clean, centers = 4, nstart = 10)\nruspini_scaled_clean_km <- ruspini_scaled_clean|>\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_clean_km, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"},{"path":"cluster-analysis.html","id":"exercises-4","chapter":"7 Cluster Analysis","heading":"7.8 Exercises*","text":"use Palmer penguin data exercises.Create R markdown file code discussion following .features use clustering? missing values?\nDiscuss answers. need scale data clustering? ?distance measure use reflect similarities penguins?\nSee Measures Similarity Dissimilarity Chapter 2.Apply k-means clustering. Use appropriate method determine number\nclusters. Compare clustering using unscaled data \nscaled data. difference? Visualize describe results.Apply hierarchical clustering.\nCreate dendrogram discuss means.Apply DBSCAN. choose parameters? well work?","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm\n##   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n## 1 Adelie  Torgersen           39.1          18.7               181\n## 2 Adelie  Torgersen           39.5          17.4               186\n## 3 Adelie  Torgersen           40.3          18                 195\n## 4 Adelie  Torgersen           NA            NA                  NA\n## 5 Adelie  Torgersen           36.7          19.3               193\n## 6 Adelie  Torgersen           39.3          20.6               190\n## # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"regression.html","id":"regression","chapter":"8 Regression*","heading":"8 Regression*","text":"packages used chapter : lars (Hastie Efron 2022)","code":"\npkgs <- c('lars')\n  \npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"regression.html","id":"introduction-1","chapter":"8 Regression*","heading":"8.1 Introduction","text":"Classification predicts one small set discrete labels (e.g., yes ).\nRegression also supervised learning problem, goal \npredict value continuous value given set predictors.\nstart popular linear regression later discuss alternative\nregression methods.Linear regression models value dependent variable \\(y\\) (also called\nresponse) \nlinear function independent variables \\(X_1, X_2, ..., X_p\\)\n(also called regressors, predictors, exogenous variables covariates).\nGiven \\(n\\) observations model :\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \\]\\(\\beta_0\\) intercept, \\(\\beta\\) \\(p\\)-dimensional parameter\nvector learned data, \\(\\epsilon\\) error term\n(called residuals).Linear regression makes several assumptions:Weak exogeneity: Predictor variables assumed error free.Linearity: linear relationship dependent\nindependent variables.Homoscedasticity: variance error (\\(\\epsilon\\))\nchange (increase) predicted value.Independence errors: Errors observations uncorrelated.multicollinearity predictors: Predictors \nperfectly correlated parameter vector identified. Note\nhighly correlated predictors lead unstable results \navoided.","code":""},{"path":"regression.html","id":"a-first-model","chapter":"8 Regression*","heading":"8.2 A First Model","text":"Load shuffle data (flowers order species)Make data little messy add useless featureCreate training learning dataCan predict Petal.Width using variables?lm uses formula interface see ?lm descriptionSummary shows:coefficients significantly different 0R-squared (coefficient determination): Proportion variability dependent variable explained model. better look adjusted R-square (adjusted number dependent vars.)Plotting model produces diagnostic plots (see ? plot.lm). example\ncheck homoscedasticity (residual vs predicted value scatter plot look like funnel)\nresiduals approximately normally distributed (Q-Q plot close straight line).","code":"\nset.seed(2000)\ndata(iris)\nx <- iris[sample(1:nrow(iris)),]\nplot(x, col=x$Species)\nx[,1] <- x[,1] + rnorm(nrow(x))\nx[,2] <- x[,2] + rnorm(nrow(x))\nx[,3] <- x[,3] + rnorm(nrow(x))\nx <- cbind(x[,-5], useless = mean(x[,1]) + rnorm(nrow(x)), Species = x[,5])\n\nplot(x, col=x$Species)\nsummary(x)\n##   Sepal.Length   Sepal.Width     Petal.Length     Petal.Width \n##  Min.   :2.68   Min.   :0.611   Min.   :-0.713   Min.   :0.1  \n##  1st Qu.:5.05   1st Qu.:2.306   1st Qu.: 1.876   1st Qu.:0.3  \n##  Median :5.92   Median :3.171   Median : 4.102   Median :1.3  \n##  Mean   :5.85   Mean   :3.128   Mean   : 3.781   Mean   :1.2  \n##  3rd Qu.:6.70   3rd Qu.:3.945   3rd Qu.: 5.546   3rd Qu.:1.8  \n##  Max.   :8.81   Max.   :5.975   Max.   : 7.708   Max.   :2.5  \n##     useless           Species  \n##  Min.   :2.92   setosa    :50  \n##  1st Qu.:5.23   versicolor:50  \n##  Median :5.91   virginica :50  \n##  Mean   :5.88                  \n##  3rd Qu.:6.57                  \n##  Max.   :8.11\nhead(x)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width useless\n## 85         2.980       1.464        5.227         1.5   5.712\n## 104        5.096       3.044        5.187         1.8   6.569\n## 30         4.361       2.832        1.861         0.2   4.299\n## 53         8.125       2.406        5.526         1.5   6.124\n## 143        6.372       1.565        6.147         1.9   6.553\n## 142        6.526       3.697        5.708         2.3   5.222\n##        Species\n## 85  versicolor\n## 104  virginica\n## 30      setosa\n## 53  versicolor\n## 143  virginica\n## 142  virginica\ntrain <- x[1:100,]\ntest <- x[101:150,]\nmodel1 <- lm(Petal.Width ~ Sepal.Length\n            + Sepal.Width + Petal.Length + useless,\n            data = train)\nmodel1\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + \n##     useless, data = train)\n## \n## Coefficients:\n##  (Intercept)  Sepal.Length   Sepal.Width  Petal.Length       useless  \n##     -0.20589       0.01957       0.03406       0.30814       0.00392\ncoef(model1)\n##  (Intercept) Sepal.Length  Sepal.Width Petal.Length      useless \n##    -0.205886     0.019568     0.034062     0.308139     0.003917\nsummary(model1)\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + \n##     useless, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0495 -0.2033  0.0074  0.2038  0.8564 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -0.20589    0.28723   -0.72     0.48    \n## Sepal.Length  0.01957    0.03265    0.60     0.55    \n## Sepal.Width   0.03406    0.03549    0.96     0.34    \n## Petal.Length  0.30814    0.01819   16.94   <2e-16 ***\n## useless       0.00392    0.03558    0.11     0.91    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.366 on 95 degrees of freedom\n## Multiple R-squared:  0.778,  Adjusted R-squared:  0.769 \n## F-statistic: 83.4 on 4 and 95 DF,  p-value: <2e-16\nplot(model1, which = 1:2)"},{"path":"regression.html","id":"comparing-nested-models","chapter":"8 Regression*","heading":"8.3 Comparing Nested Models","text":"try simpler modelsNo interceptVery simple modelCompare models (Null hypothesis: treatments=models effect). Note: works nested models. Models nested one model contains predictors model.Models 1 significantly better model 2. Model 2 significantly better model 3. Model 3 significantly better model 4! Use model 4 (simplest model)","code":"\nmodel2 <- lm(Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length,\n             data = train)\nsummary(model2)\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length, \n##     data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0440 -0.2024  0.0099  0.1998  0.8513 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -0.1842     0.2076   -0.89     0.38    \n## Sepal.Length   0.0199     0.0323    0.62     0.54    \n## Sepal.Width    0.0339     0.0353    0.96     0.34    \n## Petal.Length   0.3080     0.0180   17.07   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.365 on 96 degrees of freedom\n## Multiple R-squared:  0.778,  Adjusted R-squared:  0.771 \n## F-statistic:  112 on 3 and 96 DF,  p-value: <2e-16\nmodel3 <- lm(Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length - 1,\n             data = train)\nsummary(model3)\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length - \n##     1, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0310 -0.1961 -0.0051  0.2264  0.8246 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## Sepal.Length -0.00168    0.02122   -0.08     0.94    \n## Sepal.Width   0.01859    0.03073    0.61     0.55    \n## Petal.Length  0.30666    0.01796   17.07   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.364 on 97 degrees of freedom\n## Multiple R-squared:  0.938,  Adjusted R-squared:  0.936 \n## F-statistic:  486 on 3 and 97 DF,  p-value: <2e-16\nmodel4 <- lm(Petal.Width ~ Petal.Length -1,\n             data = train)\nsummary(model4)\n## \n## Call:\n## lm(formula = Petal.Width ~ Petal.Length - 1, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.9774 -0.1957  0.0078  0.2536  0.8455 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## Petal.Length  0.31586    0.00822    38.4   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.362 on 99 degrees of freedom\n## Multiple R-squared:  0.937,  Adjusted R-squared:  0.936 \n## F-statistic: 1.47e+03 on 1 and 99 DF,  p-value: <2e-16\nanova(model1, model2, model3, model4)\n## Analysis of Variance Table\n## \n## Model 1: Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + useless\n## Model 2: Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length\n## Model 3: Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length - 1\n## Model 4: Petal.Width ~ Petal.Length - 1\n##   Res.Df  RSS Df Sum of Sq    F Pr(>F)\n## 1     95 12.8                         \n## 2     96 12.8 -1   -0.0016 0.01   0.91\n## 3     97 12.9 -1   -0.1046 0.78   0.38\n## 4     99 13.0 -2   -0.1010 0.38   0.69"},{"path":"regression.html","id":"stepwise-variable-selection","chapter":"8 Regression*","heading":"8.4 Stepwise Variable Selection","text":"Automatically looks smallest AIC (Akaike information criterion)","code":"\ns1 <- step(lm(Petal.Width ~ . -Species, data=train))\n## Start:  AIC=-195.9\n## Petal.Width ~ (Sepal.Length + Sepal.Width + Petal.Length + useless + \n##     Species) - Species\n## \n##                Df Sum of Sq  RSS    AIC\n## - useless       1       0.0 12.8 -197.9\n## - Sepal.Length  1       0.0 12.8 -197.5\n## - Sepal.Width   1       0.1 12.9 -196.9\n## <none>                      12.8 -195.9\n## - Petal.Length  1      38.6 51.3  -58.7\n## \n## Step:  AIC=-197.9\n## Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length\n## \n##                Df Sum of Sq  RSS    AIC\n## - Sepal.Length  1       0.1 12.8 -199.5\n## - Sepal.Width   1       0.1 12.9 -198.9\n## <none>                      12.8 -197.9\n## - Petal.Length  1      38.7 51.5  -60.4\n## \n## Step:  AIC=-199.5\n## Petal.Width ~ Sepal.Width + Petal.Length\n## \n##                Df Sum of Sq  RSS    AIC\n## - Sepal.Width   1       0.1 12.9 -200.4\n## <none>                      12.8 -199.5\n## - Petal.Length  1      44.7 57.5  -51.3\n## \n## Step:  AIC=-200.4\n## Petal.Width ~ Petal.Length\n## \n##                Df Sum of Sq  RSS    AIC\n## <none>                      12.9 -200.4\n## - Petal.Length  1      44.6 57.6  -53.2\nsummary(s1)\n## \n## Call:\n## lm(formula = Petal.Width ~ Petal.Length, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.9848 -0.1873  0.0048  0.2466  0.8343 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    0.0280     0.0743    0.38     0.71    \n## Petal.Length   0.3103     0.0169   18.38   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.364 on 98 degrees of freedom\n## Multiple R-squared:  0.775,  Adjusted R-squared:  0.773 \n## F-statistic:  338 on 1 and 98 DF,  p-value: <2e-16"},{"path":"regression.html","id":"modeling-with-interaction-terms","chapter":"8 Regression*","heading":"8.5 Modeling with Interaction Terms","text":"two variables important together? Interaction terms\nmodeled : * formula (literally multiplied).\nSee ? formula.Model 5 significantly better model 4","code":"\nmodel5 <- step(lm(Petal.Width ~ Sepal.Length * Sepal.Width * Petal.Length,\n             data = train))\n## Start:  AIC=-196.4\n## Petal.Width ~ Sepal.Length * Sepal.Width * Petal.Length\n## \n##                                         Df Sum of Sq  RSS  AIC\n## <none>                                               11.9 -196\n## - Sepal.Length:Sepal.Width:Petal.Length  1     0.265 12.2 -196\nsummary(model5)\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length * Sepal.Width * Petal.Length, \n##     data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1064 -0.1882  0.0238  0.1767  0.8577 \n## \n## Coefficients:\n##                                       Estimate Std. Error t value\n## (Intercept)                            -2.4207     1.3357   -1.81\n## Sepal.Length                            0.4484     0.2313    1.94\n## Sepal.Width                             0.5983     0.3845    1.56\n## Petal.Length                            0.7275     0.2863    2.54\n## Sepal.Length:Sepal.Width               -0.1115     0.0670   -1.66\n## Sepal.Length:Petal.Length              -0.0833     0.0477   -1.75\n## Sepal.Width:Petal.Length               -0.0941     0.0850   -1.11\n## Sepal.Length:Sepal.Width:Petal.Length   0.0201     0.0141    1.43\n##                                       Pr(>|t|)  \n## (Intercept)                              0.073 .\n## Sepal.Length                             0.056 .\n## Sepal.Width                              0.123  \n## Petal.Length                             0.013 *\n## Sepal.Length:Sepal.Width                 0.100 .\n## Sepal.Length:Petal.Length                0.084 .\n## Sepal.Width:Petal.Length                 0.272  \n## Sepal.Length:Sepal.Width:Petal.Length    0.157  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.36 on 92 degrees of freedom\n## Multiple R-squared:  0.792,  Adjusted R-squared:  0.777 \n## F-statistic: 50.2 on 7 and 92 DF,  p-value: <2e-16\nanova(model5, model4)\n## Analysis of Variance Table\n## \n## Model 1: Petal.Width ~ Sepal.Length * Sepal.Width * Petal.Length\n## Model 2: Petal.Width ~ Petal.Length - 1\n##   Res.Df  RSS Df Sum of Sq    F Pr(>F)\n## 1     92 11.9                         \n## 2     99 13.0 -7     -1.01 1.11   0.36"},{"path":"regression.html","id":"prediction","chapter":"8 Regression*","heading":"8.6 Prediction","text":"Calculate root-mean-square error (RMSE): less betterCompare predicted vs. actual values","code":"\ntest[1:5,]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width useless\n## 128        8.017       1.541        3.515         1.8   6.110\n## 92         5.268       4.064        6.064         1.4   4.938\n## 50         5.461       4.161        1.117         0.2   6.373\n## 134        6.055       2.951        4.599         1.5   5.595\n## 8          4.900       5.096        1.086         0.2   7.270\n##        Species\n## 128  virginica\n## 92  versicolor\n## 50      setosa\n## 134  virginica\n## 8       setosa\ntest[1:5,]$Petal.Width\n## [1] 1.8 1.4 0.2 1.5 0.2\npredict(model4, test[1:5,])\n##    128     92     50    134      8 \n## 1.1104 1.9155 0.3529 1.4526 0.3429\nRMSE <- function(predicted, true) mean((predicted-true)^2)^.5\nRMSE(predict(model4, test), test$Petal.Width)\n## [1] 0.3874\nplot(test[,\"Petal.Width\"], predict(model4, test),\n  xlim=c(0,3), ylim=c(0,3), xlab = \"actual\", ylab = \"predicted\",\n  main = \"Petal.Width\")\nabline(0,1, col=\"red\")\ncor(test[,\"Petal.Width\"], predict(model4, test))\n## [1] 0.8636"},{"path":"regression.html","id":"using-nominal-variables","chapter":"8 Regression*","heading":"8.7 Using Nominal Variables","text":"Dummy coding used factors\n(.e., levels translated individual 0-1 variable).\nfirst level factors automatically used reference \nlevels presented 0-1 dummy variables called contrasts.model.matrix used internally create dummy coding.Note dummy variable species Setosa, \nused reference. often useful set reference level.\nsimple way use \nfunction relevel change factor listed first.","code":"\nlevels(train$Species)\n## [1] \"setosa\"     \"versicolor\" \"virginica\"\nhead(model.matrix(Petal.Width ~ ., data=train))\n##     (Intercept) Sepal.Length Sepal.Width Petal.Length useless\n## 85            1        2.980       1.464        5.227   5.712\n## 104           1        5.096       3.044        5.187   6.569\n## 30            1        4.361       2.832        1.861   4.299\n## 53            1        8.125       2.406        5.526   6.124\n## 143           1        6.372       1.565        6.147   6.553\n## 142           1        6.526       3.697        5.708   5.222\n##     Speciesversicolor Speciesvirginica\n## 85                  1                0\n## 104                 0                1\n## 30                  0                0\n## 53                  1                0\n## 143                 0                1\n## 142                 0                1\nmodel6 <- step(lm(Petal.Width ~ ., data=train))\n## Start:  AIC=-308.4\n## Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + useless + \n##     Species\n## \n##                Df Sum of Sq   RSS  AIC\n## - Sepal.Length  1      0.01  3.99 -310\n## - Sepal.Width   1      0.01  3.99 -310\n## - useless       1      0.02  4.00 -310\n## <none>                       3.98 -308\n## - Petal.Length  1      0.47  4.45 -299\n## - Species       2      8.78 12.76 -196\n## \n## Step:  AIC=-310.2\n## Petal.Width ~ Sepal.Width + Petal.Length + useless + Species\n## \n##                Df Sum of Sq   RSS  AIC\n## - Sepal.Width   1      0.01  4.00 -312\n## - useless       1      0.02  4.00 -312\n## <none>                       3.99 -310\n## - Petal.Length  1      0.49  4.48 -300\n## - Species       2      8.82 12.81 -198\n## \n## Step:  AIC=-311.9\n## Petal.Width ~ Petal.Length + useless + Species\n## \n##                Df Sum of Sq   RSS  AIC\n## - useless       1      0.02  4.02 -313\n## <none>                       4.00 -312\n## - Petal.Length  1      0.48  4.48 -302\n## - Species       2      8.95 12.95 -198\n## \n## Step:  AIC=-313.4\n## Petal.Width ~ Petal.Length + Species\n## \n##                Df Sum of Sq   RSS  AIC\n## <none>                       4.02 -313\n## - Petal.Length  1      0.50  4.52 -304\n## - Species       2      8.93 12.95 -200\nmodel6\n## \n## Call:\n## lm(formula = Petal.Width ~ Petal.Length + Species, data = train)\n## \n## Coefficients:\n##       (Intercept)       Petal.Length  Speciesversicolor  \n##            0.1597             0.0664             0.8938  \n##  Speciesvirginica  \n##            1.4903\nsummary(model6)\n## \n## Call:\n## lm(formula = Petal.Width ~ Petal.Length + Species, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.7208 -0.1437  0.0005  0.1254  0.5460 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         0.1597     0.0441    3.62  0.00047 ***\n## Petal.Length        0.0664     0.0192    3.45  0.00084 ***\n## Speciesversicolor   0.8938     0.0746   11.98  < 2e-16 ***\n## Speciesvirginica    1.4903     0.1020   14.61  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.205 on 96 degrees of freedom\n## Multiple R-squared:  0.93,   Adjusted R-squared:  0.928 \n## F-statistic:  427 on 3 and 96 DF,  p-value: <2e-16\n\nRMSE(predict(model6, test), test$Petal.Width)\n## [1] 0.1885\nplot(test[,\"Petal.Width\"], predict(model6, test),\n  xlim=c(0,3), ylim=c(0,3), xlab = \"actual\", ylab = \"predicted\",\n  main = \"Petal.Width\")\nabline(0,1, col=\"red\")\ncor(test[,\"Petal.Width\"], predict(model6, test))\n## [1] 0.9696"},{"path":"regression.html","id":"alternative-regression-models","chapter":"8 Regression*","heading":"8.8 Alternative Regression Models","text":"","code":""},{"path":"regression.html","id":"regression-trees","chapter":"8 Regression*","heading":"8.8.1 Regression Trees","text":"Many models use classification can also perform regression\nproduce piece-wise predictors.\nexample CART:Note: nested model linear regressions \nANOVA compare models!","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\nmodel7 <- rpart(Petal.Width ~ ., data=train,\n  control=rpart.control(cp=0.01))\nmodel7\n## n= 100 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n## 1) root 100 57.5700 1.2190  \n##   2) Species=setosa 32  0.3797 0.2469 *\n##   3) Species=versicolor,virginica 68 12.7200 1.6760  \n##     6) Species=versicolor 35  1.5350 1.3310 *\n##     7) Species=virginica 33  2.6010 2.0420 *\nrpart.plot(model7)\n\nRMSE(predict(model7, test), test$Petal.Width)\n## [1] 0.182\nplot(test[,\"Petal.Width\"], predict(model7, test),\n  xlim=c(0,3), ylim=c(0,3), xlab = \"actual\", ylab = \"predicted\",\n  main = \"Petal.Width\")\nabline(0,1, col=\"red\")\ncor(test[,\"Petal.Width\"], predict(model7, test))\n## [1] 0.9717"},{"path":"regression.html","id":"regularized-regression","chapter":"8 Regression*","heading":"8.8.2 Regularized Regression","text":"LASSO LAR try reduce number parameters using \nregularization term (see lars package lars https://en.wikipedia.org/wiki/Elastic_net_regularization)create design matrix (dummy variables interaction terms).\nlm automatically us, lars implementation\nmanually.plot shows variables added (left right model)find best model (using Mallows’s Cp statistic, see https://en.wikipedia.org/wiki/Mallows's_Cp)make predictions","code":"\nlibrary(lars)\n## Loaded lars 1.3\nx <- model.matrix(~ . + Sepal.Length*Sepal.Width*Petal.Length ,\n  data = train[, -4])\nhead(x)\n##     (Intercept) Sepal.Length Sepal.Width Petal.Length useless\n## 85            1        2.980       1.464        5.227   5.712\n## 104           1        5.096       3.044        5.187   6.569\n## 30            1        4.361       2.832        1.861   4.299\n## 53            1        8.125       2.406        5.526   6.124\n## 143           1        6.372       1.565        6.147   6.553\n## 142           1        6.526       3.697        5.708   5.222\n##     Speciesversicolor Speciesvirginica Sepal.Length:Sepal.Width\n## 85                  1                0                    4.362\n## 104                 0                1                   15.511\n## 30                  0                0                   12.349\n## 53                  1                0                   19.546\n## 143                 0                1                    9.972\n## 142                 0                1                   24.126\n##     Sepal.Length:Petal.Length Sepal.Width:Petal.Length\n## 85                     15.578                    7.650\n## 104                    26.431                   15.787\n## 30                      8.114                    5.269\n## 53                     44.900                   13.294\n## 143                    39.168                    9.620\n## 142                    37.253                   21.103\n##     Sepal.Length:Sepal.Width:Petal.Length\n## 85                                  22.80\n## 104                                 80.45\n## 30                                  22.98\n## 53                                 108.01\n## 143                                 61.30\n## 142                                137.72\ny <- train[, 4]\n\nmodel_lars <- lars(x, y)\nsummary(model_lars)\n## LARS/LASSO\n## Call: lars(x = x, y = y)\n##    Df  Rss      Cp\n## 0   1 57.6 1225.20\n## 1   2 28.1  549.74\n## 2   3 11.6  172.16\n## 3   4 10.1  140.94\n## 4   5  4.2    5.49\n## 5   6  4.1    6.08\n## 6   7  4.0    5.14\n## 7   8  3.9    6.21\n## 8   9  3.9    7.94\n## 9  10  3.9    9.87\n## 10  9  3.9    7.75\n## 11 10  3.9    9.73\n## 12 11  3.9   11.58\n## 13 10  3.9    9.54\n## 14 11  3.9   11.00\nmodel_lars\n## \n## Call:\n## lars(x = x, y = y)\n## R-squared: 0.933 \n## Sequence of LASSO moves:\n##      Petal.Length Speciesvirginica Speciesversicolor\n## Var             4                7                 6\n## Step            1                2                 3\n##      Sepal.Width:Petal.Length Sepal.Length:Sepal.Width:Petal.Length\n## Var                        10                                    11\n## Step                        4                                     5\n##      useless Sepal.Width Sepal.Length:Petal.Length Sepal.Length\n## Var        5           3                         9            2\n## Step       6           7                         8            9\n##      Sepal.Width:Petal.Length Sepal.Width:Petal.Length\n## Var                       -10                       10\n## Step                       10                       11\n##      Sepal.Length:Sepal.Width Sepal.Width Sepal.Width\n## Var                         8          -3           3\n## Step                       12          13          14\nplot(model_lars)\nplot(model_lars, plottype = \"Cp\")\nbest <- which.min(model_lars$Cp)\ncoef(model_lars, s = best)\n##                           (Intercept) \n##                             0.0000000 \n##                          Sepal.Length \n##                             0.0000000 \n##                           Sepal.Width \n##                             0.0000000 \n##                          Petal.Length \n##                             0.0603837 \n##                               useless \n##                            -0.0086551 \n##                     Speciesversicolor \n##                             0.8463786 \n##                      Speciesvirginica \n##                             1.4181850 \n##              Sepal.Length:Sepal.Width \n##                             0.0000000 \n##             Sepal.Length:Petal.Length \n##                             0.0000000 \n##              Sepal.Width:Petal.Length \n##                             0.0029688 \n## Sepal.Length:Sepal.Width:Petal.Length \n##                             0.0003151\nx_test <- model.matrix(~ . + Sepal.Length*Sepal.Width*Petal.Length ,\n  data = test[, -4])\npredict(model_lars, x_test[1:5,], s = best)\n## $s\n## 6 \n## 7 \n## \n## $fraction\n##      6 \n## 0.4286 \n## \n## $mode\n## [1] \"step\"\n## \n## $fit\n##    128     92     50    134      8 \n## 1.8237 1.5003 0.2505 1.9300 0.2439\ntest[1:5, ]$Petal.Width\n## [1] 1.8 1.4 0.2 1.5 0.2\n\nRMSE(predict(model_lars, x_test, s = best)$fit, test$Petal.Width)\n## [1] 0.1907\nplot(test[,\"Petal.Width\"],predict(model_lars, x_test, s = best)$fit,\n  xlim=c(0,3), ylim=c(0,3), xlab = \"actual\", ylab = \"predicted\",\n  main = \"Petal.Width\")\nabline(0,1, col=\"red\")\ncor(test[,\"Petal.Width\"], predict(model_lars, x_test, s = best)$fit)\n## [1] 0.9686"},{"path":"regression.html","id":"other-types-of-regression","chapter":"8 Regression*","heading":"8.8.3 Other Types of Regression","text":"Robust regression: robust violation assumptions like heteroscedasticity outliers (roblm robglm package robustbase)Generalized linear models (glm). example logistic regression discussed next chapter.Nonlinear least squares (nlm)","code":""},{"path":"regression.html","id":"exercises-5","chapter":"8 Regression*","heading":"8.9 Exercises","text":"use Palmer penguin data exercises.Create R markdown document performs following:Create linear regression model predict weight\npenguin (body_mass_g).Create linear regression model predict weight\npenguin (body_mass_g).high R-squared. mean.high R-squared. mean.variables significant, ?variables significant, ?Use stepwise variable selection remove unnecessary variables.Use stepwise variable selection remove unnecessary variables.Predict weight following new penguin:\n\nnew_penguin <- tibble(\n  species = factor(\"Adelie\", \n    levels = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")),\n  island = factor(\"Dream\", \n    levels = c(\"Biscoe\", \"Dream\", \"Torgersen\")),\n bill_length_mm = 39.8, \n bill_depth_mm = 19.1, \n flipper_length_mm = 184, \n body_mass_g = NA, \n sex = factor(\"male\", levels = c(\"female\", \"male\")), \n year = 2007\n) \nnew_penguin\n## # tibble: 1 × 8\n##   species island bill_length_mm bill_depth_mm flipper_length_mm\n##   <fct>   <fct>           <dbl>         <dbl>             <dbl>\n## 1 Adelie  Dream            39.8          19.1               184\n## # ℹ 3 variables: body_mass_g <lgl>, sex <fct>, year <dbl>Predict weight following new penguin:Create regression tree. Look tree explain .\nuse regression tree predict weight penguin.Create regression tree. Look tree explain .\nuse regression tree predict weight penguin.","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm\n##   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n## 1 Adelie  Torgersen           39.1          18.7               181\n## 2 Adelie  Torgersen           39.5          17.4               186\n## 3 Adelie  Torgersen           40.3          18                 195\n## 4 Adelie  Torgersen           NA            NA                  NA\n## 5 Adelie  Torgersen           36.7          19.3               193\n## 6 Adelie  Torgersen           39.3          20.6               190\n## # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\nnew_penguin <- tibble(\n  species = factor(\"Adelie\", \n    levels = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")),\n  island = factor(\"Dream\", \n    levels = c(\"Biscoe\", \"Dream\", \"Torgersen\")),\n bill_length_mm = 39.8, \n bill_depth_mm = 19.1, \n flipper_length_mm = 184, \n body_mass_g = NA, \n sex = factor(\"male\", levels = c(\"female\", \"male\")), \n year = 2007\n) \nnew_penguin\n## # A tibble: 1 × 8\n##   species island bill_length_mm bill_depth_mm flipper_length_mm\n##   <fct>   <fct>           <dbl>         <dbl>             <dbl>\n## 1 Adelie  Dream            39.8          19.1               184\n## # ℹ 3 more variables: body_mass_g <lgl>, sex <fct>, year <dbl>"},{"path":"logistic-regression-1.html","id":"logistic-regression-1","chapter":"9 Logistic Regression*","heading":"9 Logistic Regression*","text":"chapter uses R’s base functionality need extra packages.","code":""},{"path":"logistic-regression-1.html","id":"introduction-2","chapter":"9 Logistic Regression*","heading":"9.1 Introduction","text":"Logistic regression contains word regression, actually \nprobabilistic statistical classification\nmodel predict binary outcome (probability) given set features.\npowerful model can fit quickly. one \nfirst classification models try new data.Logistic regression can thought linear regression \nlog odds ratio (logit)\nbinary outcome dependent variable:\\[logit(p) = ln(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\]equivalent modeling probability outcome \\(p\\) \\[ p = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...}}{1 +  e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...}} = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...)}}\\]","code":"\nlogit  <- function(p) log(p/(1-p))\nx <- seq(0, 1, length.out = 100)\nplot(x, logit(x), type = \"l\")\nabline(v=0.5, lty = 2)\nabline(h=0, lty = 2)"},{"path":"logistic-regression-1.html","id":"data-preparation-1","chapter":"9 Logistic Regression*","heading":"9.2 Data Preparation","text":"Load shuffle data. also add useless variable see logistic regression removes .Make Species binary classification problem \nclassify flower species Virginica","code":"\ndata(iris)\nx <- iris[sample(1:nrow(iris)),]\nx <- cbind(x, useless = rnorm(nrow(x)))\nx$virginica <- x$Species == \"virginica\"\nx$Species <- NULL\nplot(x, col=x$virginica+1)"},{"path":"logistic-regression-1.html","id":"create-a-logistic-regression-model","chapter":"9 Logistic Regression*","heading":"9.3 Create a Logistic Regression Model","text":"Logistic regression generalized linear model (GLM) logit \nlink function binomial error model.warning: glm.fit: fitted probabilities numerically 0 1 occurred means data possibly linearly separable.Check features significant?AIC can used model selection","code":"\nmodel <- glm(virginica ~ .,\n  family = binomial(logit), data=x)\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nmodel\n## \n## Call:  glm(formula = virginica ~ ., family = binomial(logit), data = x)\n## \n## Coefficients:\n##  (Intercept)  Sepal.Length   Sepal.Width  Petal.Length   Petal.Width  \n##       -47.72         -2.31         -7.96         10.11         20.87  \n##      useless  \n##        -0.39  \n## \n## Degrees of Freedom: 149 Total (i.e. Null);  144 Residual\n## Null Deviance:       191 \n## Residual Deviance: 11.7  AIC: 23.7\nsummary(model)\n## \n## Call:\n## glm(formula = virginica ~ ., family = binomial(logit), data = x)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)  \n## (Intercept)   -47.721     31.513   -1.51    0.130  \n## Sepal.Length   -2.309      2.311   -1.00    0.318  \n## Sepal.Width    -7.956      6.019   -1.32    0.186  \n## Petal.Length   10.109      5.251    1.92    0.054 .\n## Petal.Width    20.872     13.066    1.60    0.110  \n## useless        -0.390      0.999   -0.39    0.696  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 190.954  on 149  degrees of freedom\n## Residual deviance:  11.738  on 144  degrees of freedom\n## AIC: 23.74\n## \n## Number of Fisher Scoring iterations: 12"},{"path":"logistic-regression-1.html","id":"stepwise-variable-selection-1","chapter":"9 Logistic Regression*","heading":"9.4 Stepwise Variable Selection","text":"estimates (\\(\\beta_0, \\beta_1,...\\) ) \nlog-odds can converted odds using \\(exp(\\beta)\\).\nnegative log-odds ratio means odds go increase \nvalue predictor. predictor \npositive log-odds ratio increases odds. case, odds \nlooking Virginica iris goes Sepal.Width increases \ntwo predictors.","code":"\nmodel2 <- step(model, data = x)\n## Start:  AIC=23.74\n## virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + \n##     useless\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n##                Df Deviance  AIC\n## - useless       1     11.9 21.9\n## - Sepal.Length  1     13.0 23.0\n## <none>                11.7 23.7\n## - Sepal.Width   1     15.2 25.2\n## - Petal.Width   1     23.0 33.0\n## - Petal.Length  1     25.5 35.5\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## \n## Step:  AIC=21.9\n## virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n##                Df Deviance  AIC\n## - Sepal.Length  1     13.3 21.3\n## <none>                11.9 21.9\n## - Sepal.Width   1     15.5 23.5\n## - Petal.Width   1     23.8 31.8\n## - Petal.Length  1     25.9 33.9\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## \n## Step:  AIC=21.27\n## virginica ~ Sepal.Width + Petal.Length + Petal.Width\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n##                Df Deviance  AIC\n## <none>                13.3 21.3\n## - Sepal.Width   1     20.6 26.6\n## - Petal.Length  1     27.4 33.4\n## - Petal.Width   1     31.5 37.5\nsummary(model2)\n## \n## Call:\n## glm(formula = virginica ~ Sepal.Width + Petal.Length + Petal.Width, \n##     family = binomial(logit), data = x)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)  \n## (Intercept)    -50.53      23.99   -2.11    0.035 *\n## Sepal.Width     -8.38       4.76   -1.76    0.079 .\n## Petal.Length     7.87       3.84    2.05    0.040 *\n## Petal.Width     21.43      10.71    2.00    0.045 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 190.954  on 149  degrees of freedom\n## Residual deviance:  13.266  on 146  degrees of freedom\n## AIC: 21.27\n## \n## Number of Fisher Scoring iterations: 12"},{"path":"logistic-regression-1.html","id":"calculate-the-response","chapter":"9 Logistic Regression*","heading":"9.5 Calculate the Response","text":"Note: -sample testing data learned data\n. get generalization error estimate use test set \ncross-validation!","code":"\npr <- predict(model2, x, type=\"response\")\nround(pr, 2)\n##   39  137  136    1   57  135   30   23  148  150   43  139  144   91 \n## 0.00 1.00 1.00 0.00 0.00 0.86 0.00 0.00 1.00 0.96 0.00 0.67 1.00 0.00 \n##   61   55   72  125   97  111   24   77  143  112   78   56  102   59 \n## 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 1.00 1.00 0.54 0.00 1.00 0.00 \n##  122   98   68   58  133    9   96  114   71   41  117   31   10   75 \n## 1.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.28 0.00 1.00 0.00 0.00 0.00 \n##   53  110   73   36   66   79  119   26  107   51   49   92   29   94 \n## 0.00 1.00 0.32 0.00 0.00 0.00 1.00 0.00 0.60 0.00 0.00 0.00 0.00 0.00 \n##   99   85  128   40   37  129  126   86    5   18   87  131  116   65 \n## 0.00 0.00 0.82 0.00 0.00 1.00 1.00 0.00 0.00 0.00 0.00 1.00 1.00 0.00 \n##   35   62  115   38  130  149  109   32  146  140  108   50  121   88 \n## 0.00 0.00 1.00 0.00 0.99 1.00 1.00 0.00 1.00 1.00 1.00 0.00 1.00 0.00 \n##   70   90  105  132  118    7   15  147  141   42  127  100   74   19 \n## 0.00 0.00 1.00 1.00 1.00 0.00 0.00 1.00 1.00 0.00 0.92 0.00 0.00 0.00 \n##   44  134   81   28    8  142   83   95   47   13   17   63    2   12 \n## 0.00 0.16 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n##   84   21  104  138   27   20   64   25   76   60   11  101   93  145 \n## 0.79 0.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 \n##   33   52  113   80  123   67   48   82   69   46    4   54   14   34 \n## 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00 \n##   22  106  120    3   45  103   89  124   16    6 \n## 0.00 1.00 0.93 0.00 0.00 1.00 0.00 0.98 0.00 0.00\nhist(pr, breaks=20)\nhist(pr[x$virginica==TRUE], col=\"red\", breaks=20, add=TRUE)"},{"path":"logistic-regression-1.html","id":"check-classification-performance","chapter":"9 Logistic Regression*","heading":"9.6 Check Classification Performance","text":"calculate predicted class checking probability larger \n.5.Now er can create confusion table calculate accuracy.can also use caret’s advanced function confusionMatrix(). code\nuses logical vectors.\nfoo caret, need make sure , reference predictions\ncoded factor.see model performs well high accuracy kappa value.","code":"\npred <- pr > .5\ntbl <- table(actual = x$virginica, predicted = pr>.5)\ntbl\n##        predicted\n## actual  FALSE TRUE\n##   FALSE    98    2\n##   TRUE      1   49\n\nsum(diag(tbl))/sum(tbl)\n## [1] 0.98\ncaret::confusionMatrix(\n  reference = factor(x$virginica, levels = c(TRUE, FALSE)), \n  data = factor(pr>.5, levels = c(TRUE, FALSE)))\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction TRUE FALSE\n##      TRUE    49     2\n##      FALSE    1    98\n##                                         \n##                Accuracy : 0.98          \n##                  95% CI : (0.943, 0.996)\n##     No Information Rate : 0.667         \n##     P-Value [Acc > NIR] : <2e-16        \n##                                         \n##                   Kappa : 0.955         \n##                                         \n##  Mcnemar's Test P-Value : 1             \n##                                         \n##             Sensitivity : 0.980         \n##             Specificity : 0.980         \n##          Pos Pred Value : 0.961         \n##          Neg Pred Value : 0.990         \n##              Prevalence : 0.333         \n##          Detection Rate : 0.327         \n##    Detection Prevalence : 0.340         \n##       Balanced Accuracy : 0.980         \n##                                         \n##        'Positive' Class : TRUE          \n## "},{"path":"logistic-regression-1.html","id":"exercises-6","chapter":"9 Logistic Regression*","heading":"9.7 Exercises","text":"use Palmer penguin data exercises.Create R markdown document performs following:Create test training data set (see section Holdout Method Chapter 3).Create logistic regression using training set predict variable sex.Use stepwise variable selection. variables selected?parameters selected features tell ?Predict sex penguins test set. Create \nconfusion table calculate accuracy discuss well model works.","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_length_mm\n##   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n## 1 Adelie  Torgersen           39.1          18.7               181\n## 2 Adelie  Torgersen           39.5          17.4               186\n## 3 Adelie  Torgersen           40.3          18                 195\n## 4 Adelie  Torgersen           NA            NA                  NA\n## 5 Adelie  Torgersen           36.7          19.3               193\n## 6 Adelie  Torgersen           39.3          20.6               190\n## # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Agrawal, Rakesh, Tomasz Imielinski, Arun Swami. 1993. “Mining Association Rules Sets Items Large Databases.” Proceedings 1993 Acm Sigmod International Conference Management Data, 207–16. Washington, D.C., United States: ACM Press.Allaire, JJ, François Chollet. 2024. Keras: R Interface Keras. https://tensorflow.rstudio.com/.Bates, Douglas, Martin Maechler, Mikael Jagan. 2024. Matrix: Sparse Dense Matrix Classes Methods. https://Matrix.R-forge.R-project.org.Blake, Catherine L., Christopher J. Merz. 1998. UCI Repository Machine Learning Databases. Irvine, CA: University California, Irvine, Department Information; Computer Sciences.Breiman, Leo, Adele Cutler, Andy Liaw, Matthew Wiener. 2022. RandomForest: Breiman Cutler’s Random Forests Classification Regression. https://www.stat.berkeley.edu/~breiman/RandomForests/.Carr, Dan, Nicholas Lewin-Koh, Martin Maechler. 2023. Hexbin: Hexagonal Binning Routines. https://github.com/edzer/hexbin.Chen, Tianqi, Tong , Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2024. Xgboost: Extreme Gradient Boosting. https://github.com/dmlc/xgboost.Chen, Ying-Ju, Fadel M. Megahed, L. Allison Jones-Farmer, Steven E. Rigdon. 2023. Basemodels: Baseline Models Classification Regression. https://github.com/Ying-Ju/basemodels.Fraley, Chris, Adrian E. Raftery, Luca Scrucca. 2024. Mclust: Gaussian Mixture Modelling Model-Based Clustering, Classification, Density Estimation. https://mclust-org.github.io/mclust/.Grolemund, Garrett, Hadley Wickham. 2011. “Dates Times Made Easy lubridate.” Journal Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.Hahsler, Michael. 2017a. “Experimental Comparison Seriation Methods One-Mode Two-Way Data.” European Journal Operational Research 257 (1): 133–43. https://doi.org/10.1016/j.ejor.2016.08.066.———. 2017b. “ArulesViz: Interactive Visualization Association Rules R.” R Journal 9 (2): 163–75. https://doi.org/10.32614/RJ-2017-047.———. 2021. R Companion Introduction Data Mining. Online Book. https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book.———. 2024. ArulesViz: Visualizing Association Rules Frequent Itemsets. https://github.com/mhahsler/arulesViz.Hahsler, Michael, Christian Buchta, Bettina Gruen, Kurt Hornik. 2023. Arules: Mining Association Rules Frequent Itemsets. https://github.com/mhahsler/arules.Hahsler, Michael, Christian Buchta, Kurt Hornik. 2024. Seriation: Infrastructure Ordering Objects Using Seriation. https://github.com/mhahsler/seriation.Hahsler, Michael, Sudheer Chelluboina, Kurt Hornik, Christian Buchta. 2011. “Arules R-Package Ecosystem: Analyzing Interesting Patterns Large Transaction Datasets.” Journal Machine Learning Research 12: 1977–81. https://jmlr.csail.mit.edu/papers/v12/hahsler11a.html.Hahsler, Michael, Bettina Gruen, Kurt Hornik. 2005. “Arules – Computational Environment Mining Association Rules Frequent Item Sets.” Journal Statistical Software 14 (15): 1–25. https://doi.org/10.18637/jss.v014.i15.Hahsler, Michael, Bettina Grün, Kurt Hornik. 2005. “Arules – Computational Environment Mining Association Rules Frequent Item Sets.” Journal Statistical Software 14 (15): 1–25. http://www.jstatsoft.org/v14/i15/.Hahsler, Michael, Kurt Hornik, Christian Buchta. 2008. “Getting Things Order: Introduction R Package Seriation.” Journal Statistical Software 25 (3): 1–34. https://doi.org/10.18637/jss.v025.i03.Hahsler, Michael, Matthew Piekenbrock. 2024. Dbscan: Density-Based Spatial Clustering Applications Noise (Dbscan) Related Algorithms. https://github.com/mhahsler/dbscan.Hahsler, Michael, Matthew Piekenbrock, Derek Doran. 2019. “dbscan: Fast Density-Based Clustering R.” Journal Statistical Software 91 (1): 1–30. https://doi.org/10.18637/jss.v091.i01.Hastie, Trevor, Brad Efron. 2022. Lars: Least Angle Regression, Lasso Forward Stagewise. https://doi.org/10.1214/009053604000000067.Hennig, Christian. 2024. Fpc: Flexible Procedures Clustering. https://www.unibo./sitoweb/christian.hennig/en/.Hornik, Kurt. 2023. RWeka: R/Weka Interface. https://CRAN.R-project.org/package=RWeka.Hornik, Kurt, Christian Buchta, Achim Zeileis. 2009. “Open-Source Machine Learning: R Meets Weka.” Computational Statistics 24 (2): 225–32. https://doi.org/10.1007/s00180-008-0119-7.Horst, Allison, Alison Hill, Kristen Gorman. 2022. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/.Hothorn, Torsten, Peter Buehlmann, Sandrine Dudoit, Annette Molinaro, Mark Van Der Laan. 2006. “Survival Ensembles.” Biostatistics 7 (3): 355–73.Hothorn, Torsten, Kurt Hornik, Carolin Strobl, Achim Zeileis. 2024. Party: Laboratory Recursive Partytioning. http://party.R-forge.R-project.org.Hothorn, Torsten, Kurt Hornik, Achim Zeileis. 2006. “Unbiased Recursive Partitioning: Conditional Inference Framework.” Journal Computational Graphical Statistics 15 (3): 651–74. https://doi.org/10.1198/106186006X133933.Karatzoglou, Alexandros, Alex Smola, Kurt Hornik. 2023. Kernlab: Kernel-Based Machine Learning Lab. https://CRAN.R-project.org/package=kernlab.Karatzoglou, Alexandros, Alex Smola, Kurt Hornik, Achim Zeileis. 2004. “Kernlab – S4 Package Kernel Methods R.” Journal Statistical Software 11 (9): 1–20. https://doi.org/10.18637/jss.v011.i09.Kassambara, Alboukadel. 2023. Ggcorrplot: Visualization Correlation Matrix Using Ggplot2. http://www.sthda.com/english/wiki/ggcorrplot-visualization---correlation-matrix-using-ggplot2.Kassambara, Alboukadel, Fabian Mundt. 2020. Factoextra: Extract Visualize Results Multivariate Data Analyses. http://www.sthda.com/english/rpkgs/factoextra.Kuhn, Max. 2023. Caret: Classification Regression Training. https://github.com/topepo/caret/.Kuhn, Max, Ross Quinlan. 2023. C50: C5.0 Decision Trees Rule-Based Models. https://topepo.github.io/C5.0/.Kuhn, Max. 2008. “Building Predictive Models R Using Caret Package.” Journal Statistical Software 28 (5): 1–26. https://doi.org/10.18637/jss.v028.i05.Leisch, Friedrich, Evgenia Dimitriadou. 2024. Mlbench: Machine Learning Benchmark Problems. https://CRAN.R-project.org/package=mlbench.Liaw, Andy, Matthew Wiener. 2002. “Classification Regression randomForest.” R News 2 (3): 18–22. https://CRAN.R-project.org/doc/Rnews/.Maechler, Martin, Peter Rousseeuw, Anja Struyf, Mia Hubert. 2023. Cluster: \"Finding Groups Data\": Cluster Analysis Extended Rousseeuw et Al. https://svn.r-project.org/R-packages/trunk/cluster/.Meyer, David, Christian Buchta. 2022. Proxy: Distance Similarity Measures. https://CRAN.R-project.org/package=proxy.Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, Friedrich Leisch. 2023. E1071: Misc Functions Department Statistics, Probability Theory Group (Formerly: E1071), Tu Wien. https://CRAN.R-project.org/package=e1071.Milborrow, Stephen. 2024. Rpart.plot: Plot Rpart Models: Enhanced Version Plot.rpart. http://www.milbo.org/rpart-plot/index.html.Müller, Kirill, Hadley Wickham. 2023. Tibble: Simple Data Frames. https://tibble.tidyverse.org/.R Core Team. 2024. R: Language Environment Statistical Computing. Vienna, Austria: R Foundation Statistical Computing. https://www.R-project.org/.Ripley, Brian. 2023a. MASS: Support Functions Datasets Venables Ripley’s Mass. http://www.stats.ox.ac.uk/pub/MASS4/.———. 2023b. Nnet: Feed-Forward Neural Networks Multinomial Log-Linear Models. http://www.stats.ox.ac.uk/pub/MASS4/.Robin, Xavier, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez, Markus Müller. 2011. “PROC: Open-Source Package R S+ Analyze Compare Roc Curves.” BMC Bioinformatics 12: 77.———. 2023. PROC: Display Analyze Roc Curves. https://xrobin.github.io/pROC/.Roever, Christian, Nils Raabe, Karsten Luebke, Uwe Ligges, Gero Szepannek, Marc Zentgraf, David Meyer. 2023. KlaR: Classification Visualization. https://statistik.tu-dortmund.de.Romanski, Piotr, Lars Kotthoff, Patrick Schratz. 2023. FSelector: Selecting Attributes. https://github.com/larskotthoff/fselector.Sarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization R. New York: Springer. http://lmdvr.r-forge.r-project.org.———. 2023. Lattice: Trellis Graphics R. https://lattice.r-forge.r-project.org/.Schloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, Jason Crowley. 2024. GGally: Extension Ggplot2. https://ggobi.github.io/ggally/.Scrucca, Luca, Chris Fraley, T. Brendan Murphy, Adrian E. Raftery. 2023. Model-Based Clustering, Classification, Density Estimation Using mclust R. Chapman; Hall/CRC. https://doi.org/10.1201/9781003277965.Sievert, Carson. 2020. Interactive Web-Based Data Visualization R, Plotly, Shiny. Chapman; Hall/CRC. https://plotly-r.com.Sievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, Pedro Despouy. 2024. Plotly: Create Interactive Web Graphics via Plotly.js. https://plotly-r.com.Spinu, Vitalie, Garrett Grolemund, Hadley Wickham. 2023. Lubridate: Make Dealing Dates Little Easier. https://lubridate.tidyverse.org.Strobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, Achim Zeileis. 2008. “Conditional Variable Importance Random Forests.” BMC Bioinformatics 9 (307). https://doi.org/10.1186/1471-2105-9-307.Strobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, Torsten Hothorn. 2007. “Bias Random Forest Variable Importance Measures: Illustrations, Sources Solution.” BMC Bioinformatics 8 (25). https://doi.org/10.1186/1471-2105-8-25.Tan, Pang-Ning, Michael S. Steinbach, Anuj Karpatne, Vipin Kumar. 2017. Introduction Data Mining. 2nd Edition. Pearson. https://www-users.cs.umn.edu/~kumar001/dmbook.Tan, Pang-Ning, Michael S. Steinbach, Vipin Kumar. 2005. Introduction Data Mining. 1st Edition. Addison-Wesley. https://www-users.cs.umn.edu/~kumar001/dmbook/firsted.php.Therneau, Terry, Beth Atkinson. 2023. Rpart: Recursive Partitioning Regression Trees. https://github.com/bethatkinson/rpart.Tillé, Yves, Alina Matei. 2023. Sampling: Survey Sampling. https://CRAN.R-project.org/package=sampling.Venables, W. N., B. D. Ripley. 2002a. Modern Applied Statistics S. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.———. 2002b. Modern Applied Statistics S. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.Venables, W. N., D. M. Smith, R Core Team. 2021. Introduction R.Weihs, Claus, Uwe Ligges, Karsten Luebke, Nils Raabe. 2005. “KlaR Analyzing German Business Cycles.” Data Analysis Decision Support, edited D. Baier, R. Decker, L. Schmidt-Thieme, 335–43. Berlin: Springer-Verlag.Wickham, Hadley. 2016. Ggplot2: Elegant Graphics Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.———. 2023a. Forcats: Tools Working Categorical Variables (Factors). https://forcats.tidyverse.org/.———. 2023b. Stringr: Simple, Consistent Wrappers Common String Operations. https://stringr.tidyverse.org.———. 2023c. Tidyverse: Easily Install Load Tidyverse. https://tidyverse.tidyverse.org.Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome tidyverse.” Journal Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.Wickham, Hadley, Mine Çetinkaya-Rundel, Garrett Grolemund. 2023. R Data Science: Import, Tidy, Transform, Visualize, Model Data. 2st ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/.Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using Grammar Graphics. https://ggplot2.tidyverse.org.Wickham, Hadley, Romain François, Lionel Henry, Kirill Müller, Davis Vaughan. 2023. Dplyr: Grammar Data Manipulation. https://dplyr.tidyverse.org.Wickham, Hadley, Lionel Henry. 2023. Purrr: Functional Programming Tools. https://purrr.tidyverse.org/.Wickham, Hadley, Jim Hester, Jennifer Bryan. 2024. Readr: Read Rectangular Text Data. https://readr.tidyverse.org.Wickham, Hadley, Thomas Lin Pedersen, Dana Seidel. 2023. Scales: Scale Functions Visualization. https://scales.r-lib.org.Wickham, Hadley, Davis Vaughan, Maximilian Girlich. 2024. Tidyr: Tidy Messy Data. https://tidyr.tidyverse.org.Wilkinson, Leland. 2005. Grammar Graphics (Statistics Computing). Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.Witten, Ian H., Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools Techniques. 2nd ed. San Francisco: Morgan Kaufmann.Yu, Guangchuang. 2024. Scatterpie: Scatter Pie Plot. https://CRAN.R-project.org/package=scatterpie.Zeileis, Achim, Torsten Hothorn, Kurt Hornik. 2008. “Model-Based Recursive Partitioning.” Journal Computational Graphical Statistics 17 (2): 492–514. https://doi.org/10.1198/106186008X319331.","code":""}]
