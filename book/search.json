[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"companion book contains documented R examples accompany several chapters\npopular data mining textbook Introduction Data\nMining Pang-Ning\nTan, Michael Steinbach, Anuj Karpatne Vipin Kumar.\nintended replacement textbook since cover \ntheory, guide accompanying textbook.\ncompanion\nbook can used either edition: 1st edition (Tan, Steinbach, Kumar 2005) 2nd\nedition (Tan et al. 2017). sections numbered match 2nd edition. Sections\nmarked asterisk additional content covered \ntextbook.code examples collected book developed course\nCS 5/7331 Data Mining taught\nadvanced undergraduate graduate level\nComputer Science Department \nSouthern Methodist University (SMU) since Spring 2013 regularly\nupdated improved.\nlearning method used book learning--.\ncode examples throughout book \nwritten self-contained manner can copy paste portion code,\ntry provided dataset apply directly \ndata. Instructors can use companion component create \nintroduction data mining course advanced undergraduates graduate\nstudents proficient programming basic statistics knowledge.\nlatest version book (html PDF)\ncomplete set lecture slides (PDF PowerPoint)\nprovided \nbook’s GitHub page.latest update includes use popular\npackages meta-package tidyverse (Wickham 2023b) including\nggplot2 (Wickham et al. 2025) data wrangling visualization, along \ncaret (Kuhn 2024) model building evaluation.\nPlease use edit function within book visit book’s\nGitHub project\npage\nsubmit corrections suggest improvements.cite book, use:Michael Hahsler (2024). R Companion Introduction Data\nMining. figshare.\nDOI: 10.6084/m9.figshare.26750404\nURL: https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/hope book helps learn use R efficiently \ndata mining projects.Michael Hahsler© 2024 Michael Hahsler. book licensed Creative Commons\nAttribution 4.0 International\nLicense.cover art based \n“rocks” \nstebulus licensed CC\n\n2.0.book built Mon Nov 10 11:25:07 2025 (latest GitHub tag: 1.0.3)","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Data mining goal \nfinding patterns large data sets. popular data mining textbook\nIntroduction Data\nMining (Tan et al. 2017)\ncovers many important aspects data mining. companion contains\nannotated R code examples complement textbook. make following\nalong easier, follow chapters data mining textbook \norganized main data mining tasks:Data covers types data. also include data preparation \nexploratory data analysis Appendix Data Exploration Visualization.Data covers types data. also include data preparation \nexploratory data analysis Appendix Data Exploration Visualization.Classification: Basic Concepts introduces purpose \nclassification, basic classifiers using decision trees,\nmodel training evaluation.Classification: Basic Concepts introduces purpose \nclassification, basic classifiers using decision trees,\nmodel training evaluation.Classification: Alternative Techniques introduces compares\nmethods including rule-based classifiers, nearest neighbor\nclassifiers, naive Bayes classifier, logistic regression \nartificial neural networks.Classification: Alternative Techniques introduces compares\nmethods including rule-based classifiers, nearest neighbor\nclassifiers, naive Bayes classifier, logistic regression \nartificial neural networks.Association Analysis: Basic Concepts covers algorithms \nfrequent itemset association rule generation analysis\nincluding visualization.Association Analysis: Basic Concepts covers algorithms \nfrequent itemset association rule generation analysis\nincluding visualization.Association Analysis: Advanced Concepts covers categorical continuous\nattributes, concept hierarchies, frequent sequence pattern mining.Association Analysis: Advanced Concepts covers categorical continuous\nattributes, concept hierarchies, frequent sequence pattern mining.Cluster Analysis discusses clustering approaches including\nk-means, hierarchical clustering, DBSCAN evaluate\nclustering results.Cluster Analysis discusses clustering approaches including\nk-means, hierarchical clustering, DBSCAN evaluate\nclustering results.completeness, added sections Regression \nLogistic Regression Appendix.Sections names followed asterisk\ncontain code examples methods included data\nmining textbook.book assumes \nfamiliar basics R, run R code, install packages.\nrest chapter provide overview point \ncan learn R used packages.","code":""},{"path":"introduction.html","id":"used-software","chapter":"1 Introduction","heading":"1.1 Used Software","text":"use book, need current version \nR RStudio\nDesktop installed.book chapter use set packages must installed. \ninstallation code can found beginning chapter. \ncode install packages used chapter:code examples book use R package collection tidyverse\n(Wickham 2023b) manipulate data. Tidyverse also includes package\nggplot2 (Wickham et al. 2025) visualization. Tidyverse packages make\nworking data R convenient. Data analysis data mining\nreports typically done creating\nR Markdown documents. Everything R built top \ncore R programming language packages automatically\ninstalled R. referred Base-R.","code":"\npkgs <- c('tidyverse')\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"introduction.html","id":"base-r","chapter":"1 Introduction","heading":"1.2 Base-R","text":"Base-R covered detail Introduction \nR. \nimportant differences R many programming languages\nlike Python :R functional programming language (extensions).R uses vectors operations vectorized. rarely see loops.R starts indexing vectors 1 0.R uses <- assignment. use = assignment.","code":""},{"path":"introduction.html","id":"vectors","chapter":"1 Introduction","heading":"1.2.1 Vectors","text":"basic data structure R vector real numbers called numeric.\nScalars exist, just vectors length 1.can combine values vector using combine function c().\nSpecial values infinity (Inf) missing values (NA) can used.often use sequences. simple sequence integers can produced using\ncolon operator form :.complicated sequences can created using seq().","code":"\nx <- c(10.4, 5.6, Inf, NA, 21.7)\nx\n## [1] 10.4  5.6  Inf   NA 21.7\n3:10\n## [1]  3  4  5  6  7  8  9 10\ny <- seq(from = 0, to = 1, length.out = 5)\ny\n## [1] 0.00 0.25 0.50 0.75 1.00"},{"path":"introduction.html","id":"vectorized-operations","chapter":"1 Introduction","heading":"1.2.2 Vectorized Operations","text":"Operations vectorized applied element,\nloops typically necessary R.Comparisons also vectorized performed element-wise. \nreturn logical vector (R’s name datatype Boolean).","code":"\nx + 1\n## [1] 11.4  6.6  Inf   NA 22.7\nx > y\n## [1] TRUE TRUE TRUE   NA TRUE"},{"path":"introduction.html","id":"subsetting","chapter":"1 Introduction","heading":"1.2.3 Subsetting","text":"can select vector elements using [ operator like \nprogramming languages index always starts 1.can select elements 1 3 using index sequence.Negative indices remove elements. can select first element.can use function creates logical vector subsetting.\nselect non-missing values using function .na().can also assign values selection. example, code gets rid \ninfinite values.","code":"\nx[1:3]\n## [1] 10.4  5.6  Inf\nx[-1]\n## [1]  5.6  Inf   NA 21.7\nis.na(x)\n## [1] FALSE FALSE FALSE  TRUE FALSE\nx[!is.na(x)] # select all non-missing values\n## [1] 10.4  5.6  Inf 21.7\nx[!is.finite(x)] <- NA\nx\n## [1] 10.4  5.6   NA   NA 21.7"},{"path":"introduction.html","id":"names","chapter":"1 Introduction","heading":"1.2.4 Names","text":"Another useful thing vectors many data structures R support\nnames. example, can store count cars different colors \nfollowing way.Names can used subsetting remember entry\nassociated color.","code":"\ncounts <- c(12, 5, 18, 13)\nnames(counts) <- c(\"black\", \"red\", \"white\", \"silver\")\n\ncounts\n##  black    red  white silver \n##     12      5     18     13\ncounts[c(\"red\", \"silver\")]\n##    red silver \n##      5     13"},{"path":"introduction.html","id":"lists","chapter":"1 Introduction","heading":"1.2.5 Lists","text":"Lists can store sequence elements different datatypes.Elements can accessed index name.Many functions R return list.","code":"\nlst <- list(name = \"Fred\", spouse = \"Mary\", no.children = 3,\n  child.ages = c(4, 7, 9))\nlst\n## $name\n## [1] \"Fred\"\n## \n## $spouse\n## [1] \"Mary\"\n## \n## $no.children\n## [1] 3\n## \n## $child.ages\n## [1] 4 7 9\nlst[[2]]\n## [1] \"Mary\"\nlst$child.ages\n## [1] 4 7 9"},{"path":"introduction.html","id":"data-frames","chapter":"1 Introduction","heading":"1.2.6 Data Frames","text":"data frame looks like spread sheet represents matrix-like structure\nrows columns different columns can contain different data types.Data frames stored list columns. can access elements using\nmatrix subsetting (missing indices mean whole row column) list\nsubsetting. examplesData frames important way represent data data mining.","code":"\ndf <-  data.frame(name = c(\"Michael\", \"Mark\", \"Maggie\"), \n                  children = c(2, 0, 2), age = c(40, 30, 50))\ndf\n##      name children age\n## 1 Michael        2  40\n## 2    Mark        0  30\n## 3  Maggie        2  50\ndf[1,1]\n## [1] \"Michael\"\ndf[, 2]\n## [1] 2 0 2\ndf$name\n## [1] \"Michael\" \"Mark\"    \"Maggie\""},{"path":"introduction.html","id":"matrices","chapter":"1 Introduction","heading":"1.2.7 Matrices","text":"matrix similar data frame contains values data type.\ndata mining algorithms require numeric matrix input. numeric part \nData frames can coerced matrix. following example cast last two columns\ndf matrix. Column names taken data frame add manually row names.Data type coercion functions R start .. Examples .logical(),\n.data.frame(), .list(), etc.","code":"\nx <- as.matrix(df[, -1])\nrownames(x) <- df[, 1]\nx\n##         children age\n## Michael        2  40\n## Mark           0  30\n## Maggie         2  50"},{"path":"introduction.html","id":"strings","chapter":"1 Introduction","heading":"1.2.8 Strings","text":"R uses character vectors datatype character represents string\nlike programming language just single character.\nR accepts double single quotation marks delimit strings.Strings can combined using paste().Note paste vectorized string \"World!\" used twice \nmatch length string. behavior called R recycling\nworks one vector’s length exact multiple \nvector’s length. special case one vector one element \nparticularly often used. used already expression\nx + 1 one recycled element x.","code":"\nstring <- c(\"Hello\", \"Goodbye\")\nstring\n## [1] \"Hello\"   \"Goodbye\"\npaste(string, \"World!\")\n## [1] \"Hello World!\"   \"Goodbye World!\""},{"path":"introduction.html","id":"objects","chapter":"1 Introduction","heading":"1.2.9 Objects","text":"often used data structures include list, data.frame, matrix,\nfactor. R, everything object. Objects printed \neither just using object’s name put explicitly \nprint() function.many objects, summary can created.Objects class.functions generic, meaning something else\ndepending class first argument. example, plot() \nmany methods implemented visualize data different classes. \nspecific function specified period method name.\nalso exists default method. plot, method \nplot.default(). Different methods may manual page, \nspecifying class dot can help finding documentation.often useful know information stored object.\nstr() returns humanly readable string representation object.","code":"\nx\n##         children age\n## Michael        2  40\n## Mark           0  30\n## Maggie         2  50\nsummary(x)\n##     children         age    \n##  Min.   :0.00   Min.   :30  \n##  1st Qu.:1.00   1st Qu.:35  \n##  Median :2.00   Median :40  \n##  Mean   :1.33   Mean   :40  \n##  3rd Qu.:2.00   3rd Qu.:45  \n##  Max.   :2.00   Max.   :50\nclass(x)\n## [1] \"matrix\" \"array\"\nstr(x)\n##  num [1:3, 1:2] 2 0 2 40 30 50\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : chr [1:3] \"Michael\" \"Mark\" \"Maggie\"\n##   ..$ : chr [1:2] \"children\" \"age\""},{"path":"introduction.html","id":"functions","chapter":"1 Introduction","heading":"1.2.10 Functions","text":"Functions work like many languages. However, also operate\nvectorized arguments can specified positional named. \nincrement function can defined :value last evaluated expression function body \nautomatically returned function. function can also\nexplicitly use return(return_value).Calling increment function vector numbers 1 10.implement function, check R already provide \nimplementation.\nR many built-functions like min(), max(), mean(), \nsum().","code":"\ninc <- function(x, by = 1) { \n    x + by \n  }\nv <- 1:10\ninc(v, by = 2)\n##  [1]  3  4  5  6  7  8  9 10 11 12"},{"path":"introduction.html","id":"plotting","chapter":"1 Introduction","heading":"1.2.11 Plotting","text":"Basic plotting Base-R done calling plot().plot functions pairs(), hist(), barplot(). book,\nfocus plots created ggplot2 package introduced .","code":"\nplot(x, y)"},{"path":"introduction.html","id":"more-on-r","chapter":"1 Introduction","heading":"1.2.12 More on R","text":"much learn R. highly recommended go \nofficial\nIntroduction \nR manual.\nalso good Base R Cheat Sheet\navailable.","code":""},{"path":"introduction.html","id":"r-markdown","chapter":"1 Introduction","heading":"1.3 R Markdown","text":"R Markdown simple method include R code inside text document\nwritten using markdown syntax. Using R markdown especially convention analyze\ndata compose data mining report. RStudio makes creating \ntranslating R Markdown document easy. Just choose\nFile -> New File -> R Markdown... RStudio create small\ndemo markdown document already includes examples code \ninclude plots. can switch visual mode prefer \nSee Get editor.convert R markdown document HTML, PDF Word, just click \nKnit button. code executed combined text\ncomplete document.Examples detailed documentation can found \nRStudio’s R Markdown website \nR Markdown Cheatsheet.","code":""},{"path":"introduction.html","id":"tidyverse","chapter":"1 Introduction","heading":"1.4 Tidyverse","text":"tidyverse (Wickham 2023b) collection many useful packages \nwork well together sharing design principles data structures.\ntidyverse also includes ggplot2 (Wickham et al. 2025) visualization.book, typically usetidyverse tibbles replace R’s built-data.frames,pipe operator |> chain functions together, anddata transformation functions like filter(), arrange(),\nselect(), group_by(), mutate() provided tidyverse\npackage dplyr (Wickham et al. 2023).good introduction can found Section Data\nTransformation (Wickham, Çetinkaya-Rundel, Grolemund 2023).use tidyverse, first load tidyverse meta package.Conflicts may indicate base R functionality changed \ntidyverse packages.","code":"\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.2\n## ✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n## ✔ purrr     1.1.0     \n## ── Conflicts ────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"},{"path":"introduction.html","id":"tibbles","chapter":"1 Introduction","heading":"1.4.1 Tibbles","text":"Tibbles tidyverse’s replacement R’s data.frame.\nshort example analyses vitamin C content different\nfruits get familiar basic syntax.\ncreate tibble price dollars per pound \nvitamin C content milligrams (mg) per pound five different fruits.Next, transform data find affordable fruits \nhigh vitamin C content visualize data.","code":"\nfruit <- tibble(\n  name = c(\"apple\", \"banana\", \"mango\", \"orange\", \"lime\"), \n  price = c(2.5, 2.0, 4.0, 3.5, 2.5), \n  vitamin_c = c(20, 45, 130, 250, 132),\n  type = c(\"pome\", \"tropical\", \"tropical\", \"citrus\", \"citrus\"))\nfruit\n## # A tibble: 5 × 4\n##   name   price vitamin_c type    \n##   <chr>  <dbl>     <dbl> <chr>   \n## 1 apple    2.5        20 pome    \n## 2 banana   2          45 tropical\n## 3 mango    4         130 tropical\n## 4 orange   3.5       250 citrus  \n## 5 lime     2.5       132 citrus"},{"path":"introduction.html","id":"transformations","chapter":"1 Introduction","heading":"1.4.2 Transformations","text":"can modify table adding column vitamin C (mg)\ndollar buys using mutate(). filter rows \nfruit provides 20 mg per dollar, finally arrange data\nrows vitamin C per dollar largest smallest.pipes operator |> lets pass value left (often \nresult function) first argument function \nright. makes composing sequence function calls transform\ndata much easier write read.\noften see %>% pipe operator, especially examples using\ntidyverse. operators work similarly |> native R operator\n%>% provided extension package magrittr.code starts fruit\ndata pipes three transformation functions. final\nresult assigned <- variable.can create summary statistics price using summarize().can also calculate statistics groups first grouping data\ngroup_by(). \nproduce statistics fruit types.Often, want apply function multiple columns. can\nachieved using across(columns, function).dplyr syntax evaluation slightly different standard R\nmay lead confusion. One example column names can \nreferences without quotation marks. useful reference resource\nworking dplyr RStudio Data Transformation Cheatsheet\ncovers two pages almost everything need also\ncontains contains simple example code can take modify \nuse case.","code":"\naffordable_vitamin_c_sources <- fruit |>\n  mutate(vitamin_c_per_dollar = vitamin_c / price) |> \n  filter(vitamin_c_per_dollar > 20) |>\n  arrange(desc(vitamin_c_per_dollar))\n\naffordable_vitamin_c_sources \n## # A tibble: 4 × 5\n##   name   price vitamin_c type     vitamin_c_per_dollar\n##   <chr>  <dbl>     <dbl> <chr>                   <dbl>\n## 1 orange   3.5       250 citrus                   71.4\n## 2 lime     2.5       132 citrus                   52.8\n## 3 mango    4         130 tropical                 32.5\n## 4 banana   2          45 tropical                 22.5\naffordable_vitamin_c_sources |> \n  summarize(min = min(price), \n            mean = mean(price), \n            max = max(price))\n## # A tibble: 1 × 3\n##     min  mean   max\n##   <dbl> <dbl> <dbl>\n## 1     2     3     4\naffordable_vitamin_c_sources |> \n  group_by(type) |>\n  summarize(min = min(price), \n            mean = mean(price), \n            max = max(price))\n## # A tibble: 2 × 4\n##   type       min  mean   max\n##   <chr>    <dbl> <dbl> <dbl>\n## 1 citrus     2.5     3   3.5\n## 2 tropical   2       3   4\naffordable_vitamin_c_sources |> \n  summarize(across(c(price, vitamin_c), mean))\n## # A tibble: 1 × 2\n##   price vitamin_c\n##   <dbl>     <dbl>\n## 1     3      139."},{"path":"introduction.html","id":"ggplot2","chapter":"1 Introduction","heading":"1.4.3 ggplot2","text":"visualization, use mainly ggplot2. gg ggplot2\nstands Grammar Graphics introduced Wilkinson (2005). \nmain idea every graph built basic components:data,coordinate system, andvisual marks representing data (geoms).main plotting function ggplot(). components plot\ncombined using + operator.Since typically use Cartesian coordinate system, ggplot() uses \ndefault. geom_ function uses stat_ function calculate\nvisualizes. example, geom_bar() uses stat_count() create\nbar chart counting often value appears data (see\n? geom_bar). geom_point() just uses stat \"identity\" display\npoints using coordinates . great introduction can\nfound Section Data\nVisualization (Wickham, Çetinkaya-Rundel, Grolemund 2023),\nuseful RStudio’s Data Visualization Cheatsheet.can visualize fruit data scatter plot.easy add geoms. example, can add regression line\nusing geom_smooth method \"lm\" (linear model). suppress\nconfidence interval since 3 data points.Alternatively, can visualize fruit’s vitamin C content per\ndollar using bar chart.Note geom_bar default uses stat_count function \naggregate data counting, just want visualize value \ntibble, specify identity function instead.","code":"\nggplot(data, mapping = aes(x = ..., y = ..., color = ...)) +\n  geom_point()\nggplot(fruit, aes(x = price, y = vitamin_c)) + \n  geom_point()\nggplot(fruit, aes(x = price, y = vitamin_c)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n## `geom_smooth()` using formula = 'y ~ x'\nggplot(fruit, aes(x = name, y = vitamin_c)) + \n  geom_bar(stat = \"identity\")"},{"path":"data.html","id":"data","chapter":"2 Data","heading":"2 Data","text":"Data data mining typically organized tabular form, rows containing\nobjects interest columns representing attributes describing objects.\ndiscuss topics like data quality, sampling, feature selection,\nmeasure similarities objects features.chapter, can read Appendix Data Exploration Visualization\nlearn data exploration visualization R.","code":""},{"path":"data.html","id":"packages-used-in-this-chapter","chapter":"2 Data","heading":"Packages Used in this Chapter","text":"packages used chapter :arules (Hahsler et al. 2025)caret (Kuhn 2024)factoextra (Kassambara Mundt 2020)GGally (Schloerke et al. 2025)palmerpenguins (Horst, Hill, Gorman 2022)plotly (Sievert et al. 2025)proxy (Meyer Buchta 2022)Rtsne (Krijthe 2023)tidyverse (Wickham 2023b)","code":"\npkgs <- c(\"arules\", \"caret\", \"factoextra\", \"GGally\", \n          \"palmerpenguins\", \"plotly\", \n          \"proxy\", \"Rtsne\", \"tidyverse\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"data.html","id":"types-of-data","chapter":"2 Data","heading":"2.1 Types of Data","text":"","code":""},{"path":"data.html","id":"attributes-and-measurement","chapter":"2 Data","heading":"2.1.1 Attributes and Measurement","text":"values features can measured several scales\nranging \nsimple labels way numbers. scales come four levels.scales build meaning ordinal variable also characteristics \nnominal variable added order information.\noften differentiate interval ratio scale rarely\nneed calculate percentages statistics require meaningful zero value.Nominal data created using factor(). factor levels\nspecified, created alphabetical order.Ordinal data created using ordered(). levels specify order.Ratio/interval data created simple vector.","code":"\nfactor(c(\"red\", \"green\", \"green\", \"blue\"))\n## [1] red   green green blue \n## Levels: blue green red\nordered(c(\"S\", \"L\", \"M\", \"S\"), \n       levels = c(\"S\", \"M\", \"L\"))\n## [1] S L M S\n## Levels: S < M < L\nc(1, 2, 3, 4, 3, 3)\n## [1] 1 2 3 4 3 3"},{"path":"data.html","id":"the-iris-dataset","chapter":"2 Data","heading":"2.1.2 The Iris Dataset","text":"use toy dataset comes R. Fisher’s iris\ndataset gives \nmeasurements centimeters variables sepal length, sepal width\npetal length, petal width representing features 150 flowers (objects).\ndataset contains 50\nflowers 3 species iris. species Iris Setosa,\nIris Versicolor, Iris Virginica. details see ? iris.load iris data set. Datasets come R R packages can\nloaded data(). standard format data R \ndata.frame. convert data.frame tidyverse tibble.see data contains 150 rows (flowers) 5 features. tibbles\nshow first rows show features, \nfit screen width. can call print define many rows\nshow using parameter n force print show features \nchanging width infinity.","code":"\nlibrary(tidyverse)\ndata(iris)\niris <- as_tibble(iris)\niris\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # ℹ 140 more rows\nprint(iris, n = 3, width = Inf)\n## # A tibble: 150 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          5.1         3.5          1.4         0.2 setosa \n## 2          4.9         3            1.4         0.2 setosa \n## 3          4.7         3.2          1.3         0.2 setosa \n## # ℹ 147 more rows"},{"path":"data.html","id":"data-quality","chapter":"2 Data","heading":"2.2 Data Quality","text":"Assessing quality available data crucial start\nusing data. Start summary statistics column \nidentify outliers missing values. easiest way use base R\nfunction summary().Feature names used plots analysis understandable\nfeature names part data quality.\nIris’s names good datasets, may need change\nnames using either rownames()\nrename().can also summarize\nindividual columns using tidyverse’s dplyr functions.Using across(), multiple columns can summarized. Un following,\ncalculate numeric columns using mean function.find outliers data problems, need look small\nvalues (often suspicious large number zeros) using min \nextremely large values using max. Comparing median mean tells us \ndistribution symmetric.visual method inspect data use scatterplot matrix (\nuse ggpairs() package GGally). plot, can\nvisually identify noise data points outliers (points far\nmajority points).useful visualization combines many visualizations used understand \ndata check quality issues. Rows columns features data.\nalso specified aesthetic want group species using different color.visualizations diagonal panels show \nsmoothed histograms distribution feature.\nplot tries pick good number bins histogram\n(see messages ). distribution can checked close normal,\nunimodal highly skewed. Also, can see different groups overlapping\nseparable feature. example, three distributions Sepal.Width\nalmost identical meaning hard distinguish \ndifferent species using feature alone. Petal.Lenght Petal.Width\nmuch better.visualizations diagonal panels show \nsmoothed histograms distribution feature.\nplot tries pick good number bins histogram\n(see messages ). distribution can checked close normal,\nunimodal highly skewed. Also, can see different groups overlapping\nseparable feature. example, three distributions Sepal.Width\nalmost identical meaning hard distinguish \ndifferent species using feature alone. Petal.Lenght Petal.Width\nmuch better.lower-left triangle panels contain scatterplots pairs features. \nuseful see features correlated (pearson correlation\ncoefficient printed upper-right triangle). example,\nPetal.Length Petal.Width highly correlated overall\nmakes sense since larger plants longer wider petals.\nInside Setosa group correlation lot weaker.\ncan also see groups \nwell separated using projections two variables. Almost panels show \nSetosa forms point cloud well separated two classes \nVersicolor Virginica overlap.\ncan also see outliers far data points group.\nSee can spot one red dot far away others.lower-left triangle panels contain scatterplots pairs features. \nuseful see features correlated (pearson correlation\ncoefficient printed upper-right triangle). example,\nPetal.Length Petal.Width highly correlated overall\nmakes sense since larger plants longer wider petals.\nInside Setosa group correlation lot weaker.\ncan also see groups \nwell separated using projections two variables. Almost panels show \nSetosa forms point cloud well separated two classes \nVersicolor Virginica overlap.\ncan also see outliers far data points group.\nSee can spot one red dot far away others.last row/column represents data set class label Species.\nnominal variable plots different. bottom row panels\nshow (regular) histograms. last column shows boxplots represent\ndistribution different features group. Dots represent\noutliers. Finally, bottom-right panel contains counts different\ngroups barplot. data set, group number observations.last row/column represents data set class label Species.\nnominal variable plots different. bottom row panels\nshow (regular) histograms. last column shows boxplots represent\ndistribution different features group. Dots represent\noutliers. Finally, bottom-right panel contains counts different\ngroups barplot. data set, group number observations.Many data mining methods require complete data, data \ncontain missing values (NA). remove missing values duplicates\n(identical data points might mistake data), often\n:iris dataset missing values, one non-unique case gone\nleaving 149 flowers. Since 1 150 flowers dataset \naffected, results remaining data give similar results \ncomplete data.Typically, spend lot time data cleaning.\nimportant always describe clean data,\nmany objects removed. need argue conclusions based \nremaining data still valid.","code":"\nsummary(iris)\n##   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  \n##  Median :5.80   Median :3.00   Median :4.35   Median :1.3  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \niris |> \n  summarize(mean = mean(Sepal.Length))\n## # A tibble: 1 × 1\n##    mean\n##   <dbl>\n## 1  5.84\niris |> \n  summarize(across(where(is.numeric), mean))\n## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         5.84        3.06         3.76        1.20\nlibrary(GGally)\nggpairs(iris, aes(color = Species), progress = FALSE)\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\nclean.data <- iris |> \n  drop_na() |> \n  unique()\n\nsummary(clean.data)\n##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.30  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.75  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width         Species  \n##  Min.   :0.10   setosa    :50  \n##  1st Qu.:0.30   versicolor:50  \n##  Median :1.30   virginica :49  \n##  Mean   :1.19                  \n##  3rd Qu.:1.80                  \n##  Max.   :2.50"},{"path":"data.html","id":"data-preprocessing","chapter":"2 Data","heading":"2.3 Data Preprocessing","text":"","code":""},{"path":"data.html","id":"aggregation","chapter":"2 Data","heading":"2.3.1 Aggregation","text":"Data often contains groups want compare groups. group\niris dataset species calculate summary statistic \ngroup.Using information, can compare features differ \ngroups.","code":"\niris |> \n  group_by(Species) |> \n  summarize(across(everything(), mean))\n## # A tibble: 3 × 5\n##   Species  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <fct>           <dbl>       <dbl>        <dbl>       <dbl>\n## 1 setosa           5.01        3.43         1.46       0.246\n## 2 versico…         5.94        2.77         4.26       1.33 \n## 3 virgini…         6.59        2.97         5.55       2.03\niris |> \n  group_by(Species) |> \n  summarize(across(everything(), median))\n## # A tibble: 3 × 5\n##   Species  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <fct>           <dbl>       <dbl>        <dbl>       <dbl>\n## 1 setosa            5           3.4         1.5          0.2\n## 2 versico…          5.9         2.8         4.35         1.3\n## 3 virgini…          6.5         3           5.55         2"},{"path":"data.html","id":"sampling","chapter":"2 Data","heading":"2.3.2 Sampling","text":"Sampling often\nused data mining reduce dataset size modeling \nvisualization. Another application split data randomly training testing\ndata working supervised models (e.g., classification models).","code":""},{"path":"data.html","id":"random-sampling","chapter":"2 Data","heading":"2.3.2.1 Random Sampling","text":"built-sample function can sample vector. sample\nreplacement.often want sample rows dataset. can done \nsampling without replacement vector row indices (using \nfunctions seq() nrow()). sample vector used \nsubset rows dataset.dplyr tidyverse lets us sample rows tibbles directly using\nslice_sample(). set random number generator seed make \nresults reproducible.\nInstead n can also specify proportion rows select using\nprob.","code":"\nsample(c(\"A\", \"B\", \"C\"), size = 10, replace = TRUE)\n##  [1] \"A\" \"C\" \"B\" \"B\" \"A\" \"A\" \"C\" \"C\" \"C\" \"B\"\ntake <- sample(seq(nrow(iris)), size = 15)\ntake\n##  [1] 108  22  17  59  66  94 123  53  80  98 138  64  28  48\n## [15]  47\niris[take, ]\n## # A tibble: 15 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          7.3         2.9          6.3         1.8 virgin…\n##  2          5.1         3.7          1.5         0.4 setosa \n##  3          5.4         3.9          1.3         0.4 setosa \n##  4          6.6         2.9          4.6         1.3 versic…\n##  5          6.7         3.1          4.4         1.4 versic…\n##  6          5           2.3          3.3         1   versic…\n##  7          7.7         2.8          6.7         2   virgin…\n##  8          6.9         3.1          4.9         1.5 versic…\n##  9          5.7         2.6          3.5         1   versic…\n## 10          6.2         2.9          4.3         1.3 versic…\n## 11          6.4         3.1          5.5         1.8 virgin…\n## 12          6.1         2.9          4.7         1.4 versic…\n## 13          5.2         3.5          1.5         0.2 setosa \n## 14          4.6         3.2          1.4         0.2 setosa \n## 15          5.1         3.8          1.6         0.2 setosa\nset.seed(1000)\ns <- iris |> \n  slice_sample(n = 15)\n\nlibrary(GGally)\nggpairs(s, aes(color = Species), progress = FALSE)\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`."},{"path":"data.html","id":"stratified-sampling","chapter":"2 Data","heading":"2.3.2.2 Stratified Sampling","text":"Stratified sampling\nmethod sampling population can partitioned \nsubpopulations, controlling proportions subpopulation\nresulting sample.following, subpopulations different types species\nwant make sure sample number (5) flowers \n. can achieved first grouping data species \nsampling number flowers group.sophisticated sampling procedures implemented package\nsampling.","code":"\nset.seed(1000)\n\ns2 <- iris |> \n  group_by(Species) |>\n  slice_sample(n = 5) |>\n  ungroup()\n\nlibrary(GGally)\nggpairs(s2, aes(color = Species), progress = FALSE)\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`."},{"path":"data.html","id":"dimensionality-reduction","chapter":"2 Data","heading":"2.3.3 Dimensionality Reduction","text":"number features often called dimensional data following \nidea feature (least numeric features) can seen axis data.\nHigh-dimensional data harder analyze user (e.g., visualize).\nalso problematic many data mining algorithms since requires memory\ncomputational resources.Dimensionality reduction\ntries represent high-dimensional data low-dimensional space \nlow-dimensional representation retains meaningful properties (e.g., information similarity distances) original data. Dimensionality reduction \nused visualization prepossessing technique using \ndata mining methods like clustering classification.Recently, data embeddings using artificial neural networks become popular.\napproaches can reduce dimensionality data, \nlearn better representation various kinds data (e.g., text). \napproaches can seen automatically engineering features \nhigh-dimensional original data.","code":""},{"path":"data.html","id":"principal-components-analysis-pca","chapter":"2 Data","heading":"2.3.3.1 Principal Components Analysis (PCA)","text":"PCA\ncalculates principal components (set new orthonormal basis vectors\ndata space) data points first principal\ncomponent explains variability data, second next\n. data analysis, PCA used project\nhigh-dimensional data points onto first (typically two)\nprincipal components visualization scatter plot \npreprocessing modeling (e.g., k-means clustering). Points\ncloser together high-dimensional original space, tend\nalso closer together projected lower-dimensional space,can use interactive 3-d plot (package plotly) look \nthree four dimensions iris dataset. Note hard\nvisualize 3 dimensions.principal components can calculated matrix using \nfunction prcomp(). select numeric columns (unselecting \nspecies column) convert tibble matrix \ncalculation.important principal component can also seen using \nscree plot. plot\nfunction result prcomp function visualizes much\nvariability data explained additional principal\ncomponent.Note first principal component (PC1) explains \nvariability iris dataset.find information stored object pc, can\ninspect raw object (display structure).object pc (like objects R) list class\nattribute. list element x contains data points projected \nprincipal components. can convert matrix tibble \nadd species column original dataset back (since rows\norder), display data projected first\ntwo principal components.Flowers displayed close together projection also\nclose together original 4-dimensional space. Since first\nprincipal component represents variability, can also show\ndata projected PC1.see can perfectly separate species Setosa using just \nfirst principal component. two species harder separate.plot projected data original axes added arrows \ncalled biplot. arrows\n(original axes) align roughly axes projection, \ncorrelated (linearly dependent).can also display old new axes.see Petal.Width Petal.Length point direction \nindicates highly correlated. also roughly aligned\nPC1 (called Dim1 plot) means PC1 represents \nvariability two variables. Sepal.Width almost aligned\ny-axis therefore represented PC2 (Dim2).\nPetal.Width/Petal.Length Sepal.Width almost 90 degrees,\nindicating close uncorrelated. Sepal.Length \ncorrelated variables represented , PC1 PC2\nprojection.","code":"\nplotly::plot_ly(iris, \n                x = ~Sepal.Length, \n                y = ~Petal.Length, \n                z = ~Sepal.Width, \n      color = ~Species, size = 1) |> \n  plotly::add_markers()\npc <- iris |> \n  select(-Species) |> \n  as.matrix() |> \n  prcomp()\nsummary(pc)\n## Importance of components:\n##                          PC1    PC2    PC3     PC4\n## Standard deviation     2.056 0.4926 0.2797 0.15439\n## Proportion of Variance 0.925 0.0531 0.0171 0.00521\n## Cumulative Proportion  0.925 0.9777 0.9948 1.00000\nplot(pc, type = \"line\")\nstr(pc)\n## List of 5\n##  $ sdev    : num [1:4] 2.056 0.493 0.28 0.154\n##  $ rotation: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 -0.6566 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n##   .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\"\n##  $ center  : Named num [1:4] 5.84 3.06 3.76 1.2\n##   ..- attr(*, \"names\")= chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n##  $ scale   : logi FALSE\n##  $ x       : num [1:150, 1:4] -2.68 -2.71 -2.89 -2.75 -2.73 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\"\n##  - attr(*, \"class\")= chr \"prcomp\"\niris_projected <- as_tibble(pc$x) |> \n  add_column(Species = iris$Species)\n\nggplot(iris_projected, aes(x = PC1, y = PC2, color = Species)) + \n  geom_point()\nggplot(iris_projected, \n  aes(x = PC1, y = 0, color = Species)) + \n  geom_point() +\n  scale_y_continuous(expand=c(0,0)) +\n  theme(axis.text.y = element_blank(),\n      axis.title.y = element_blank()\n  )\nlibrary(factoextra)\nfviz_pca(pc)\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2\n## 3.4.0.\n## ℹ Please use `linewidth` instead.\n## ℹ The deprecated feature was likely used in the ggpubr\n##   package.\n##   Please report the issue at\n##   <https://github.com/kassambara/ggpubr/issues>.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where\n## this warning was generated.\nfviz_pca_var(pc)"},{"path":"data.html","id":"multi-dimensional-scaling-mds","chapter":"2 Data","heading":"2.3.3.2 Multi-Dimensional Scaling (MDS)","text":"MDS similar\nPCA. Instead data points, starts pairwise distances (.e.,\ndistance matrix) produces space points placed \nrepresent distances well possible. axes space\ncalled components similar principal components \nPCA.First, calculate distance matrix (Euclidean distances) 4-d\nspace iris dataset.Metric (classic) MDS tries construct space points lower\ndistances placed closer together. project data represented \ndistance matrix k = 2 dimensions.resulting projection similar (except rotation reflection)\nresult projection using PCA.","code":"\nd <- iris |> \n  select(-Species) |> \n  dist()\nfit <- cmdscale(d, k = 2)\ncolnames(fit) <- c(\"comp1\", \"comp2\")\nfit <- as_tibble(fit) |> \n  add_column(Species = iris$Species)\n\nggplot(fit, aes(x = comp1, y = comp2, color = Species)) + \n  geom_point()"},{"path":"data.html","id":"non-parametric-multidimensional-scaling","chapter":"2 Data","heading":"2.3.3.3 Non-Parametric Multidimensional Scaling","text":"Non-parametric multidimensional scaling performs MDS relaxing \nneed linear relationships. Methods available package MASS \nfunctions isoMDS() (implements isoMAP) sammon().","code":""},{"path":"data.html","id":"embeddings-nonlinear-dimensionality-reduction-methods","chapter":"2 Data","heading":"2.3.3.4 Embeddings: Nonlinear Dimensionality Reduction Methods","text":"Nonlinear dimensionality reduction\nalso called manifold learning creating low-dimensional embedding.\nmethods become popular support visualizing high-dimensional data,\nmine text converting words numeric vectors can used features models,\nmethod automatically create features data mining models finding efficient\nrepresentation. Popular methods :Visualization: t-distributed stochastic neighbor embedding\n(Rtsne() package Rtsne) uniform manifold approximation projection\n(umap() package umap) used\nprojecting data 2 dimensions visualization. example \nembedding 4-dimensional iris data using tsne. embedding algorithm\nrequires unique data points.\n\niris_distinct <- iris |> distinct(Sepal.Length, Sepal.Width, \n                                  Petal.Length, Petal.Width, \n                                  .keep_all = TRUE)\n\ntsne <- Rtsne::Rtsne(iris_distinct |> select(-Species))\n\nemb <- data.frame(tsne$Y, Species = iris_distinct$Species)\nggplot(emb, aes(X1, X2, color = Species)) + geom_point()\n\nsee embedding separates class Setosa well \ntwo species indicating flowers different.Visualization: t-distributed stochastic neighbor embedding\n(Rtsne() package Rtsne) uniform manifold approximation projection\n(umap() package umap) used\nprojecting data 2 dimensions visualization. example \nembedding 4-dimensional iris data using tsne. embedding algorithm\nrequires unique data points.see embedding separates class Setosa well \ntwo species indicating flowers different.Text mining: Word2vec (word2vec() \nword2vec) used natural language processing\nconvert words numeric vectors represent similarities words.\nvectors can used features data mining models clustering\nclassification. popular methods text embedding \nGloVe \nBERT.Text mining: Word2vec (word2vec() \nword2vec) used natural language processing\nconvert words numeric vectors represent similarities words.\nvectors can used features data mining models clustering\nclassification. popular methods text embedding \nGloVe \nBERT.Representation learning: Autoencoders\nartificial neural networks used learn efficient representation (encoding)\nset data minimizing reconstruction error. new representation\ncan seen way automatically creating features describe \nimportant characteristics data reducing noise. representation\noften helps models learn better.\nR, autoencoders typically created using\nkeras package. Creating \nautoencoder requires work. \nneed decide network topology sufficient training data.Representation learning: Autoencoders\nartificial neural networks used learn efficient representation (encoding)\nset data minimizing reconstruction error. new representation\ncan seen way automatically creating features describe \nimportant characteristics data reducing noise. representation\noften helps models learn better.\nR, autoencoders typically created using\nkeras package. Creating \nautoencoder requires work. \nneed decide network topology sufficient training data.","code":"\niris_distinct <- iris |> distinct(Sepal.Length, Sepal.Width, \n                                  Petal.Length, Petal.Width, \n                                  .keep_all = TRUE)\n\ntsne <- Rtsne::Rtsne(iris_distinct |> select(-Species))\n\nemb <- data.frame(tsne$Y, Species = iris_distinct$Species)\nggplot(emb, aes(X1, X2, color = Species)) + geom_point()"},{"path":"data.html","id":"feature-subset-selection","chapter":"2 Data","heading":"2.3.4 Feature Subset Selection","text":"Feature selection process identifying features \nused create model. talk feature selection \ndiscuss classification models Chapter 3 Feature Selection*.","code":""},{"path":"data.html","id":"discretization","chapter":"2 Data","heading":"2.3.5 Discretization","text":"data mining methods require discrete data. Discretization converts\ncontinuous features discrete features. example, \ndiscretize continuous feature Petal.Width. perform\ndiscretization, look distribution see gives\nus idea group continuous values set \ndiscrete values. histogram visualizes distribution single\ncontinuous feature.bins histogram represent discretization using fixed bin\nwidth. R function cut() performs equal interval width\ndiscretization creating vector type factor level\nrepresents interval.discretization methods include equal frequency discretization \nusing k-means clustering. methods implemented several R\npackages. use implementation package arules \nvisualize results histograms blue lines separate\nintervals assigned discrete value.show differences methods, use three\ndiscretization methods draw blue lines histogram show\ncut data.user needs decide number intervals used method.","code":"\nggplot(iris, aes(x = Petal.Width)) + \n  geom_histogram(binwidth = .2)\niris |> \n  pull(Sepal.Width) |> \n  cut(breaks = 3)\n##   [1] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##   [6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [11] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (3.6,4.4]\n##  [16] (3.6,4.4] (3.6,4.4] (2.8,3.6] (3.6,4.4] (3.6,4.4]\n##  [21] (2.8,3.6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [26] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [31] (2.8,3.6] (2.8,3.6] (3.6,4.4] (3.6,4.4] (2.8,3.6]\n##  [36] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [41] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (3.6,4.4]\n##  [46] (2.8,3.6] (3.6,4.4] (2.8,3.6] (3.6,4.4] (2.8,3.6]\n##  [51] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]  \n##  [56] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [61] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6]\n##  [66] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]  \n##  [71] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n##  [76] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2,2.8]  \n##  [81] (2,2.8]   (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n##  [86] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [91] (2,2.8]   (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]  \n##  [96] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]  \n## [101] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## [106] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6]\n## [111] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2,2.8]  \n## [116] (2.8,3.6] (2.8,3.6] (3.6,4.4] (2,2.8]   (2,2.8]  \n## [121] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n## [126] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6]\n## [131] (2,2.8]   (3.6,4.4] (2,2.8]   (2,2.8]   (2,2.8]  \n## [136] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## [141] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6]\n## [146] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## Levels: (2,2.8] (2.8,3.6] (3.6,4.4]\nlibrary(arules)\n## Loading required package: Matrix\n## \n## Attaching package: 'Matrix'\n## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack\n## \n## Attaching package: 'arules'\n## The following object is masked from 'package:dplyr':\n## \n##     recode\n## The following objects are masked from 'package:base':\n## \n##     abbreviate, write\niris |> pull(Petal.Width) |> \n  discretize(method = \"interval\", breaks = 3)\n##   [1] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##   [6] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [11] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [16] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [21] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [26] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [31] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [36] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [41] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [46] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [51] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [56] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [61] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [66] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [71] [1.7,2.5] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [76] [0.9,1.7) [0.9,1.7) [1.7,2.5] [0.9,1.7) [0.9,1.7)\n##  [81] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [86] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [91] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [96] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n## [101] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [106] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [111] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [116] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7)\n## [121] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [126] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7)\n## [131] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) [0.9,1.7)\n## [136] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [141] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [146] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## attr(,\"discretized:breaks\")\n## [1] 0.1 0.9 1.7 2.5\n## attr(,\"discretized:method\")\n## [1] interval\n## Levels: [0.1,0.9) [0.9,1.7) [1.7,2.5]\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(\n      xintercept = iris |> \n      pull(Petal.Width) |> \n      discretize(method = \"interval\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\"\n  ) +\n  labs(title = \"Discretization: interval\", \n       subtitle = \"Blue lines are discretization boundaries\")\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(\n    xintercept = iris |> \n    pull(Petal.Width) |> \n    discretize(method = \"frequency\", breaks = 3, onlycuts = TRUE),\n   color = \"blue\"\n  ) +\n  labs(title = \"Discretization: frequency\", \n       subtitle = \"Blue lines are discretization boundaries\")\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(\n    xintercept = iris |> \n    pull(Petal.Width) |> \n    discretize(method = \"cluster\", breaks = 3, onlycuts = TRUE),\n   color = \"blue\"\n  ) +\n  labs(title = \"Discretization: cluster\", \n       subtitle = \"Blue lines are discretization boundaries\")"},{"path":"data.html","id":"variable-transformation-standardization","chapter":"2 Data","heading":"2.3.6 Variable Transformation: Standardization","text":"Standardizing (scaling, normalizing) range features values \nimportant make comparable. popular method convert\nvalues feature \nz-scores. subtracting\nmean (centering) dividing standard deviation (scaling).\nstandardized feature mean zero measured \nstandard deviations mean. Positive values indicate many\nstandard deviation original feature value average.\nNegative standardized values indicate -average values.R-base provides function scale() standardize columns data.frame. Tidyverse currently simple scale function, \nmake one mutates numeric columns using anonymous function\ncalculates z-score.standardized feature mean zero “normal” values\nfall range \\([-3,3]\\) measured standard deviations average.\nNegative values mean smaller average positive values mean larger average.","code":"\nscale_numeric <- function(x) \n  x |> \n  mutate(across(where(is.numeric), \n                function(y) (y - mean(y, na.rm = TRUE)) / sd(y, na.rm = TRUE)))\niris.scaled <- iris |> \n  scale_numeric()\niris.scaled\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1       -0.898      1.02          -1.34       -1.31 setosa \n##  2       -1.14      -0.132         -1.34       -1.31 setosa \n##  3       -1.38       0.327         -1.39       -1.31 setosa \n##  4       -1.50       0.0979        -1.28       -1.31 setosa \n##  5       -1.02       1.25          -1.34       -1.31 setosa \n##  6       -0.535      1.93          -1.17       -1.05 setosa \n##  7       -1.50       0.786         -1.34       -1.18 setosa \n##  8       -1.02       0.786         -1.28       -1.31 setosa \n##  9       -1.74      -0.361         -1.34       -1.31 setosa \n## 10       -1.14       0.0979        -1.28       -1.44 setosa \n## # ℹ 140 more rows\nsummary(iris.scaled)\n##   Sepal.Length      Sepal.Width      Petal.Length   \n##  Min.   :-1.8638   Min.   :-2.426   Min.   :-1.562  \n##  1st Qu.:-0.8977   1st Qu.:-0.590   1st Qu.:-1.222  \n##  Median :-0.0523   Median :-0.132   Median : 0.335  \n##  Mean   : 0.0000   Mean   : 0.000   Mean   : 0.000  \n##  3rd Qu.: 0.6722   3rd Qu.: 0.557   3rd Qu.: 0.760  \n##  Max.   : 2.4837   Max.   : 3.080   Max.   : 1.780  \n##   Petal.Width           Species  \n##  Min.   :-1.442   setosa    :50  \n##  1st Qu.:-1.180   versicolor:50  \n##  Median : 0.132   virginica :50  \n##  Mean   : 0.000                  \n##  3rd Qu.: 0.788                  \n##  Max.   : 1.706"},{"path":"data.html","id":"measures-of-similarity-and-dissimilarity","chapter":"2 Data","heading":"2.4 Measures of Similarity and Dissimilarity","text":"Proximities help quantifying similar two objects .\nRemember, objects rows data. call one row feature vector object.Similariy concept geometry.\nopposite dissimilarity. best-known\nway measure dissimilarity Euclidean distance, dissimilarity similarity can measured many different ways depending information objects.R stores similarity information always dissimilarities/distances matrices.\nSimilarities converted dissimilarities.\nDistances symmetric, .e.,\ndistance B distance B . R\ntherefore stores triangle (typically lower triangle) \ndistance matrix.","code":""},{"path":"data.html","id":"minkowsky-distances","chapter":"2 Data","heading":"2.4.1 Minkowsky Distances","text":"Minkowsky\ndistance family\ndistances including Euclidean Manhattan distance. defined\ntwo feature vectors \\(\\mathbf{p} = (p_1, p_2, ..., p_n)\\) \n\\(\\mathbf{q} = (q_1, q_2, ..., q_n)\\) \\[\nd(\\mathbf{p},\\mathbf{q}) = \\left(\\sum_{=1}^n |p_i - q_i|^r\\right)^{\\frac{1}{r}} = ||\\mathbf{q}-\\mathbf{q}||_r.\n\\]\npower \\(r\\) positive integer. type distance also called\n\\(r\\)-norm written \\(L^r\\). Special values \\(r\\) :\\(r = 1\\): Manhattan distance\\(r = 2\\): Euclidean distance\\(r = \\infty\\): Maximum norm (largest component distance counts)avoid\none feature dominate distance calculation, scaled data \ntypically used. select first 5 flowers example.Different types Minkowsky distance matrices first 5\nflowers can calculated using dist().see lower triangle distance matrices stored\n(note rows start row 2).","code":"\niris_sample <- iris.scaled |> \n  select(-Species) |> \n  slice(1:5)\niris_sample\n## # A tibble: 5 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1       -0.898      1.02          -1.34       -1.31\n## 2       -1.14      -0.132         -1.34       -1.31\n## 3       -1.38       0.327         -1.39       -1.31\n## 4       -1.50       0.0979        -1.28       -1.31\n## 5       -1.02       1.25          -1.34       -1.31\ndist(iris_sample, method = \"euclidean\")\n##        1      2      3      4\n## 2 1.1723                     \n## 3 0.8428 0.5216              \n## 4 1.1000 0.4326 0.2829       \n## 5 0.2593 1.3819 0.9883 1.2460\ndist(iris_sample, method = \"manhattan\")\n##        1      2      3      4\n## 2 1.3887                     \n## 3 1.2280 0.7570              \n## 4 1.5782 0.6484 0.4635       \n## 5 0.3502 1.4973 1.3367 1.6868\ndist(iris_sample, method = \"maximum\")\n##        1      2      3      4\n## 2 1.1471                     \n## 3 0.6883 0.4589              \n## 4 0.9177 0.3623 0.2294       \n## 5 0.2294 1.3766 0.9177 1.1471"},{"path":"data.html","id":"distances-for-binary-data","chapter":"2 Data","heading":"2.4.2 Distances for Binary Data","text":"Binary data can encodes 0 1 (numeric) TRUE \nFALSE (logical).","code":"\nb <- rbind(\n  c(0,0,0,1,1,1,1,0,0,1),\n  c(0,0,1,1,1,0,0,1,0,0)\n  )\nb\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,]    0    0    0    1    1    1    1    0    0     1\n## [2,]    0    0    1    1    1    0    0    1    0     0\nb_logical <- apply(b, MARGIN = 2, as.logical)\nb_logical\n##       [,1]  [,2]  [,3] [,4] [,5]  [,6]  [,7]  [,8]  [,9]\n## [1,] FALSE FALSE FALSE TRUE TRUE  TRUE  TRUE FALSE FALSE\n## [2,] FALSE FALSE  TRUE TRUE TRUE FALSE FALSE  TRUE FALSE\n##      [,10]\n## [1,]  TRUE\n## [2,] FALSE"},{"path":"data.html","id":"hamming-distance","chapter":"2 Data","heading":"2.4.2.1 Hamming Distance","text":"Hamming distance\nnumber mismatches two binary vectors. 0-1 data\nequivalent Manhattan distance also squared\nEuclidean distance.","code":"\ndist(b, method = \"manhattan\")\n##   1\n## 2 5\ndist(b, method = \"euclidean\")^2\n##   1\n## 2 5"},{"path":"data.html","id":"jaccard-index","chapter":"2 Data","heading":"2.4.2.2 Jaccard Index","text":"Jaccard index \nsimilarity measure focuses matching 1s. R converts \nsimilarity dissimilarity using \\(d_{J} = 1 - s_{J}\\).","code":"\ndist(b, method = \"binary\")\n##        1\n## 2 0.7143"},{"path":"data.html","id":"distances-for-mixed-data","chapter":"2 Data","heading":"2.4.3 Distances for Mixed Data","text":"distance measures work numeric data. Often, \nmixture numbers nominal ordinal features like data:important nominal features stored factors \ncharacter (<chr>).","code":"\npeople <- tibble(\n  height = c(      160,    185,    170),\n  weight = c(       52,     90,     75),\n  sex    = c( \"female\", \"male\", \"male\")\n)\npeople\n## # A tibble: 3 × 3\n##   height weight sex   \n##    <dbl>  <dbl> <chr> \n## 1    160     52 female\n## 2    185     90 male  \n## 3    170     75 male\npeople <- people |> \n  mutate(across(where(is.character), factor))\npeople\n## # A tibble: 3 × 3\n##   height weight sex   \n##    <dbl>  <dbl> <fct> \n## 1    160     52 female\n## 2    185     90 male  \n## 3    170     75 male"},{"path":"data.html","id":"gowers-coefficient","chapter":"2 Data","heading":"2.4.3.1 Gower’s Coefficient","text":"Gower’s coefficient similarity works mixed data \ncalculating appropriate similarity feature \naggregating single measure. package proxy implements\nGower’s coefficient converted distance.Gower’s coefficient calculation implicitly scales data \ncalculates distances feature individually, need\nscale data first.","code":"\nlibrary(proxy)\n## \n## Attaching package: 'proxy'\n## The following object is masked from 'package:Matrix':\n## \n##     as.matrix\n## The following objects are masked from 'package:stats':\n## \n##     as.dist, dist\n## The following object is masked from 'package:base':\n## \n##     as.matrix\nd_Gower <- dist(people, method = \"Gower\")\nd_Gower\n##        1      2\n## 2 1.0000       \n## 3 0.6684 0.3316"},{"path":"data.html","id":"using-euclidean-distance-with-mixed-data","chapter":"2 Data","heading":"2.4.3.2 Using Euclidean Distance with Mixed Data","text":"Sometimes methods (e.g., k-means) can use Euclidean distance. \ncase, nominal features can converted 0-1 dummy variables.\nscaling, Euclidean distance result usable distance\nmeasure.use package caret create dummy variables.Note feature sex now two columns. want height,\nweight sex influence distance measure, \nneed weight sex columns 1/2 scaling.distance using dummy variables consistent Gower’s distance.\nHowever, note Gower’s distance scaled 0 1 \nEuclidean distance .","code":"\nlibrary(caret)\n## Loading required package: lattice\n## \n## Attaching package: 'caret'\n## The following object is masked from 'package:purrr':\n## \n##     lift\ndata_dummy <- dummyVars(~., people) |> \n  predict(people)\ndata_dummy\n##   height weight sex.female sex.male\n## 1    160     52          1        0\n## 2    185     90          0        1\n## 3    170     75          0        1\nweight_matrix <- matrix(c(1, 1, 1/2, 1/2), \n                        ncol = 4, \n                        nrow = nrow(data_dummy), \n                        byrow = TRUE)\ndata_dummy_scaled <- scale(data_dummy) * weight_matrix\n\nd_dummy <- dist(data_dummy_scaled)\nd_dummy\n##       1     2\n## 2 3.064      \n## 3 1.891 1.427\nggplot(tibble(d_dummy, d_Gower), aes(x = d_dummy, y = d_Gower)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n## Don't know how to automatically pick scale for object of\n## type <dist>. Defaulting to continuous.\n## Don't know how to automatically pick scale for object of\n## type <dist>. Defaulting to continuous.\n## `geom_smooth()` using formula = 'y ~ x'"},{"path":"data.html","id":"more-proximity-measures","chapter":"2 Data","heading":"2.4.4 More Proximity Measures","text":"package proxy implements wide array proximity measures\n(similarity measures converted distances).Note loading package proxy overwrites default dist() function\nR.\ncan specify dist function use specifying package \ncall. example stats::dist() calls default function R\n(package stats part R) proxy::dist() calls \nversion package proxy.","code":"\nlibrary(proxy)\npr_DB$get_entry_names()\n##  [1] \"Jaccard\"         \"Kulczynski1\"     \"Kulczynski2\"    \n##  [4] \"Mountford\"       \"Fager\"           \"Russel\"         \n##  [7] \"simple matching\" \"Hamman\"          \"Faith\"          \n## [10] \"Tanimoto\"        \"Dice\"            \"Phi\"            \n## [13] \"Stiles\"          \"Michael\"         \"Mozley\"         \n## [16] \"Yule\"            \"Yule2\"           \"Ochiai\"         \n## [19] \"Simpson\"         \"Braun-Blanquet\"  \"cosine\"         \n## [22] \"angular\"         \"eJaccard\"        \"eDice\"          \n## [25] \"correlation\"     \"Chi-squared\"     \"Phi-squared\"    \n## [28] \"Tschuprow\"       \"Cramer\"          \"Pearson\"        \n## [31] \"Gower\"           \"Euclidean\"       \"Mahalanobis\"    \n## [34] \"Bhjattacharyya\"  \"Manhattan\"       \"supremum\"       \n## [37] \"Minkowski\"       \"Canberra\"        \"Wave\"           \n## [40] \"divergence\"      \"Kullback\"        \"Bray\"           \n## [43] \"Soergel\"         \"Levenshtein\"     \"Podani\"         \n## [46] \"Chord\"           \"Geodesic\"        \"Whittaker\"      \n## [49] \"Hellinger\"       \"fJaccard\""},{"path":"data.html","id":"exercises","chapter":"2 Data","heading":"2.5 Exercises*","text":"R package palmerpenguins contains measurements penguin different\nspecies Palmer Archipelago, Antarctica. Install package.\nprovides CSV file can read following way:Create RStudio new R Markdown document.\nApply code sections chapter data set answer \nfollowing questions.scale measurement column?missing values data? much data missing?Compute discuss basic statistics.Calculate similarity five randomly chosen penguins. need scale data?\nDiscuss measures appropriate data.Make sure markdown document contains now well formatted report.\nUse Knit button create HTML document.","code":"\nlibrary(\"palmerpenguins\")\n## \n## Attaching package: 'palmerpenguins'\n## The following objects are masked from 'package:datasets':\n## \n##     penguins, penguins_raw\npenguins <- read_csv(path_to_file(\"penguins.csv\"))\n## Rows: 344 Columns: 8\n## ── Column specification ────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): species, island, sex\n## dbl (5): bill_length_mm, bill_depth_mm, flipper_length_m...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm\n##   <chr>   <chr>              <dbl>         <dbl>\n## 1 Adelie  Torgersen           39.1          18.7\n## 2 Adelie  Torgersen           39.5          17.4\n## 3 Adelie  Torgersen           40.3          18  \n## 4 Adelie  Torgersen           NA            NA  \n## 5 Adelie  Torgersen           36.7          19.3\n## 6 Adelie  Torgersen           39.3          20.6\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"classification-basic-concepts.html","id":"classification-basic-concepts","chapter":"3 Classification: Basic Concepts","heading":"3 Classification: Basic Concepts","text":"chapter introduces decision trees classification discusses\nmodels built evaluated.corresponding chapter \ndata mining textbook available online:\nChapter 3: Classification: Basic Concepts Techniques.","code":""},{"path":"classification-basic-concepts.html","id":"packages-used-in-this-chapter-1","chapter":"3 Classification: Basic Concepts","heading":"Packages Used in this Chapter","text":"packages used chapter :basemodels (Y.-J. Chen et al. 2023)caret (Kuhn 2024)FSelector (Romanski, Kotthoff, Schratz 2023)lattice (Sarkar 2025)mlbench (Leisch Dimitriadou 2024)palmerpenguins (Horst, Hill, Gorman 2022)party (Hothorn et al. 2025)pROC (Robin et al. 2025)rpart (Therneau Atkinson 2025)rpart.plot (Milborrow 2025)tidyverse (Wickham 2023b)examples book, use popular machine learning R package caret. makes preparing\ntraining sets, building classification (regression) models \nevaluation easier. great cheat sheet can found\n.newer R framework machine learning\ntidymodels, set packages \nintegrate naturally tidyverse. Using tidymodels, \nframework (e.g., Python’s scikit-learn) \nrelatively easy \nlearning concepts using caret.","code":"\npkgs <- c(\"basemodels\", \"caret\", \"FSelector\", \"lattice\", \"mlbench\", \n          \"palmerpenguins\", \"party\", \"pROC\", \"rpart\", \n          \"rpart.plot\", \"tidyverse\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"classification-basic-concepts.html","id":"basic-concepts","chapter":"3 Classification: Basic Concepts","heading":"3.1 Basic Concepts","text":"Classification machine learning task goal learn predictive\nfunction form\\[y = f(\\mathbf{x}),\\]\\(\\mathbf{x}\\) called attribute set \\(y\\) class label. attribute set\nconsists feature describe object. features can measured using scale\n(.e., nominal, interval, …). class label nominal attribute. binary\nattribute, problem called binary classification problem.Classification learns classification model training data features \ncorrect class label available. called supervised learning problem.related supervised learning problem regression,\n\\(y\\) number instead label.\nLinear regression popular supervised learning model\ntaught almost introductory statistics course.\nCode examples regression available extra Chapter\nRegression.chapter introduce decision trees, model evaluation comparison, feature selection,\nexplore methods handle class imbalance problem.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 3. Classification: Basic Concepts \nTechniques","code":""},{"path":"classification-basic-concepts.html","id":"general-framework-for-classification","chapter":"3 Classification: Basic Concepts","heading":"3.2 General Framework for Classification","text":"Supervised learning two steps:Induction: Training model training data known class labels.Deduction: Predicting class labels new data.often test model predicting class data know \ncorrect label. test model test data known labels\ncan calculate error comparing prediction \nknown correct label.\ntempting measure well model learned \ntraining data, testing training data. error \ntraining data called resubstitution error. \nhelp us find model generalizes well new data \npart training.typically want \nevaluate well model generalizes new data, important \ntest data training data overlap. call \nerror proper test data generalization error.chapter builds needed concepts.\ncomplete example perform model selection estimate\ngeneralization error section Hyperparameter Tuning.","code":""},{"path":"classification-basic-concepts.html","id":"the-zoo-dataset","chapter":"3 Classification: Basic Concepts","heading":"3.2.1 The Zoo Dataset","text":"demonstrate classification, use Zoo dataset included R package\nmlbench (may install ). Zoo dataset containing 17\n(mostly logical) variables 101 animals data frame \n17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator,\ntoothed, backbone, breathes, venomous, fins, legs, tail, domestic,\ncatsize, type).\nfirst 16 columns represent feature vector \\(\\mathbf{x}\\) last column\ncalled type class label \\(y\\).\nconvert data frame tidyverse tibble\n(optional).Note: data.frames R can row names. Zoo data set uses \nanimal name row names. tibbles tidyverse support\nrow names. keep animal name can add column animal\nname.remove animal column learning model since unique identifier!translate TRUE/FALSE values factors (nominal). \noften needed building models. Always check summary() make sure\ndata ready model learning.","code":"\ndata(Zoo, package=\"mlbench\")\nhead(Zoo)\n##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n## bear      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## boar      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## buffalo   TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n##          predator toothed backbone breathes venomous  fins\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE  TRUE\n## bear         TRUE    TRUE     TRUE     TRUE    FALSE FALSE\n## boar         TRUE    TRUE     TRUE     TRUE    FALSE FALSE\n## buffalo     FALSE    TRUE     TRUE     TRUE    FALSE FALSE\n##          legs  tail domestic catsize   type\n## aardvark    4 FALSE    FALSE    TRUE mammal\n## antelope    4  TRUE    FALSE    TRUE mammal\n## bass        0  TRUE    FALSE   FALSE   fish\n## bear        4 FALSE    FALSE    TRUE mammal\n## boar        4  TRUE    FALSE    TRUE mammal\n## buffalo     4  TRUE    FALSE    TRUE mammal\nlibrary(tidyverse)\nZoo <- as_tibble(Zoo, rownames = \"animal\")\nZoo\n## # A tibble: 101 × 18\n##    animal   hair  feathers eggs  milk  airborne aquatic\n##    <chr>    <lgl> <lgl>    <lgl> <lgl> <lgl>    <lgl>  \n##  1 aardvark TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  2 antelope TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  3 bass     FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n##  4 bear     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  5 boar     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  6 buffalo  TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  7 calf     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  8 carp     FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n##  9 catfish  FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n## 10 cavy     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n## # ℹ 91 more rows\n## # ℹ 11 more variables: predator <lgl>, toothed <lgl>,\n## #   backbone <lgl>, breathes <lgl>, venomous <lgl>,\n## #   fins <lgl>, legs <int>, tail <lgl>, domestic <lgl>,\n## #   catsize <lgl>, type <fct>\nZoo <- Zoo |>\n  mutate(across(where(is.logical), \n         function (x) factor(x, levels = c(TRUE, FALSE)))) |>\n  mutate(across(where(is.character), factor))\n\nsummary(Zoo)\n##       animal      hair     feathers     eggs       milk   \n##  aardvark: 1   TRUE :43   TRUE :20   TRUE :59   TRUE :41  \n##  antelope: 1   FALSE:58   FALSE:81   FALSE:42   FALSE:60  \n##  bass    : 1                                              \n##  bear    : 1                                              \n##  boar    : 1                                              \n##  buffalo : 1                                              \n##  (Other) :95                                              \n##   airborne   aquatic    predator   toothed    backbone \n##  TRUE :24   TRUE :36   TRUE :56   TRUE :61   TRUE :83  \n##  FALSE:77   FALSE:65   FALSE:45   FALSE:40   FALSE:18  \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##   breathes   venomous     fins         legs         tail   \n##  TRUE :80   TRUE : 8   TRUE :17   Min.   :0.00   TRUE :75  \n##  FALSE:21   FALSE:93   FALSE:84   1st Qu.:2.00   FALSE:26  \n##                                   Median :4.00             \n##                                   Mean   :2.84             \n##                                   3rd Qu.:4.00             \n##                                   Max.   :8.00             \n##                                                            \n##   domestic   catsize              type   \n##  TRUE :13   TRUE :44   mammal       :41  \n##  FALSE:88   FALSE:57   bird         :20  \n##                        reptile      : 5  \n##                        fish         :13  \n##                        amphibian    : 4  \n##                        insect       : 8  \n##                        mollusc.et.al:10"},{"path":"classification-basic-concepts.html","id":"decision-tree-classifiers","chapter":"3 Classification: Basic Concepts","heading":"3.3 Decision Tree Classifiers","text":"use recursive partitioning implementation (rpart) follows largely\nCART uses Gini index make\nsplitting decisions uses early stopping (also called pre-pruning).","code":"\nlibrary(rpart)"},{"path":"classification-basic-concepts.html","id":"create-tree","chapter":"3 Classification: Basic Concepts","heading":"3.3.1 Create Tree","text":"create first tree default settings (see ? rpart.control). \nimportant use identifier column algorithm use \ncolumn potentially run memory.Alternatively, can use . - animal formula .Notes:|> supplies data rpart. Since data \nfirst argument rpart, syntax data = _ used specify\ndata Zoo goes. call equivalent \ntree_default <- rpart(type ~ ., data = Zoo).formula models type variable features represented \nsingle period (.).class variable needs factor recognized nominal\nrpart create regression tree instead decision tree.\nUse .factor() column class label first, necessary.can plot resulting decision tree.Note: extra=2 prints leaf node number correctly\nclassified objects data total number objects \ntraining data falling node (correct/total).","code":"\nZoo <- Zoo |> select(-animal)\ntree_default <- Zoo |> \n  rpart(type ~ ., data = _)\ntree_default\n## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##  1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##    2) milk=TRUE 41  0 mammal (1 0 0 0 0 0 0) *\n##    3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##      6) feathers=TRUE 20  0 bird (0 1 0 0 0 0 0) *\n##      7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##       14) fins=TRUE 13  0 fish (0 0 0 1 0 0 0) *\n##       15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##         30) backbone=TRUE 9  4 reptile (0 0 0.56 0 0.44 0 0) *\n##         31) backbone=FALSE 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56) *\nlibrary(rpart.plot)\nrpart.plot(tree_default, extra = 2)"},{"path":"classification-basic-concepts.html","id":"make-predictions-for-new-data","chapter":"3 Classification: Basic Concepts","heading":"3.3.2 Make Predictions for New Data","text":"make animal: lion feathered wings.data types need match original data \nchange columns factors like training set.Next, make prediction using default tree","code":"\nmy_animal <- tibble(hair = TRUE, feathers = TRUE, eggs = FALSE,\n  milk = TRUE, airborne = TRUE, aquatic = FALSE, predator = TRUE,\n  toothed = TRUE, backbone = TRUE, breathes = TRUE, \n  venomous = FALSE, fins = FALSE, legs = 4, tail = TRUE, \n  domestic = FALSE, catsize = FALSE, type = NA)\nmy_animal <- my_animal |> \n  mutate(across(where(is.logical), \n                function(x) factor(x, levels = c(TRUE, FALSE))))\nmy_animal\n## # A tibble: 1 × 17\n##   hair  feathers eggs  milk  airborne aquatic predator\n##   <fct> <fct>    <fct> <fct> <fct>    <fct>   <fct>   \n## 1 TRUE  TRUE     FALSE TRUE  TRUE     FALSE   TRUE    \n## # ℹ 10 more variables: toothed <fct>, backbone <fct>,\n## #   breathes <fct>, venomous <fct>, fins <fct>, legs <dbl>,\n## #   tail <fct>, domestic <fct>, catsize <fct>, type <fct>\npredict(tree_default , my_animal, type = \"class\")\n##      1 \n## mammal \n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al"},{"path":"classification-basic-concepts.html","id":"calculation-of-the-resubstitution-error","chapter":"3 Classification: Basic Concepts","heading":"3.3.3 Calculation of the Resubstitution Error","text":"calculate error model training data manually first,\nsee calculated. error (.e., number incorrectly\nclassified examples) can broken error class. information\ntypically presented form confusion matrix.can easily tabulate true predicted labels create \nconfusion matrix.counts diagonal correct predictions. -diagonal\ncounts represent errors (.e., confusions).can summarize confusion matrix using accuracy measure.Accuracy just 1 \\(-\\) error rate give proportion correctly classified\nexamples.accuracy calculation simple function.caret package provides convenient way calculate \nanalyze classification errors confusion matrix. needs \npredicted class labels correct class labels reference.addition confusion matrix, also includes\nadditional statistics measures. Details can found Model Evaluation\nsection.Important note: Calculating accuracy training data good idea!\ncomplete example code holding test set performing hyperparameter\nselection using cross-validation can found section Hyperparameter Tuning.","code":"\npredict(tree_default, Zoo) |> head ()\n##   mammal bird reptile fish amphibian insect mollusc.et.al\n## 1      1    0       0    0         0      0             0\n## 2      1    0       0    0         0      0             0\n## 3      0    0       0    1         0      0             0\n## 4      1    0       0    0         0      0             0\n## 5      1    0       0    0         0      0             0\n## 6      1    0       0    0         0      0             0\npred <- predict(tree_default, Zoo, type=\"class\")\nhead(pred)\n##      1      2      3      4      5      6 \n## mammal mammal   fish mammal mammal mammal \n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al\nconfusion_table <- with(Zoo, table(type, pred))\nconfusion_table\n##                pred\n## type            mammal bird reptile fish amphibian insect\n##   mammal            41    0       0    0         0      0\n##   bird               0   20       0    0         0      0\n##   reptile            0    0       5    0         0      0\n##   fish               0    0       0   13         0      0\n##   amphibian          0    0       4    0         0      0\n##   insect             0    0       0    0         0      0\n##   mollusc.et.al      0    0       0    0         0      0\n##                pred\n## type            mollusc.et.al\n##   mammal                    0\n##   bird                      0\n##   reptile                   0\n##   fish                      0\n##   amphibian                 0\n##   insect                    8\n##   mollusc.et.al            10\ncorrect <- confusion_table |> diag() |> sum()\ncorrect\n## [1] 89\nerror <- confusion_table |> sum() - correct\nerror\n## [1] 12\naccuracy <- correct / (correct + error)\naccuracy\n## [1] 0.8812\naccuracy <- function(prediction, truth) {\n    tbl <- table(truth, prediction)\n    sum(diag(tbl))/sum(tbl)\n}\n\naccuracy(pred, Zoo |> pull(type))\n## [1] 0.8812\nlibrary(caret)\nconfusionMatrix(data = pred, \n                reference = Zoo |> pull(type))\n## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian insect\n##   mammal            41    0       0    0         0      0\n##   bird               0   20       0    0         0      0\n##   reptile            0    0       5    0         4      0\n##   fish               0    0       0   13         0      0\n##   amphibian          0    0       0    0         0      0\n##   insect             0    0       0    0         0      0\n##   mollusc.et.al      0    0       0    0         0      8\n##                Reference\n## Prediction      mollusc.et.al\n##   mammal                    0\n##   bird                      0\n##   reptile                   0\n##   fish                      0\n##   amphibian                 0\n##   insect                    0\n##   mollusc.et.al            10\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.881         \n##                  95% CI : (0.802, 0.937)\n##     No Information Rate : 0.406         \n##     P-Value [Acc > NIR] : <2e-16        \n##                                         \n##                   Kappa : 0.843         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       1.000\n## Pos Pred Value               1.000       1.000\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.406       0.198\n## Detection Rate               0.406       0.198\n## Detection Prevalence         0.406       0.198\n## Balanced Accuracy            1.000       1.000\n##                      Class: reptile Class: fish\n## Sensitivity                  1.0000       1.000\n## Specificity                  0.9583       1.000\n## Pos Pred Value               0.5556       1.000\n## Neg Pred Value               1.0000       1.000\n## Prevalence                   0.0495       0.129\n## Detection Rate               0.0495       0.129\n## Detection Prevalence         0.0891       0.129\n## Balanced Accuracy            0.9792       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                    0.0000        0.0000\n## Specificity                    1.0000        1.0000\n## Pos Pred Value                    NaN           NaN\n## Neg Pred Value                 0.9604        0.9208\n## Prevalence                     0.0396        0.0792\n## Detection Rate                 0.0000        0.0000\n## Detection Prevalence           0.0000        0.0000\n## Balanced Accuracy              0.5000        0.5000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         0.912\n## Pos Pred Value                      0.556\n## Neg Pred Value                      1.000\n## Prevalence                          0.099\n## Detection Rate                      0.099\n## Detection Prevalence                0.178\n## Balanced Accuracy                   0.956"},{"path":"classification-basic-concepts.html","id":"model-overfitting","chapter":"3 Classification: Basic Concepts","heading":"3.4 Model Overfitting","text":"tempted create largest possible tree\nget accurate model. can achieved \nchanging algorithms hyperparameter (parameters \nchange algorithm works). \nset complexity parameter cp 0 (split\neven improve fit) set minimum number \nobservations node needed split smallest value 2 (see:\n?rpart.control). Note: good idea!\nsee later, full trees overfit training data!Error training set full treeWe see error smaller pruned tree. ,\nhowever, mean model better. actually overfitting\ntraining data (just memorizes ) likely worse generalization\nperformance new data. effect\ncalled overfitting training data needs avoided.","code":"\ntree_full <- Zoo |> \n  rpart(type ~ . , data = _, \n        control = rpart.control(minsplit = 2, cp = 0))\nrpart.plot(tree_full, extra = 2, \n           roundint=FALSE,\n           box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \n                              \"Or\", \"Rd\", \"Pu\")) \ntree_full\n## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##     2) milk=TRUE 41  0 mammal (1 0 0 0 0 0 0) *\n##     3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##       6) feathers=TRUE 20  0 bird (0 1 0 0 0 0 0) *\n##       7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##        14) fins=TRUE 13  0 fish (0 0 0 1 0 0 0) *\n##        15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##          30) backbone=TRUE 9  4 reptile (0 0 0.56 0 0.44 0 0)  \n##            60) aquatic=FALSE 4  0 reptile (0 0 1 0 0 0 0) *\n##            61) aquatic=TRUE 5  1 amphibian (0 0 0.2 0 0.8 0 0)  \n##             122) eggs=FALSE 1  0 reptile (0 0 1 0 0 0 0) *\n##             123) eggs=TRUE 4  0 amphibian (0 0 0 0 1 0 0) *\n##          31) backbone=FALSE 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56)  \n##            62) airborne=TRUE 6  0 insect (0 0 0 0 0 1 0) *\n##            63) airborne=FALSE 12  2 mollusc.et.al (0 0 0 0 0 0.17 0.83)  \n##             126) predator=FALSE 4  2 insect (0 0 0 0 0 0.5 0.5)  \n##               252) legs>=3 2  0 insect (0 0 0 0 0 1 0) *\n##               253) legs< 3 2  0 mollusc.et.al (0 0 0 0 0 0 1) *\n##             127) predator=TRUE 8  0 mollusc.et.al (0 0 0 0 0 0 1) *\npred_full <- predict(tree_full, Zoo, type = \"class\")\n\naccuracy(pred_full, Zoo |> pull(type))\n## [1] 1"},{"path":"classification-basic-concepts.html","id":"model-selection","chapter":"3 Classification: Basic Concepts","heading":"3.5 Model Selection","text":"often can create many different models classification problem.\n, created decision tree using default settings also\nfull tree. question : one use. problem called\nmodel selection.order select model need split training data \nvalidation set training set actually used train model.\nerror rate validation set can used choose\nseveral models.Caret model selection build train() function. select\ndefault complexity cp = 0.01 full tree cp = 0 (set via tuneGrid).\ntrControl specified validation set obtained. use\nLeave Group Cross-Validation (LGOCV) \npicks randomly proportion p data train uses rest \nvalidation set. get better estimate \nerror, process repeated number times errors averaged.see case, full tree model performs slightly better.\nHowever, given small dataset, may significant difference\nlook statistical\ntest later.","code":"\nfit <- Zoo |>\n  train(type ~ .,\n    data = _ ,\n    method = \"rpart\",\n    control = rpart.control(minsplit = 2), # we have little data\n    tuneGrid = data.frame(cp = c(0.01, 0)),\n    trControl = trainControl(method = \"LGOCV\", \n                             p = 0.8, \n                             number = 10),\n    tuneLength = 5)\n\nfit\n## CART \n## \n## 101 samples\n##  16 predictor\n##   7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Repeated Train/Test Splits Estimated (10 reps, 80%) \n## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa \n##   0.00  0.9722    0.9616\n##   0.01  0.9667    0.9538\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final value used for the model was cp = 0."},{"path":"classification-basic-concepts.html","id":"model-evaluation","chapter":"3 Classification: Basic Concepts","heading":"3.6 Model Evaluation","text":"Models evaluated test set overlap \ntraining set. typically split data using random sampling.\nget reproducible results,\nset random number generator seed.","code":"\nset.seed(2000)"},{"path":"classification-basic-concepts.html","id":"holdout-method","chapter":"3 Classification: Basic Concepts","heading":"3.6.1 Holdout Method","text":"Test data used model building process set aside purely\ntesting model. , partition data 80% training 20%\ntesting.Now can train test set get generalization error \ntest set.","code":"\ninTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]\nZoo_train <- Zoo |> slice(inTrain)\nZoo_test <- Zoo |> slice(-inTrain)"},{"path":"classification-basic-concepts.html","id":"cross-validation-methods","chapter":"3 Classification: Basic Concepts","heading":"3.6.2 Cross-Validation Methods","text":"several cross-validation methods can use available datsa efficiently\nholdout method.\npopular method k-fold cross-validation splits data randomly \\(k\\) folds. \nholds one fold back testing trains \\(k-1\\) folds. \ndone fold resulting statistic (e.g., accuracy) averaged.\nmethod uses data efficiently holdout method.Cross validation can directly used train() using\ntrControl = trainControl(method = \"cv\", number = 10).\nmodel selection necessary give \ngeneralization error.Cross-validation runs independent can done faster \nparallel. enable multi-core support, caret uses package\nforeach need load backend. Linux, can use\ndoMC 4 cores. Windows needs different backend like doParallel\n(see caret cheat sheet ).","code":"\n## Linux backend\n# library(doMC)\n# registerDoMC(cores = 4)\n# getDoParWorkers()\n\n## Windows backend\n# library(doParallel)\n# cl <- makeCluster(4, type=\"SOCK\")\n# registerDoParallel(cl)"},{"path":"classification-basic-concepts.html","id":"hyperparameter-tuning","chapter":"3 Classification: Basic Concepts","heading":"3.7 Hyperparameter Tuning","text":"Note: section contains complete code example data used.\nfirst holds test set performing hyperparameter selection\nusing cross-validation.Hyperparameters parameters change \ntraining algorithm works. example complexity parameter\ncp rpart decision trees. Tuning hyperparameter means \nwant perform model selection pick best setting.typically first use holdout method create test set use\ncross validation using training data model selection. Let us use\n80% training hold 20% testing.package caret combines training validation hyperparameter\ntuning train() function. internally splits \ndata training validation sets thus provide \nerror estimates different hyperparameter settings. trainControl \nused choose testing performed.rpart, train tries tune cp parameter (tree complexity)\nusing accuracy chose best model. set minsplit 2 since \nmuch data. Note: Parameters used tuning (case\ncp) need set using data.frame argument tuneGrid!\nSetting control ignored.Note: Train built 10 trees using training folds \nvalue cp reported values accuracy Kappa \naverages validation folds.model using best tuning parameters using data supplied\ntrain() available fit$finalModel.caret also computes variable importance. default uses competing\nsplits (splits runners , get chosen \ntree) rpart models (see ? varImp). Toothed runner \nmany splits, never gets chosen!variable importance without competing splits.Note: models provide variable importance function. \ncase caret might calculate variable importance ignore\nmodel (see ? varImp)!Now, can estimate generalization error best model \nheld test data.Caret’s confusionMatrix() function calculates accuracy, confidence\nintervals, kappa many evaluation metrics. need use\nseparate test data create confusion matrix based \ngeneralization error.Definitions additional statistics class (including alternative names) can found\ncaret’s confusion matrix man page.notesMany classification algorithms train caret deal well\nmissing values. classification model can deal \nmissing values (e.g., rpart) use na.action = na.pass \ncall train predict. Otherwise, need remove\nobservations missing values na.omit use imputation \nreplace missing values train model. Make sure\nstill enough observations left.Make sure nominal variables (includes logical variables)\ncoded factors.class variable train caret level names \nkeywords R (e.g., TRUE FALSE). Rename , \nexample, “yes” “.”Make sure nominal variables (factors) examples \npossible values. methods might problems variable\nvalues without examples. can drop empty levels using\ndroplevels factor.Sampling train might create sample contain\nexamples values nominal (factor) variable. get\nerror message. likely happens variables \none rare value. may remove variable.","code":"\ninTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]\nZoo_train <- Zoo |> slice(inTrain)\nZoo_test <- Zoo |> slice(-inTrain)\nfit <- Zoo_train |>\n  train(type ~ .,\n    data = _ ,\n    method = \"rpart\",\n    control = rpart.control(minsplit = 2), # we have little data\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 5)\n\nfit\n## CART \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 73, 77, 75, 73, 75, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa \n##   0.00  0.9289    0.9058\n##   0.08  0.8603    0.8179\n##   0.16  0.7296    0.6422\n##   0.22  0.6644    0.5448\n##   0.32  0.4383    0.1136\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final value used for the model was cp = 0.\nlibrary(rpart.plot)\nrpart.plot(fit$finalModel, extra = 2,\n  box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \"Or\", \"Rd\", \"Pu\"))\nvarImp(fit)\n## rpart variable importance\n## \n##               Overall\n## toothedFALSE    100.0\n## feathersFALSE    79.5\n## eggsFALSE        67.7\n## milkFALSE        63.3\n## backboneFALSE    57.3\n## finsFALSE        53.5\n## hairFALSE        52.1\n## breathesFALSE    48.9\n## legs             41.4\n## tailFALSE        29.0\n## aquaticFALSE     27.5\n## airborneFALSE    26.5\n## predatorFALSE    10.6\n## venomousFALSE     1.8\n## catsizeFALSE      0.0\n## domesticFALSE     0.0\nimp <- varImp(fit, compete = FALSE)\nimp\n## rpart variable importance\n## \n##               Overall\n## milkFALSE      100.00\n## feathersFALSE   55.69\n## finsFALSE       39.45\n## aquaticFALSE    28.11\n## backboneFALSE   21.76\n## eggsFALSE       12.32\n## legs             7.28\n## tailFALSE        0.00\n## domesticFALSE    0.00\n## airborneFALSE    0.00\n## catsizeFALSE     0.00\n## toothedFALSE     0.00\n## venomousFALSE    0.00\n## hairFALSE        0.00\n## breathesFALSE    0.00\n## predatorFALSE    0.00\nggplot(imp)\npred <- predict(fit, newdata = Zoo_test)\npred\n##  [1] mammal        bird          mollusc.et.al bird         \n##  [5] mammal        mammal        insect        bird         \n##  [9] mammal        mammal        mammal        mammal       \n## [13] bird          fish          fish          reptile      \n## [17] mammal        mollusc.et.al\n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al\nconfusionMatrix(data = pred, \n                ref = Zoo_test |> pull(type))\n## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian insect\n##   mammal             8    0       0    0         0      0\n##   bird               0    4       0    0         0      0\n##   reptile            0    0       1    0         0      0\n##   fish               0    0       0    2         0      0\n##   amphibian          0    0       0    0         0      0\n##   insect             0    0       0    0         0      1\n##   mollusc.et.al      0    0       0    0         0      0\n##                Reference\n## Prediction      mollusc.et.al\n##   mammal                    0\n##   bird                      0\n##   reptile                   0\n##   fish                      0\n##   amphibian                 0\n##   insect                    0\n##   mollusc.et.al             2\n## \n## Overall Statistics\n##                                     \n##                Accuracy : 1         \n##                  95% CI : (0.815, 1)\n##     No Information Rate : 0.444     \n##     P-Value [Acc > NIR] : 4.58e-07  \n##                                     \n##                   Kappa : 1         \n##                                     \n##  Mcnemar's Test P-Value : NA        \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       1.000\n## Pos Pred Value               1.000       1.000\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.444       0.222\n## Detection Rate               0.444       0.222\n## Detection Prevalence         0.444       0.222\n## Balanced Accuracy            1.000       1.000\n##                      Class: reptile Class: fish\n## Sensitivity                  1.0000       1.000\n## Specificity                  1.0000       1.000\n## Pos Pred Value               1.0000       1.000\n## Neg Pred Value               1.0000       1.000\n## Prevalence                   0.0556       0.111\n## Detection Rate               0.0556       0.111\n## Detection Prevalence         0.0556       0.111\n## Balanced Accuracy            1.0000       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                        NA        1.0000\n## Specificity                         1        1.0000\n## Pos Pred Value                     NA        1.0000\n## Neg Pred Value                     NA        1.0000\n## Prevalence                          0        0.0556\n## Detection Rate                      0        0.0556\n## Detection Prevalence                0        0.0556\n## Balanced Accuracy                  NA        1.0000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-basic-concepts.html","id":"pitfalls-of-model-selection-and-evaluation","chapter":"3 Classification: Basic Concepts","heading":"3.8 Pitfalls of Model Selection and Evaluation","text":"measure error training set \nuse validation error generalization error estimate.\nAlways use generalization error test set!training data test sets overlap \nevaluate generalization performance. training set can come\ncontaminated things like preprocessing data together.","code":""},{"path":"classification-basic-concepts.html","id":"model-comparison","chapter":"3 Classification: Basic Concepts","heading":"3.9 Model Comparison","text":"compare three models, majority class baseline classifier, \ndecision trees k-nearest neighbors (kNN)\nclassifier. use 10-fold cross-validation hyper parameter tuning.\nCaret’s train() function refits selected model training data \nperforms cross-validation estimate generalization error. cross-validation\nresults can used compare models statistically.","code":""},{"path":"classification-basic-concepts.html","id":"build-models","chapter":"3 Classification: Basic Concepts","heading":"3.9.1 Build models","text":"Caret provide baseline classifier, package basemodels .\nfirst create weak baseline model always predicts majority\nclass mammal.second model default decision tree.third model kNN classifier, classifier discussed\nnext Chapter. kNN uses Euclidean distance objects.\nLogicals used 0-1 variables. make sure range \nvariables compatible, \nask train scale data using\npreProcess = \"scale\".Compare accuracy kappa distributions final model folds.caret provides visualizations. \nexample, boxplot compare accuracy kappa distribution (\n10 folds).see baseline predictive power produces consistently kappa 0. KNN performs consistently best.\nfind one models statistically better , can use statistical test.p-values gives us probability seeing even extreme value\n(difference accuracy kappa) given null hypothesis (difference\n= 0) true. better classifier, p-value “significant,” .e., less \n.05 0.01. diff automatically applies Bonferroni correction \nmultiple comparisons adjust p-value upwards. case, CART kNN perform significantly better\nbaseline classifiers. difference CART kNN \nsignificant 0.05 level, kNN might slightly better.","code":"\nbaseline <- Zoo_train |> train(type ~ .,\n  method = basemodels::dummyClassifier,\n  data = _,\n  strategy = \"constant\",\n  constant = \"mammal\",\n  trControl = trainControl(method = \"cv\" \n                           ))\nbaseline\n## dummyClassifier \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 73, 74, 73, 75, 74, 77, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.4047    0\nrpartFit <- Zoo_train |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        tuneLength = 10,\n        trControl = trainControl(method = \"cv\")\n  )\nrpartFit\n## CART \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 75, 75, 73, 76, 72, ... \n## Resampling results across tuning parameters:\n## \n##   cp       Accuracy  Kappa \n##   0.00000  0.7841    0.7195\n##   0.03556  0.7841    0.7195\n##   0.07111  0.7841    0.7195\n##   0.10667  0.7841    0.7182\n##   0.14222  0.7841    0.7182\n##   0.17778  0.7271    0.6369\n##   0.21333  0.7071    0.6109\n##   0.24889  0.5940    0.4423\n##   0.28444  0.5940    0.4423\n##   0.32000  0.4968    0.2356\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final value used for the model was cp = 0.1422.\nknnFit <- Zoo_train |> \n  train(type ~ .,\n        data = _,\n        method = \"knn\",\n        preProcess = \"scale\",\n          tuneLength = 10,\n          trControl = trainControl(method = \"cv\")\n  )\nknnFit\n## k-Nearest Neighbors \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## Pre-processing: scaled (16) \n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 74, 74, 74, 74, 75, ... \n## Resampling results across tuning parameters:\n## \n##   k   Accuracy  Kappa \n##    5  0.9092    0.8835\n##    7  0.8715    0.8301\n##    9  0.8579    0.8113\n##   11  0.8590    0.8131\n##   13  0.8727    0.8302\n##   15  0.8727    0.8302\n##   17  0.8490    0.7989\n##   19  0.8490    0.7967\n##   21  0.7943    0.7219\n##   23  0.7943    0.7217\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final value used for the model was k = 5.\nresamps <- resamples(list(\n    baseline = baseline,    \n  CART = rpartFit,\n        kNearestNeighbors = knnFit\n        ))\n\nsummary(resamps)\n## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: baseline, CART, kNearestNeighbors \n## Number of resamples: 10 \n## \n## Accuracy \n##                     Min. 1st Qu. Median   Mean 3rd Qu.\n## baseline          0.3000   0.375 0.3875 0.4047  0.4444\n## CART              0.7143   0.733 0.7639 0.7841  0.8429\n## kNearestNeighbors 0.7778   0.875 0.8819 0.9092  1.0000\n##                    Max. NA's\n## baseline          0.500    0\n## CART              0.875    0\n## kNearestNeighbors 1.000    0\n## \n## Kappa \n##                     Min. 1st Qu. Median   Mean 3rd Qu.\n## baseline          0.0000  0.0000 0.0000 0.0000  0.0000\n## CART              0.6316  0.6622 0.6994 0.7182  0.7917\n## kNearestNeighbors 0.7188  0.8307 0.8486 0.8835  1.0000\n##                    Max. NA's\n## baseline          0.000    0\n## CART              0.814    0\n## kNearestNeighbors 1.000    0\nbwplot(resamps, layout = c(3, 1))\ndifs <- diff(resamps)\ndifs\n## \n## Call:\n## diff.resamples(x = resamps)\n## \n## Models: baseline, CART, kNearestNeighbors \n## Metrics: Accuracy, Kappa \n## Number of differences: 3 \n## p-value adjustment: bonferroni\nsummary(difs)\n## \n## Call:\n## summary.diff.resamples(object = difs)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## Accuracy \n##                   baseline CART   kNearestNeighbors\n## baseline                   -0.379 -0.504           \n## CART              5.19e-06        -0.125           \n## kNearestNeighbors 4.03e-08 0.031                   \n## \n## Kappa \n##                   baseline CART   kNearestNeighbors\n## baseline                   -0.718 -0.884           \n## CART              5.79e-10        -0.165           \n## kNearestNeighbors 2.87e-09 0.0206"},{"path":"classification-basic-concepts.html","id":"feature-selection","chapter":"3 Classification: Basic Concepts","heading":"3.10 Feature Selection*","text":"Decision trees implicitly select features splitting, can also\nselect features apply learning algorithm.\nSince different features lead different models, choosing\nbest set features also type \nmodel selection.Many feature selection methods implemented FSelector package.","code":"\nlibrary(FSelector)"},{"path":"classification-basic-concepts.html","id":"univariate-feature-importance-score","chapter":"3 Classification: Basic Concepts","heading":"3.10.1 Univariate Feature Importance Score","text":"scores measure related feature class variable.\ndiscrete features (case), chi-square statistic can \nused derive score.can plot importance descending order (using reorder order factor\nlevels used ggplot).Picking best features called feature ranking approach.\npick 5 highest-ranked features.Use selected features build model (Fselector provides\n.simple.formula).many alternative ways calculate univariate importance\nscores (see package FSelector). (also) work continuous\nfeatures. One example information gain ratio based entropy \nused decision tree induction.","code":"\nweights <- Zoo_train |> \n  chi.squared(type ~ ., data = _) |>\n  as_tibble(rownames = \"feature\") |>\n  arrange(desc(attr_importance))\n\nweights\n## # A tibble: 16 × 2\n##    feature  attr_importance\n##    <chr>              <dbl>\n##  1 feathers           1    \n##  2 milk               1    \n##  3 backbone           1    \n##  4 toothed            0.981\n##  5 eggs               0.959\n##  6 breathes           0.917\n##  7 hair               0.906\n##  8 fins               0.845\n##  9 legs               0.834\n## 10 airborne           0.818\n## 11 tail               0.779\n## 12 aquatic            0.725\n## 13 catsize            0.602\n## 14 venomous           0.520\n## 15 predator           0.374\n## 16 domestic           0.256\nggplot(weights,\n  aes(x = attr_importance, \n      y = reorder(feature, attr_importance))) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Importance score\") + \n  ylab(\"Feature\")\nsubset <- cutoff.k(weights |> \n                   column_to_rownames(\"feature\"), \n                   5)\nsubset\n## [1] \"feathers\" \"milk\"     \"backbone\" \"toothed\"  \"eggs\"\nf <- as.simple.formula(subset, \"type\")\nf\n## type ~ feathers + milk + backbone + toothed + eggs\n## <environment: 0x5b61f2448f88>\nm <- Zoo_train |> rpart(f, data = _)\nrpart.plot(m, extra = 2, roundint = FALSE)\nZoo_train |> \n  gain.ratio(type ~ ., data = _) |>\n  as_tibble(rownames = \"feature\") |>\n  arrange(desc(attr_importance))\n## # A tibble: 16 × 2\n##    feature  attr_importance\n##    <chr>              <dbl>\n##  1 milk               1    \n##  2 backbone           1    \n##  3 feathers           1    \n##  4 toothed            0.959\n##  5 eggs               0.907\n##  6 breathes           0.845\n##  7 hair               0.781\n##  8 fins               0.689\n##  9 legs               0.689\n## 10 airborne           0.633\n## 11 tail               0.573\n## 12 aquatic            0.474\n## 13 venomous           0.429\n## 14 catsize            0.310\n## 15 domestic           0.115\n## 16 predator           0.110"},{"path":"classification-basic-concepts.html","id":"feature-subset-selection-1","chapter":"3 Classification: Basic Concepts","heading":"3.10.2 Feature Subset Selection","text":"Often, features related calculating importance feature\nindependently optimal. can use greedy search heuristics. \nexample cfs uses correlation/entropy best first search.disadvantage method model want train may use\ncorrelation/entropy. can use actual model using\nblack-box defined evaluator function\ncalculate score maximized.\ntypically best method, since can use model\nselection.\nFirst, define evaluation\nfunction builds model given subset features calculates \nquality score. use average 5 bootstrap samples\n(method = \"cv\" can also used instead), tuning (faster),\naverage accuracy score.Start features (class variable type)several (greedy) search strategies available. run \ncommented . Remove comment one time\ntry types feature selection.","code":"\nZoo_train |> \n  cfs(type ~ ., data = _)\n##  [1] \"hair\"     \"feathers\" \"eggs\"     \"milk\"     \"toothed\" \n##  [6] \"backbone\" \"breathes\" \"fins\"     \"legs\"     \"tail\"\nevaluator <- function(subset) {\n  model <- Zoo_train |> \n    train(as.simple.formula(subset, \"type\"),\n          data = _,\n          method = \"rpart\",\n          trControl = trainControl(method = \"boot\", number = 5),\n          tuneLength = 0)\n  \n  results <- model$resample$Accuracy\n  \n  cat(\"Trying features:\", paste(subset, collapse = \" + \"), \"\\n\")\n  \n  m <- mean(results)\n  cat(\"Accuracy:\", round(m, 2), \"\\n\\n\")\n  m\n}\nfeatures <- Zoo_train |> \n  colnames() |> \n  setdiff(\"type\")\n#subset <- backward.search(features, evaluator)\n#subset <- forward.search(features, evaluator)\n#subset <- best.first.search(features, evaluator)\n#subset <- hill.climbing.search(features, evaluator)\n#subset"},{"path":"classification-basic-concepts.html","id":"using-dummy-variables-for-factors","chapter":"3 Classification: Basic Concepts","heading":"3.10.3 Using Dummy Variables for Factors","text":"Nominal features (factors) often encoded series 0-1 dummy\nvariables. example, let us try predict animal predator\ngiven type. First use original encoding type factor\nseveral values.Note: splits use multiple values. Building tree become\nextremely slow factor many levels (different values) since \ntree check possible splits two subsets. situation\navoided.Convert type set 0-1 dummy variables using class2ind. See\nalso ? dummyVars package caret.Using caret original factor encoding automatically translates\nfactors (type) 0-1 dummy variables (e.g., typeinsect = 0).\nreason models directly use factors caret\ntries consistently work .Note: use fixed value tuning parameter cp, \ncreate tuning grid contains value.","code":"\ntree_predator <- Zoo_train |> \n  rpart(predator ~ type, data = _)\nrpart.plot(tree_predator, extra = 2, roundint = FALSE)\nZoo_train_dummy <- as_tibble(class2ind(Zoo_train$type)) |> \n  mutate(across(everything(), as.factor)) |>\n  add_column(predator = Zoo_train$predator)\nZoo_train_dummy\n## # A tibble: 83 × 8\n##    mammal bird  reptile fish  amphibian insect mollusc.et.al\n##    <fct>  <fct> <fct>   <fct> <fct>     <fct>  <fct>        \n##  1 1      0     0       0     0         0      0            \n##  2 0      0     0       1     0         0      0            \n##  3 1      0     0       0     0         0      0            \n##  4 1      0     0       0     0         0      0            \n##  5 1      0     0       0     0         0      0            \n##  6 1      0     0       0     0         0      0            \n##  7 0      0     0       1     0         0      0            \n##  8 0      0     0       1     0         0      0            \n##  9 1      0     0       0     0         0      0            \n## 10 1      0     0       0     0         0      0            \n## # ℹ 73 more rows\n## # ℹ 1 more variable: predator <fct>\ntree_predator <- Zoo_train_dummy |> \n  rpart(predator ~ ., \n        data = _,\n        control = rpart.control(minsplit = 2, cp = 0.01))\nrpart.plot(tree_predator, roundint = FALSE)\nfit <- Zoo_train |> \n  train(predator ~ type, \n        data = _, \n        method = \"rpart\",\n        control = rpart.control(minsplit = 2),\n        tuneGrid = data.frame(cp = 0.01))\nfit\n## CART \n## \n## 83 samples\n##  1 predictor\n##  2 classes: 'TRUE', 'FALSE' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... \n## Resampling results:\n## \n##   Accuracy  Kappa  \n##   0.54      0.07527\n## \n## Tuning parameter 'cp' was held constant at a value of 0.01\nrpart.plot(fit$finalModel, extra = 2)"},{"path":"classification-basic-concepts.html","id":"exercises-1","chapter":"3 Classification: Basic Concepts","heading":"3.11 Exercises*","text":"use Palmer penguin data exercises.Create R markdown file code discussion following .\nRemember, complete approach described section Hyperparameter Tuning.Split data training test set.Create rpart decision tree predict species. deal \nmissing values.Experiment setting minsplit rpart make sure tuneLength \nleast 5.\nDiscuss model selection process (hyperparameter tuning) final\nmodel chosen.Visualize tree discuss splits mean.Calculate variable importance fitted model. variables \nimportant? variables matter?Use test set evaluate generalization error accuracy.","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm\n##   <chr>   <chr>              <dbl>         <dbl>\n## 1 Adelie  Torgersen           39.1          18.7\n## 2 Adelie  Torgersen           39.5          17.4\n## 3 Adelie  Torgersen           40.3          18  \n## 4 Adelie  Torgersen           NA            NA  \n## 5 Adelie  Torgersen           36.7          19.3\n## 6 Adelie  Torgersen           39.3          20.6\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"classification-alternative-techniques.html","id":"classification-alternative-techniques","chapter":"4 Classification: Alternative Techniques","heading":"4 Classification: Alternative Techniques","text":"chapter introduces different types classifiers. also\ndiscusses important problem class imbalance data options\ndeal . addition, chapter compares visually \ndecision boundaries used different algorithms. provide \nbetter understanding model bias different algorithms .","code":""},{"path":"classification-alternative-techniques.html","id":"packages-used-in-this-chapter-2","chapter":"4 Classification: Alternative Techniques","heading":"Packages Used in this Chapter","text":"packages used chapter :basemodels (Y.-J. Chen et al. 2023)C50 (Kuhn Quinlan 2025)caret (Kuhn 2024)e1071 (Meyer et al. 2024)klaR (Roever et al. 2023)lattice (Sarkar 2025)MASS (Ripley Venables 2025)mlbench (Leisch Dimitriadou 2024)nnet (Ripley 2025)palmerpenguins (Horst, Hill, Gorman 2022)randomForest (Breiman et al. 2024)rpart (Therneau Atkinson 2025)RWeka (Hornik 2023)sampling (Tillé Matei 2025)scales (Wickham, Pedersen, Seidel 2025)tidyverse (Wickham 2023b)xgboost (T. Chen et al. 2025)","code":"\npkgs <- c(\"basemodels\", \"C50\", \"caret\", \"e1071\", \"klaR\", \n          \"lattice\", \"MASS\", \"mlbench\", \"nnet\", \"palmerpenguins\", \n          \"randomForest\", \"rpart\", \"RWeka\", \"scales\", \"tidyverse\", \n          \"xgboost\", \"sampling\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"classification-alternative-techniques.html","id":"types-of-classifiers","chapter":"4 Classification: Alternative Techniques","heading":"4.1 Types of Classifiers","text":"Many different classification algorithms\nproposed literature.\nchapter, apply popular methods.","code":""},{"path":"classification-alternative-techniques.html","id":"set-up-the-training-and-test-data","chapter":"4 Classification: Alternative Techniques","heading":"4.1.1 Set up the Training and Test Data","text":"use Zoo dataset included R package mlbench\n(may install ). Zoo dataset containing 17 (mostly\nlogical) variables different 101 animals data frame 17\ncolumns (hair, feathers, eggs, milk, airborne, aquatic, predator,\ntoothed, backbone, breathes, venomous, fins, legs, tail, domestic,\ncatsize, type). convert data frame tidyverse tibble\n(optional).use package caret \nmake preparing training sets building classification (\nregression) models easier. great cheat sheet can found\n.Multi-core support can used cross-validation. Note: \ncommented work rJava used RWeka-based\nclassifiers\n.Test data used model building process needs set\naside purely testing model completely built. \nuse 80% training.hyperparameter tuning, use 10-fold cross-validation.Note: careful many NA values data.\ntrain() cross-validation many fail cases. \ncase can remove features (columns) many NAs, omit\nNAs using na.omit() use imputation replace \nreasonable values (e.g., feature mean via kNN). Highly\nimbalanced datasets also problematic since chance \nfold contain examples class leading hard \nunderstand error message.","code":"\nlibrary(tidyverse)\ndata(Zoo, package = \"mlbench\")\n\nZoo <- as_tibble(Zoo)\nZoo\n## # A tibble: 101 × 17\n##    hair  feathers eggs  milk  airborne aquatic predator\n##    <lgl> <lgl>    <lgl> <lgl> <lgl>    <lgl>   <lgl>   \n##  1 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  2 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  3 FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE    \n##  4 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  5 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  6 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  7 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  8 FALSE FALSE    TRUE  FALSE FALSE    TRUE    FALSE   \n##  9 FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE    \n## 10 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n## # ℹ 91 more rows\n## # ℹ 10 more variables: toothed <lgl>, backbone <lgl>,\n## #   breathes <lgl>, venomous <lgl>, fins <lgl>, legs <int>,\n## #   tail <lgl>, domestic <lgl>, catsize <lgl>, type <fct>\nlibrary(caret)\n##library(doMC, quietly = TRUE)\n##registerDoMC(cores = 4)\n##getDoParWorkers()\ninTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]\nZoo_train <- Zoo |> slice(inTrain)\nZoo_test <- Zoo |> slice(-inTrain)"},{"path":"classification-alternative-techniques.html","id":"rule-based-classifier-part","chapter":"4 Classification: Alternative Techniques","heading":"4.2 Rule-based classifier: PART","text":"model selection results shown table. selected model:PART returns decision list, .e., ordered rule set. example,\nfirst rule shows animal feathers milk mammal.\nordered rule sets, decision first matching rule used.","code":"\nrulesFit <- Zoo_train |> train(type ~ .,\n  method = \"PART\",\n  data = _,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\"))\nrulesFit\n## Rule-Based Classifier \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 74, 74, 74, 75, 75, ... \n## Resampling results across tuning parameters:\n## \n##   threshold  pruned  Accuracy  Kappa \n##   0.0100     yes     0.9542    0.9382\n##   0.0100     no      0.9288    0.9060\n##   0.1325     yes     0.9542    0.9382\n##   0.1325     no      0.9288    0.9060\n##   0.2550     yes     0.9542    0.9382\n##   0.2550     no      0.9288    0.9060\n##   0.3775     yes     0.9542    0.9382\n##   0.3775     no      0.9288    0.9060\n##   0.5000     yes     0.9542    0.9382\n##   0.5000     no      0.9288    0.9060\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final values used for the model were threshold =\n##  0.5 and pruned = yes.\nrulesFit$finalModel\n## PART decision list\n## ------------------\n## \n## feathersTRUE <= 0 AND\n## milkTRUE > 0: mammal (33.0)\n## \n## feathersTRUE > 0: bird (16.0)\n## \n## backboneTRUE <= 0 AND\n## airborneTRUE <= 0: mollusc.et.al (9.0/1.0)\n## \n## airborneTRUE <= 0 AND\n## finsTRUE > 0: fish (11.0)\n## \n## airborneTRUE > 0: insect (6.0)\n## \n## aquaticTRUE > 0: amphibian (5.0/1.0)\n## \n## : reptile (3.0)\n## \n## Number of Rules  :   7"},{"path":"classification-alternative-techniques.html","id":"nearest-neighbor-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.3 Nearest Neighbor Classifier","text":"K-Nearest neighbor classifiers classify new data point looking \nmajority class labels k nearest neighbors training data set.\nused kNN implementation uses Euclidean distance determine data points\nnear , data needs standardized\n(scaled) first. legs measured 0 6 \nvariables 0 1. Scaling z-scores can directly performed \npreprocessing train using parameter preProcess = \"scale\".\\(k\\) value typically choose odd number get clear majority.kNN classifiers lazy models meaning instead learning, just\nkeep complete dataset. final model just gives us summary statistic class labels training data.","code":"\nknnFit <- Zoo_train |> train(type ~ .,\n  method = \"knn\",\n  data = _,\n  preProcess = \"scale\",\n  tuneGrid = data.frame(k = c(1, 3, 5, 7, 9)),\n    trControl = trainControl(method = \"cv\"))\nknnFit\n## k-Nearest Neighbors \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## Pre-processing: scaled (16) \n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 73, 74, 75, 74, 74, ... \n## Resampling results across tuning parameters:\n## \n##   k  Accuracy  Kappa \n##   1  0.9428    0.9256\n##   3  0.9410    0.9234\n##   5  0.9299    0.9088\n##   7  0.9031    0.8728\n##   9  0.9031    0.8721\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final value used for the model was k = 1.\nknnFit$finalModel\n## 1-nearest neighbor model\n## Training set outcome distribution:\n## \n##        mammal          bird       reptile          fish \n##            33            16             4            11 \n##     amphibian        insect mollusc.et.al \n##             4             7             8"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.4 Naive Bayes Classifier","text":"Caret’s train formula interface translates logicals factors dummy\nvariables classifier interprets numbers used Gaussian naive Bayes estimation. avoid , directly specify x y.final model contains prior probabilities class.conditional probabilities table feature. brevity,\nshow tables first three features. example,\ncondition probability\n\\(P(\\text{hair} = \\text{TRUE} | \\text{class} = \\text{mammal})\\) \n0.9641.","code":"\nNBFit <- train(x = as.data.frame(Zoo_train[, -ncol(Zoo_train)]), \n               y = pull(Zoo_train, \"type\"),\n               method = \"nb\",\n               tuneGrid = data.frame(fL = c(.2, .5, 1, 5), \n                                     usekernel = TRUE, adjust = 1),\n               trControl = trainControl(method = \"cv\"))\nNBFit\n## Naive Bayes \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 75, 74, 76, 74, 75, ... \n## Resampling results across tuning parameters:\n## \n##   fL   Accuracy  Kappa \n##   0.2  0.9274    0.9057\n##   0.5  0.9274    0.9057\n##   1.0  0.9163    0.8904\n##   5.0  0.8927    0.8550\n## \n## Tuning parameter 'usekernel' was held constant at a\n##  value of TRUE\n## Tuning parameter 'adjust' was held\n##  constant at a value of 1\n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final values used for the model were fL =\n##  0.2, usekernel = TRUE and adjust = 1.\nNBFit$finalModel$apriori\n## grouping\n##        mammal          bird       reptile          fish \n##       0.39759       0.19277       0.04819       0.13253 \n##     amphibian        insect mollusc.et.al \n##       0.04819       0.08434       0.09639\nNBFit$finalModel$tables[1:3]\n## $hair\n##                var\n## grouping          FALSE    TRUE\n##   mammal        0.03593 0.96407\n##   bird          0.98780 0.01220\n##   reptile       0.95455 0.04545\n##   fish          0.98246 0.01754\n##   amphibian     0.95455 0.04545\n##   insect        0.43243 0.56757\n##   mollusc.et.al 0.97619 0.02381\n## \n## $feathers\n##                var\n## grouping           FALSE     TRUE\n##   mammal        0.994012 0.005988\n##   bird          0.012195 0.987805\n##   reptile       0.954545 0.045455\n##   fish          0.982456 0.017544\n##   amphibian     0.954545 0.045455\n##   insect        0.972973 0.027027\n##   mollusc.et.al 0.976190 0.023810\n## \n## $eggs\n##                var\n## grouping          FALSE    TRUE\n##   mammal        0.96407 0.03593\n##   bird          0.01220 0.98780\n##   reptile       0.27273 0.72727\n##   fish          0.01754 0.98246\n##   amphibian     0.04545 0.95455\n##   insect        0.02703 0.97297\n##   mollusc.et.al 0.14286 0.85714"},{"path":"classification-alternative-techniques.html","id":"bayesian-network","chapter":"4 Classification: Alternative Techniques","heading":"4.5 Bayesian Network","text":"Bayesian networks covered . R good\nsupport modeling Bayesian Networks. example \npackage bnlearn.","code":""},{"path":"classification-alternative-techniques.html","id":"logreg","chapter":"4 Classification: Alternative Techniques","heading":"4.6 Logistic Regression","text":"Logistic regression powerful classification method\nalways tried \none first models.\ndetailed discussion code available section\nLogistic Regression Appendix.Regular logistic regression predicts one outcome coded \nbinary variable. Since data several\nclasses, use multinomial logistic regression,\nalso called log-linear model\nextension logistic regresses multi-class problems.\nCaret uses nnet::multinom() implements penalized multinomial regression.coefficients log odds ratios\nmeasured default class (mammal).\nnegative log odds ratio means odds go increase \nvalue predictor. predictor \npositive log-odds ratio increases odds. example,\nmodel , hair=TRUE negative coefficient \nbird feathers=TRUE large positive coefficient.","code":"\nlogRegFit <- Zoo_train |> train(type ~ .,\n  method = \"multinom\",\n  data = _,\n  trace = FALSE, # suppress some output\n    trControl = trainControl(method = \"cv\"))\nlogRegFit\n## Penalized Multinomial Regression \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 76, 75, 74, 74, 74, ... \n## Resampling results across tuning parameters:\n## \n##   decay  Accuracy  Kappa \n##   0e+00  0.8704    0.8306\n##   1e-04  0.9038    0.8749\n##   1e-01  0.9056    0.8777\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final value used for the model was decay = 0.1.\nlogRegFit$finalModel\n## Call:\n## nnet::multinom(formula = .outcome ~ ., data = dat, decay = param$decay, \n##     trace = FALSE)\n## \n## Coefficients:\n##               (Intercept) hairTRUE feathersTRUE eggsTRUE\n## bird             -0.33330  -1.0472       2.9696   0.8278\n## reptile           0.01303  -2.0808      -1.0891   0.6731\n## fish             -0.17462  -0.2762      -0.1135   1.8817\n## amphibian        -1.28295  -1.5165      -0.2698   0.6801\n## insect           -0.75300  -0.3903      -0.1445   0.8980\n## mollusc.et.al     1.52104  -1.2287      -0.2492   0.9320\n##               milkTRUE airborneTRUE aquaticTRUE\n## bird           -1.2523      1.17310     -0.1594\n## reptile        -2.1800     -0.51796     -1.0890\n## fish           -1.3571     -0.09009      0.5093\n## amphibian      -1.6014     -0.36649      1.6271\n## insect         -1.0130      1.37404     -1.0752\n## mollusc.et.al  -0.9035     -1.17882      0.7160\n##               predatorTRUE toothedTRUE backboneTRUE\n## bird               0.22312     -1.7846       0.4736\n## reptile            0.04172     -0.2003       0.8968\n## fish              -0.33094      0.4118       0.2768\n## amphibian         -0.13993      0.7399       0.2557\n## insect            -1.11743     -1.1852      -1.5725\n## mollusc.et.al      0.83070     -1.7390      -2.6045\n##               breathesTRUE venomousTRUE  finsTRUE     legs\n## bird                0.1337      -0.3278 -0.545979 -0.59910\n## reptile            -0.5039       1.2776 -1.192197 -0.24200\n## fish               -1.9709      -0.4204  1.472416 -1.15775\n## amphibian           0.4594       0.1611 -0.628746  0.09302\n## insect              0.1341      -0.2567 -0.002527  0.59118\n## mollusc.et.al      -0.6287       0.8411 -0.206104  0.12091\n##               tailTRUE domesticTRUE catsizeTRUE\n## bird            0.5947      0.14176     -0.1182\n## reptile         1.1863     -0.40893     -0.4305\n## fish            0.3226      0.08636     -0.3132\n## amphibian      -1.3529     -0.40545     -1.3581\n## insect         -1.6908     -0.24924     -1.0416\n## mollusc.et.al  -0.7353     -0.22601     -0.7079\n## \n## Residual Deviance: 35.46 \n## AIC: 239.5"},{"path":"classification-alternative-techniques.html","id":"artificial-neural-network-ann","chapter":"4 Classification: Alternative Techniques","heading":"4.7 Artificial Neural Network (ANN)","text":"Standard networks input layer, output layer \nsingle hidden layer.input layer size 16, one input feature \noutput layer size 7 representing 7 classes.\nModel selection chose network architecture hidden\nlayer 9 units\nresulting 223 learned weights.\nSince model considered black-box model network architecture\nused variables shown summary.deep Learning, R offers packages using tensorflow\nKeras.","code":"\nnnetFit <- Zoo_train |> train(type ~ .,\n  method = \"nnet\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\"),\n  trace = FALSE # no progress output\n  )\nnnetFit\n## Neural Network \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 74, 76, 76, 75, 74, ... \n## Resampling results across tuning parameters:\n## \n##   size  decay  Accuracy  Kappa \n##   1     0e+00  0.7536    0.6369\n##   1     1e-04  0.6551    0.4869\n##   1     1e-03  0.8201    0.7562\n##   1     1e-02  0.7979    0.7285\n##   1     1e-01  0.7263    0.6265\n##   3     0e+00  0.8344    0.7726\n##   3     1e-04  0.8219    0.7622\n##   3     1e-03  0.8798    0.8420\n##   3     1e-02  0.9063    0.8752\n##   3     1e-01  0.8809    0.8405\n##   5     0e+00  0.8319    0.7768\n##   5     1e-04  0.8448    0.7922\n##   5     1e-03  0.9020    0.8660\n##   5     1e-02  0.9131    0.8816\n##   5     1e-01  0.9020    0.8680\n##   7     0e+00  0.8405    0.7893\n##   7     1e-04  0.9031    0.8689\n##   7     1e-03  0.9020    0.8678\n##   7     1e-02  0.9131    0.8816\n##   7     1e-01  0.8909    0.8539\n##   9     0e+00  0.9145    0.8847\n##   9     1e-04  0.9131    0.8804\n##   9     1e-03  0.9242    0.8974\n##   9     1e-02  0.9131    0.8818\n##   9     1e-01  0.9020    0.8680\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final values used for the model were size = 9 and\n##  decay = 0.001.\nnnetFit$finalModel\n## a 16-9-7 network with 223 weights\n## inputs: hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \n## output(s): .outcome \n## options were - softmax modelling  decay=0.001"},{"path":"classification-alternative-techniques.html","id":"support-vector-machines","chapter":"4 Classification: Alternative Techniques","heading":"4.8 Support Vector Machines","text":"use linear support vector machine.\nSupport vector machines can use kernels create non-linear decision boundaries.\nmethod can changed \"svmPoly\" \"svmRadial\" \nuse kernels. choice kernel typically make experimentation.support vectors determining decision boundary stored model.","code":"\nsvmFit <- Zoo_train |> train(type ~.,\n  method = \"svmLinear\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\"))\nsvmFit\n## Support Vector Machines with Linear Kernel \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 77, 75, 74, 76, 75, 74, ... \n## Resampling results:\n## \n##   Accuracy  Kappa \n##   0.9317    0.9105\n## \n## Tuning parameter 'C' was held constant at a value of 1\nsvmFit$finalModel\n## Support Vector Machine object of class \"ksvm\" \n## \n## SV type: C-svc  (classification) \n##  parameter : cost C = 1 \n## \n## Linear (vanilla) kernel function. \n## \n## Number of Support Vectors : 42 \n## \n## Objective Function Value : -0.1432 -0.22 -0.1501 -0.1756 -0.0943 -0.1047 -0.2804 -0.0808 -0.1544 -0.0902 -0.1138 -0.1727 -0.5886 -0.1303 -0.1847 -0.1161 -0.0472 -0.0803 -0.125 -0.15 -0.5704 \n## Training error : 0"},{"path":"classification-alternative-techniques.html","id":"ensemble-methods","chapter":"4 Classification: Alternative Techniques","heading":"4.9 Ensemble Methods","text":"Many ensemble methods available R. cover code two\npopular methods.","code":""},{"path":"classification-alternative-techniques.html","id":"random-forest","chapter":"4 Classification: Alternative Techniques","heading":"4.9.1 Random Forest","text":"default number trees 500 \nmtry determines number variables randomly sampled candidates\nsplit. number tradeoff larger number allows tree\npick better splits, smaller\nnumber increases independence trees.model set 500 trees prediction made applying trees \nusing majority vote.Since random forests use bagging (bootstrap sampling train trees),\nremaining data can used like test set. resulting error\ncalled --bag (OOB) error gives estimate \ngeneralization error. model also shows confusion matrix based \nOOB error.","code":"\nrandomForestFit <- Zoo_train |> train(type ~ .,\n  method = \"rf\",\n  data = _,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\"))\nrandomForestFit\n## Random Forest \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 73, 72, 76, 75, 73, 77, ... \n## Resampling results across tuning parameters:\n## \n##   mtry  Accuracy  Kappa \n##    2    0.9287    0.9054\n##    5    0.9432    0.9246\n##    9    0.9398    0.9197\n##   12    0.9498    0.9331\n##   16    0.9498    0.9331\n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final value used for the model was mtry = 12.\nrandomForestFit$finalModel\n## \n## Call:\n##  randomForest(x = x, y = y, mtry = param$mtry) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 12\n## \n##         OOB estimate of  error rate: 6.02%\n## Confusion matrix:\n##               mammal bird reptile fish amphibian insect\n## mammal            33    0       0    0         0      0\n## bird               0   16       0    0         0      0\n## reptile            0    0       3    0         1      0\n## fish               0    0       0   11         0      0\n## amphibian          0    0       1    0         3      0\n## insect             0    0       0    0         0      6\n## mollusc.et.al      0    0       0    0         0      2\n##               mollusc.et.al class.error\n## mammal                    0      0.0000\n## bird                      0      0.0000\n## reptile                   0      0.2500\n## fish                      0      0.0000\n## amphibian                 0      0.2500\n## insect                    1      0.1429\n## mollusc.et.al             6      0.2500"},{"path":"classification-alternative-techniques.html","id":"gradient-boosted-decision-trees-xgboost","chapter":"4 Classification: Alternative Techniques","heading":"4.9.2 Gradient Boosted Decision Trees (xgboost)","text":"idea gradient boosting learn base model learn\nsuccessive models predict correct error previous models.\nTypically, tree models used.final model complicated set trees, summary information\nshown.","code":"\nxgboostFit <- Zoo_train |> train(type ~ .,\n  method = \"xgbTree\",\n  data = _,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\"),\n  tuneGrid = expand.grid(\n    nrounds = 20,\n    max_depth = 3,\n    colsample_bytree = .6,\n    eta = 0.1,\n    gamma=0,\n    min_child_weight = 1,\n    subsample = .5\n  ))\nxgboostFit\n## eXtreme Gradient Boosting \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 74, 76, 73, 76, 74, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.92      0.893\n## \n## Tuning parameter 'nrounds' was held constant at a value\n##  held constant at a value of 1\n## Tuning parameter\n##  'subsample' was held constant at a value of 0.5\nxgboostFit$finalModel\n## ##### xgb.Booster\n## raw: 111.6 Kb \n## call:\n##   xgboost::xgb.train(params = list(eta = param$eta, max_depth = param$max_depth, \n##     gamma = param$gamma, colsample_bytree = param$colsample_bytree, \n##     min_child_weight = param$min_child_weight, subsample = param$subsample), \n##     data = x, nrounds = param$nrounds, num_class = length(lev), \n##     objective = \"multi:softprob\")\n## params (as set within xgb.train):\n##   eta = \"0.1\", max_depth = \"3\", gamma = \"0\", colsample_bytree = \"0.6\", min_child_weight = \"1\", subsample = \"0.5\", num_class = \"7\", objective = \"multi:softprob\", validate_parameters = \"TRUE\"\n## xgb.attributes:\n##   niter\n## callbacks:\n##   cb.print.evaluation(period = print_every_n)\n## # of features: 16 \n## niter: 20\n## nfeatures : 16 \n## xNames : hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \n## problemType : Classification \n## tuneValue :\n##    nrounds max_depth eta gamma colsample_bytree\n## 1      20         3 0.1     0              0.6\n##   min_child_weight subsample\n## 1                1       0.5\n## obsLevels : mammal bird reptile fish amphibian insect mollusc.et.al \n## param :\n##  list()"},{"path":"classification-alternative-techniques.html","id":"model-comparison-1","chapter":"4 Classification: Alternative Techniques","heading":"4.10 Model Comparison","text":"first create weak baseline model always predicts majority\nclass mammal.kappa 0 clearly indicates baseline model power.collect performance metrics models trained \ndata.summary statistics shows performance. see \nmethods well easy data set, baseline model performs\nexpected poorly.Perform inference differences models. metric, \npair-wise differences computed tested assess \ndifference equal zero. default Bonferroni correction \nmultiple comparison used. Differences shown upper triangle\np-values lower triangle.perform similarly well except baseline model (differences first row\nnegative p-values first column <.05 indicating\nnull-hypothesis difference 0 can rejected).\nmodels similarly well data. choose random\nforest model evaluate generalization performance held-\ntest set.Calculate confusion matrix held-test data.","code":"\nbaselineFit <- Zoo_train |> train(type ~ .,\n  method = basemodels::dummyClassifier,\n  data = _,\n  strategy = \"constant\",\n  constant = \"mammal\",\n  trControl = trainControl(method = \"cv\" \n                           ))\nbaselineFit\n## dummyClassifier \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 76, 75, 75, 74, 76, 76, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.4025    0\nresamps <- resamples(list(\n  baseline = baselineFit,\n  PART = rulesFit,\n    kNearestNeighbors = knnFit,\n  NBayes = NBFit,\n  logReg = logRegFit,\n  ANN = nnetFit,\n  SVM = svmFit,\n  RandomForest = randomForestFit,\n  XGBoost = xgboostFit\n    ))\nresamps\n## \n## Call:\n## resamples.default(x = list(baseline = baselineFit, PART\n##  svmFit, RandomForest = randomForestFit, XGBoost\n##  = xgboostFit))\n## \n## Models: baseline, PART, kNearestNeighbors, NBayes, logReg, ANN, SVM, RandomForest, XGBoost \n## Number of resamples: 10 \n## Performance metrics: Accuracy, Kappa \n## Time estimates for: everything, final model fit\nsummary(resamps)\n## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: baseline, PART, kNearestNeighbors, NBayes, logReg, ANN, SVM, RandomForest, XGBoost \n## Number of resamples: 10 \n## \n## Accuracy \n##                     Min. 1st Qu. Median   Mean 3rd Qu. Max.\n## baseline          0.3000  0.3750 0.4286 0.4025  0.4405  0.5\n## PART              0.8750  0.8889 1.0000 0.9542  1.0000  1.0\n## kNearestNeighbors 0.8750  0.8889 0.9500 0.9428  1.0000  1.0\n## NBayes            0.7778  0.8750 0.9444 0.9274  1.0000  1.0\n## logReg            0.7500  0.8056 0.9444 0.9056  1.0000  1.0\n## ANN               0.8571  0.8750 0.8889 0.9242  1.0000  1.0\n## SVM               0.7778  0.8785 0.9500 0.9317  1.0000  1.0\n## RandomForest      0.8571  0.8835 1.0000 0.9498  1.0000  1.0\n## XGBoost           0.7778  0.8472 0.9500 0.9200  1.0000  1.0\n##                   NA's\n## baseline             0\n## PART                 0\n## kNearestNeighbors    0\n## NBayes               0\n## logReg               0\n## ANN                  0\n## SVM                  0\n## RandomForest         0\n## XGBoost              0\n## \n## Kappa \n##                     Min. 1st Qu. Median   Mean 3rd Qu. Max.\n## baseline          0.0000  0.0000 0.0000 0.0000       0    0\n## PART              0.8222  0.8542 1.0000 0.9382       1    1\n## kNearestNeighbors 0.8333  0.8588 0.9324 0.9256       1    1\n## NBayes            0.7353  0.8220 0.9297 0.9057       1    1\n## logReg            0.6596  0.7560 0.9308 0.8777       1    1\n## ANN               0.7879  0.8241 0.8615 0.8974       1    1\n## SVM               0.7273  0.8413 0.9342 0.9105       1    1\n## RandomForest      0.8000  0.8483 1.0000 0.9331       1    1\n## XGBoost           0.7000  0.7848 0.9367 0.8930       1    1\n##                   NA's\n## baseline             0\n## PART                 0\n## kNearestNeighbors    0\n## NBayes               0\n## logReg               0\n## ANN                  0\n## SVM                  0\n## RandomForest         0\n## XGBoost              0\nlibrary(lattice)\nbwplot(resamps, layout = c(3, 1))\ndifs <- diff(resamps)\nsummary(difs)\n## \n## Call:\n## summary.diff.resamples(object = difs)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## Accuracy \n##                   baseline PART     kNearestNeighbors\n## baseline                   -0.55171 -0.54032         \n## PART              8.37e-07           0.01139         \n## kNearestNeighbors 3.20e-07 1                         \n## NBayes            7.48e-06 1        1                \n## logReg            1.81e-05 1        1                \n## ANN               9.66e-06 1        1                \n## SVM               2.87e-06 1        1                \n## RandomForest      9.73e-07 1        1                \n## XGBoost           2.04e-05 1        1                \n##                   NBayes   logReg   ANN      SVM     \n## baseline          -0.52492 -0.50310 -0.52175 -0.52921\n## PART               0.02679  0.04861  0.02996  0.02250\n## kNearestNeighbors  0.01540  0.03722  0.01857  0.01111\n## NBayes                      0.02183  0.00317 -0.00429\n## logReg            1                 -0.01865 -0.02611\n## ANN               1        1                 -0.00746\n## SVM               1        1        1                \n## RandomForest      1        1        1        1       \n## XGBoost           1        1        1        1       \n##                   RandomForest XGBoost \n## baseline          -0.54738     -0.51754\n## PART               0.00433      0.03417\n## kNearestNeighbors -0.00706      0.02278\n## NBayes            -0.02246      0.00738\n## logReg            -0.04428     -0.01444\n## ANN               -0.02563      0.00421\n## SVM               -0.01817      0.01167\n## RandomForest                    0.02984\n## XGBoost           1                    \n## \n## Kappa \n##                   baseline PART     kNearestNeighbors\n## baseline                   -0.93815 -0.92557         \n## PART              1.41e-09           0.01258         \n## kNearestNeighbors 1.37e-09 1                         \n## NBayes            1.94e-08 1        1                \n## logReg            4.17e-07 1        1                \n## ANN               6.28e-09 1        1                \n## SVM               1.45e-08 1        1                \n## RandomForest      3.67e-09 1        1                \n## XGBoost           1.03e-07 1        1                \n##                   NBayes   logReg   ANN      SVM     \n## baseline          -0.90570 -0.87768 -0.89737 -0.91054\n## PART               0.03245  0.06047  0.04078  0.02761\n## kNearestNeighbors  0.01987  0.04789  0.02820  0.01503\n## NBayes                      0.02802  0.00833 -0.00485\n## logReg            1                 -0.01969 -0.03287\n## ANN               1        1                 -0.01317\n## SVM               1        1        1                \n## RandomForest      1        1        1        1       \n## XGBoost           1        1        1        1       \n##                   RandomForest XGBoost \n## baseline          -0.93305     -0.89296\n## PART               0.00510      0.04519\n## kNearestNeighbors -0.00748      0.03261\n## NBayes            -0.02735      0.01274\n## logReg            -0.05538     -0.01529\n## ANN               -0.03568      0.00441\n## SVM               -0.02251      0.01758\n## RandomForest                    0.04009\n## XGBoost           1\npr <- predict(randomForestFit, Zoo_test)\npr\n##  [1] mammal        fish          mollusc.et.al fish         \n##  [5] mammal        insect        mammal        mammal       \n##  [9] mammal        mammal        bird          mammal       \n## [13] mammal        bird          reptile       bird         \n## [17] mollusc.et.al bird         \n## 7 Levels: mammal bird reptile fish amphibian ... mollusc.et.al\nconfusionMatrix(pr, reference = Zoo_test$type)\n## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian insect\n##   mammal             8    0       0    0         0      0\n##   bird               0    4       0    0         0      0\n##   reptile            0    0       1    0         0      0\n##   fish               0    0       0    2         0      0\n##   amphibian          0    0       0    0         0      0\n##   insect             0    0       0    0         0      1\n##   mollusc.et.al      0    0       0    0         0      0\n##                Reference\n## Prediction      mollusc.et.al\n##   mammal                    0\n##   bird                      0\n##   reptile                   0\n##   fish                      0\n##   amphibian                 0\n##   insect                    0\n##   mollusc.et.al             2\n## \n## Overall Statistics\n##                                     \n##                Accuracy : 1         \n##                  95% CI : (0.815, 1)\n##     No Information Rate : 0.444     \n##     P-Value [Acc > NIR] : 4.58e-07  \n##                                     \n##                   Kappa : 1         \n##                                     \n##  Mcnemar's Test P-Value : NA        \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       1.000\n## Pos Pred Value               1.000       1.000\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.444       0.222\n## Detection Rate               0.444       0.222\n## Detection Prevalence         0.444       0.222\n## Balanced Accuracy            1.000       1.000\n##                      Class: reptile Class: fish\n## Sensitivity                  1.0000       1.000\n## Specificity                  1.0000       1.000\n## Pos Pred Value               1.0000       1.000\n## Neg Pred Value               1.0000       1.000\n## Prevalence                   0.0556       0.111\n## Detection Rate               0.0556       0.111\n## Detection Prevalence         0.0556       0.111\n## Balanced Accuracy            1.0000       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                        NA        1.0000\n## Specificity                         1        1.0000\n## Pos Pred Value                     NA        1.0000\n## Neg Pred Value                     NA        1.0000\n## Prevalence                          0        0.0556\n## Detection Rate                      0        0.0556\n## Detection Prevalence                0        0.0556\n## Balanced Accuracy                  NA        1.0000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-alternative-techniques.html","id":"class-imbalance","chapter":"4 Classification: Alternative Techniques","heading":"4.11 Class Imbalance","text":"Classifiers hard time learn data much \nobservations one class (called majority class). called\nclass imbalance problem especially problematic \nimportant identify members minority class. setting minority\nclass also often called positive class since property\nwant identify. example want identify \npatients positive rare disease.good article problem \nsolutions.examples, use Zoo dataset.always need check class imbalance problem. Just look class\ndistribution.see moderate class imbalance many mammals \namphibians reptiles data set. build classifier, \nlikely perform well identifying amphibians\nreptiles. effect seen building simple decision tree.resulting tree leaf nodes rare animal types meaning \nidentify .Deciding animal reptile class imbalanced problem.\nset problem changing class variable make binary\nreptile/non-reptile classification problem.forget make class variable factor (nominal variable)\nget regression tree instead classification tree.checking class distribution, see classification model now\nhighly imbalances 5 reptiles vs. 96 non-reptiles.graph shows data highly imbalanced encountering non-reptile\n20 times likely encountering reptile.\ntry several\ndifferent options next subsections.split data test training data can evaluate well \nclassifiers different options work. use 50/50 split make sure\ntest set samples rare reptile class.","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\ndata(Zoo, package = \"mlbench\")\nggplot(Zoo, aes(y = type)) + geom_bar()\ntree_default <- Zoo |> \n  rpart(type ~ ., data = _)\ntree_default\n## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##  1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##    2) milk>=0.5 41  0 mammal (1 0 0 0 0 0 0) *\n##    3) milk< 0.5 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##      6) feathers>=0.5 20  0 bird (0 1 0 0 0 0 0) *\n##      7) feathers< 0.5 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##       14) fins>=0.5 13  0 fish (0 0 0 1 0 0 0) *\n##       15) fins< 0.5 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##         30) backbone>=0.5 9  4 reptile (0 0 0.56 0 0.44 0 0) *\n##         31) backbone< 0.5 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56) *\nlibrary(rpart.plot)\nrpart.plot(tree_default, extra = 2)\nZoo_reptile <- Zoo |> \n  mutate(type = factor(Zoo$type == \"reptile\", \n                       levels = c(FALSE, TRUE),\n                       labels = c(\"nonreptile\", \"reptile\")))\nsummary(Zoo_reptile)\n##     hair          feathers          eggs        \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:58        FALSE:81        FALSE:42       \n##  TRUE :43        TRUE :20        TRUE :59       \n##                                                 \n##                                                 \n##                                                 \n##     milk          airborne        aquatic       \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:60        FALSE:77        FALSE:65       \n##  TRUE :41        TRUE :24        TRUE :36       \n##                                                 \n##                                                 \n##                                                 \n##   predator        toothed         backbone      \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:45        FALSE:40        FALSE:18       \n##  TRUE :56        TRUE :61        TRUE :83       \n##                                                 \n##                                                 \n##                                                 \n##   breathes        venomous          fins        \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:21        FALSE:93        FALSE:84       \n##  TRUE :80        TRUE :8         TRUE :17       \n##                                                 \n##                                                 \n##                                                 \n##       legs         tail          domestic      \n##  Min.   :0.00   Mode :logical   Mode :logical  \n##  1st Qu.:2.00   FALSE:26        FALSE:88       \n##  Median :4.00   TRUE :75        TRUE :13       \n##  Mean   :2.84                                  \n##  3rd Qu.:4.00                                  \n##  Max.   :8.00                                  \n##   catsize                type   \n##  Mode :logical   nonreptile:96  \n##  FALSE:57        reptile   : 5  \n##  TRUE :44                       \n##                                 \n##                                 \n## \nggplot(Zoo_reptile, aes(y = type)) + geom_bar()\nset.seed(1234)\n\ninTrain <- createDataPartition(y = Zoo_reptile$type, p = .5)[[1]]\ntraining_reptile <- Zoo_reptile |> slice(inTrain)\ntesting_reptile <- Zoo_reptile |> slice(-inTrain)"},{"path":"classification-alternative-techniques.html","id":"option-1-use-the-data-as-is-and-hope-for-the-best","chapter":"4 Classification: Alternative Techniques","heading":"4.11.1 Option 1: Use the Data As Is and Hope For The Best","text":"Warnings: “missing values resampled performance\nmeasures.” means test folds used hyper parameter tuning\ncontain examples \nclasses. likely strong class imbalance small datasets.tree predicts everything non-reptile. look error \ntest set.Accuracy high, exactly -information rate\nkappa zero. Sensitivity also zero, meaning \nidentify positive (reptile). cost missing positive \nmuch larger cost associated misclassifying negative,\naccuracy good measure! dealing imbalance, \nconcerned accuracy, want increase \nsensitivity, .e., chance identify positive examples.\napproach work!Note: positive class value (one want detect) \nset manually reptile using positive = \"reptile\". Otherwise\nsensitivity/specificity correctly calculated.","code":"\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"))\n## Warning in nominalTrainWorkflow(x = x, y = y, wts =\n## weights, info = trainInfo, : There were missing values in\n## resampled performance measures.\nfit\n## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 47, 46, 46, 45, 46, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.9467    0    \n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         48       2\n##   reptile             0       0\n##                                         \n##                Accuracy : 0.96          \n##                  95% CI : (0.863, 0.995)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.677         \n##                                         \n##                   Kappa : 0             \n##                                         \n##  Mcnemar's Test P-Value : 0.480         \n##                                         \n##             Sensitivity : 0.00          \n##             Specificity : 1.00          \n##          Pos Pred Value :  NaN          \n##          Neg Pred Value : 0.96          \n##              Prevalence : 0.04          \n##          Detection Rate : 0.00          \n##    Detection Prevalence : 0.00          \n##       Balanced Accuracy : 0.50          \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"option-2-balance-data-with-resampling","chapter":"4 Classification: Alternative Techniques","heading":"4.11.2 Option 2: Balance Data With Resampling","text":"use stratified sampling replacement (oversample \nminority/positive class). also use SMOTE (package DMwR)\nsampling strategies (e.g., package unbalanced). use\n50+50 observations (Note: many samples positive/minority class\nchosen several times).Build tree using balanced data.resulting tree reasonable accuracy/kappa balanced data.\nHowevver, real data distribution balanced data, \ncheck model unbalanced testing data.accuracy information rate! However,\nkappa (improvement accuracy randomness) increased. importantly,\nsensitivity (\nability identify reptiles) now one meaning classifier able \nidentify reptiles.tradeoff sensitivity specificity (many \nidentified animals really reptiles). tradeoff can controlled\nusing sample proportions. can sample reptiles increase\nsensitivity cost lower specificity (effect seen\ndata since test set reptiles).Note: used sample ratio 1:10 meaning value identifying\n1 reptile much misscassifying 10 non-reptiles. related cost\nusd cost-sensitive classifier introduced Option 4 .","code":"\nlibrary(sampling)\n## \n## Attaching package: 'sampling'\n## The following object is masked from 'package:caret':\n## \n##     cluster\nset.seed(1000) # for repeatability\n\nid <- strata(training_reptile, stratanames = \"type\", \n             size = c(50, 50), method = \"srswr\")\ntraining_reptile_balanced <- training_reptile |> \n  slice(id$ID_unit)\n\ntable(training_reptile_balanced$type)\n## \n## nonreptile    reptile \n##         50         50\nfit <- training_reptile_balanced |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"),\n        control = rpart.control(minsplit = 5))\n\nfit\n## CART \n## \n## 100 samples\n##  16 predictor\n##   2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 90, 90, 90, 90, 90, 90, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa\n##   0.18  0.81      0.62 \n##   0.30  0.63      0.26 \n##   0.34  0.53      0.06 \n## \n## Accuracy was used to select the optimal model using\n##  the largest value.\n## The final value used for the model was cp = 0.18.\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         19       0\n##   reptile            29       2\n##                                         \n##                Accuracy : 0.42          \n##                  95% CI : (0.282, 0.568)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1             \n##                                         \n##                   Kappa : 0.05          \n##                                         \n##  Mcnemar's Test P-Value : 2e-07         \n##                                         \n##             Sensitivity : 1.0000        \n##             Specificity : 0.3958        \n##          Pos Pred Value : 0.0645        \n##          Neg Pred Value : 1.0000        \n##              Prevalence : 0.0400        \n##          Detection Rate : 0.0400        \n##    Detection Prevalence : 0.6200        \n##       Balanced Accuracy : 0.6979        \n##                                         \n##        'Positive' Class : reptile       \n## \nid <- strata(training_reptile, stratanames = \"type\", \n             size = c(50, 500), method = \"srswr\")\ntraining_reptile_balanced <- training_reptile |> \n  slice(id$ID_unit)\ntable(training_reptile_balanced$type)\n## \n## nonreptile    reptile \n##         50        500\nfit <- training_reptile_balanced |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        trControl = trainControl(method = \"cv\"),\n        control = rpart.control(minsplit = 5))\n\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         33       0\n##   reptile            15       2\n##                                         \n##                Accuracy : 0.7           \n##                  95% CI : (0.554, 0.821)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1.000000      \n##                                         \n##                   Kappa : 0.15          \n##                                         \n##  Mcnemar's Test P-Value : 0.000301      \n##                                         \n##             Sensitivity : 1.000         \n##             Specificity : 0.688         \n##          Pos Pred Value : 0.118         \n##          Neg Pred Value : 1.000         \n##              Prevalence : 0.040         \n##          Detection Rate : 0.040         \n##    Detection Prevalence : 0.340         \n##       Balanced Accuracy : 0.844         \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"option-3-build-a-larger-tree-and-use-predicted-probabilities","chapter":"4 Classification: Alternative Techniques","heading":"4.11.3 Option 3: Build A Larger Tree and use Predicted Probabilities","text":"Increase complexity require less data splitting node. \nalso use AUC (area ROC) tuning metric. need \nspecify two class summary function. Note tree still trying\nimprove accuracy data AUC! also enable class\nprobabilities since want predict probabilities later.Note: Accuracy high, close \n-information rate!","code":"\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        tuneLength = 10,\n        trControl = trainControl(\n                        method = \"cv\",\n                        classProbs = TRUE,  ## for predict with type=\"prob\"\n                        summaryFunction=twoClassSummary),  ## for ROC\n        metric = \"ROC\",\n        control = rpart.control(minsplit = 3))\n## Warning in nominalTrainWorkflow(x = x, y = y, wts =\n## weights, info = trainInfo, : There were missing values in\n## resampled performance measures.\nfit\n## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 45, 46, 45, 47, 47, ... \n## Resampling results:\n## \n##   ROC     Sens  Spec\n##   0.4333  0.92  0   \n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         48       2\n##   reptile             0       0\n##                                         \n##                Accuracy : 0.96          \n##                  95% CI : (0.863, 0.995)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.677         \n##                                         \n##                   Kappa : 0             \n##                                         \n##  Mcnemar's Test P-Value : 0.480         \n##                                         \n##             Sensitivity : 0.00          \n##             Specificity : 1.00          \n##          Pos Pred Value :  NaN          \n##          Neg Pred Value : 0.96          \n##              Prevalence : 0.04          \n##          Detection Rate : 0.00          \n##    Detection Prevalence : 0.00          \n##       Balanced Accuracy : 0.50          \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"create-a-biased-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.11.3.1 Create A Biased Classifier","text":"can create classifier detect reptiles \nexpense misclassifying non-reptiles. equivalent increasing\ncost misclassifying reptile non-reptile. usual rule \npredict node majority class test data \nnode. binary classification problem means probability \n>50%. following, reduce threshold 1% . \nmeans new observation ends leaf node 1% \nreptiles training observation classified \nreptile. data set small works better data.Note accuracy goes information rate.\nHowever, measures based idea errors \ncost. important now able find \nreptiles.","code":"\nprob <- predict(fit, testing_reptile, type = \"prob\")\ntail(prob)\n##      nonreptile reptile\n## tuna     1.0000 0.00000\n## vole     0.9615 0.03846\n## wasp     0.5000 0.50000\n## wolf     0.9615 0.03846\n## worm     1.0000 0.00000\n## wren     0.9615 0.03846\npred <- ifelse(prob[,\"reptile\"] >= 0.01, \"reptile\", \"nonreptile\") |> \n  as.factor()\n\nconfusionMatrix(data = pred,\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         13       0\n##   reptile            35       2\n##                                         \n##                Accuracy : 0.3           \n##                  95% CI : (0.179, 0.446)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1             \n##                                         \n##                   Kappa : 0.029         \n##                                         \n##  Mcnemar's Test P-Value : 9.08e-09      \n##                                         \n##             Sensitivity : 1.0000        \n##             Specificity : 0.2708        \n##          Pos Pred Value : 0.0541        \n##          Neg Pred Value : 1.0000        \n##              Prevalence : 0.0400        \n##          Detection Rate : 0.0400        \n##    Detection Prevalence : 0.7400        \n##       Balanced Accuracy : 0.6354        \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"plot-the-roc-curve","chapter":"4 Classification: Alternative Techniques","heading":"4.11.3.2 Plot the ROC Curve","text":"Since binary classification problem classifier \npredicts probability observation reptile, can also\nuse receiver operating characteristic\n(ROC)\ncurve. ROC curve different cutoff thresholds \nprobability used connected line. area \ncurve represents single number well classifier works (\ncloser one, better).","code":"\nlibrary(\"pROC\")\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\nr <- roc(testing_reptile$type == \"reptile\", prob[,\"reptile\"])\n## Setting levels: control = FALSE, case = TRUE\n## Setting direction: controls < cases\nr\n## \n## Call:\n## roc.default(response = testing_reptile$type == \"reptile\", predictor = prob[,     \"reptile\"])\n## \n## Data: prob[, \"reptile\"] in 48 controls (testing_reptile$type == \"reptile\" FALSE) < 2 cases (testing_reptile$type == \"reptile\" TRUE).\n## Area under the curve: 0.766\nggroc(r) + geom_abline(intercept = 1, slope = 1, color = \"darkgrey\")"},{"path":"classification-alternative-techniques.html","id":"option-4-use-a-cost-sensitive-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.11.4 Option 4: Use a Cost-Sensitive Classifier","text":"implementation CART rpart can use cost matrix making\nsplitting decisions (parameter loss). matrix formwhere TP TN 0. make FN expensive \ncost 100 compared cost FP 1.warning “missing values resampled performance\nmeasures” means folds contain reptiles (\nclass imbalance) thus performance measures \ncalculates.high cost false negatives results classifier \nmiss reptile.Note: Using cost-sensitive classifier often best option.\nUnfortunately, classification algorithms (\nimplementation) ability consider misclassification\ncost. Cost-sensitivity can added ANNs changing optimized\nloss function pure cross-entropy loss \nweighted version. Using custom loss function possible \ndeep learning libraries.","code":"TP FP \nFN TN\ncost <- matrix(c(\n  0,   1,\n  100, 0\n), byrow = TRUE, nrow = 2)\ncost\n##      [,1] [,2]\n## [1,]    0    1\n## [2,]  100    0\nfit <- training_reptile |> \n  train(type ~ .,\n        data = _,\n        method = \"rpart\",\n        parms = list(loss = cost),\n        trControl = trainControl(method = \"cv\"))\n## Warning in nominalTrainWorkflow(x = x, y = y, wts =\n## weights, info = trainInfo, : There were missing values in\n## resampled performance measures.\nfit\n## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 46, 46, 46, 46, 45, ... \n## Resampling results:\n## \n##   Accuracy  Kappa  \n##   0.5417    -0.0409\n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n                ref = testing_reptile$type, positive = \"reptile\")\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         39       0\n##   reptile             9       2\n##                                         \n##                Accuracy : 0.82          \n##                  95% CI : (0.686, 0.914)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.99998       \n##                                         \n##                   Kappa : 0.257         \n##                                         \n##  Mcnemar's Test P-Value : 0.00766       \n##                                         \n##             Sensitivity : 1.000         \n##             Specificity : 0.812         \n##          Pos Pred Value : 0.182         \n##          Neg Pred Value : 1.000         \n##              Prevalence : 0.040         \n##          Detection Rate : 0.040         \n##    Detection Prevalence : 0.220         \n##       Balanced Accuracy : 0.906         \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"comparing-decision-boundaries-of-popular-classification-techniques","chapter":"4 Classification: Alternative Techniques","heading":"4.12 Comparing Decision Boundaries of Popular Classification Techniques*","text":"Classifiers create decision boundaries discriminate classes.\nDifferent classifiers able create different shapes decision\nboundaries (e.g., strictly linear) thus classifiers\nmay perform better certain datasets. section, visualize \ndecision boundaries found several popular classification methods.following function defines plot adds decision boundary\n(black lines) difference classification probability\ntwo best classes\n(color intensity; indifference 0 shown white) evaluating\nclassifier\nevenly spaced grid points. Note low resolution make\nevaluation faster also make decision boundary look like \nsmall steps even straight line.","code":"\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(caret)\ndecisionplot <- function(model, data, class_var, \n  predict_type = c(\"class\", \"prob\"), resolution = 3 * 72) {\n  # resolution is set to 72 dpi for 3 inches wide images. \n  \n  y <- data |> pull(class_var)\n  x <- data |> dplyr::select(-all_of(class_var))\n  \n  # resubstitution accuracy\n  prediction <- predict(model, x, type = predict_type[1])\n  \n  # LDA returns a list\n  if(is.list(prediction)) prediction <- prediction$class\n  \n  prediction <- factor(prediction, levels = levels(y))\n  cm <- confusionMatrix(data = prediction, \n                        reference = y)\n  acc <- cm$overall[\"Accuracy\"]\n  \n  # evaluate model on a grid\n  r <- sapply(x[, 1:2], range, na.rm = TRUE)\n  xs <- seq(r[1,1], r[2,1], length.out = resolution)\n  ys <- seq(r[1,2], r[2,2], length.out = resolution)\n  g <- cbind(rep(xs, each = resolution), rep(ys, \n                                             time = resolution))\n  colnames(g) <- colnames(r)\n  g <- as_tibble(g)\n  \n  # guess how to get class labels from predict\n  # (unfortunately not very consistent between models)\n  cl <- predict(model, g, type = predict_type[1])\n    \n  prob <- NULL\n  if(is.list(cl)) { # LDA returns a list\n    prob <- cl$posterior\n    cl <- cl$class\n  } else if (inherits(model, \"svm\")) \n    prob <- attr(predict(model, g, probability = TRUE), \"probabilities\")\n  else \n    if(!is.na(predict_type[2]))\n      try(prob <- predict(model, g, type = predict_type[2]))\n  \n  # We visualize the difference in probability/score between\n  # the winning class and the second best class. We only use\n  # probability if the classifier's predict function supports it.\n  delta_prob <- 1\n  if(!is.null(prob))\n    \n    try({\n      if (any(rowSums(prob) != 1))\n        prob <- cbind(prob, 1 - rowSums(prob))\n    \n      max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))\n      delta_prob <- max_prob[,1] - max_prob[,2]\n    }, silent = TRUE) \n  \n  cl <- factor(cl, levels = levels(y))\n  \n  g <- g |> add_column(prediction = cl, \n                       delta_prob = delta_prob)\n  \n  ggplot(g, mapping = aes(\n    x = .data[[colnames(g)[1]]], \n    y = .data[[colnames(g)[2]]])) +\n    geom_raster(mapping = aes(fill = prediction, \n                              alpha = delta_prob)) +\n    geom_contour(mapping = aes(z = as.numeric(prediction)), \n      bins = length(levels(cl)), \n      linewidth = .5, \n      color = \"black\") +\n    geom_point(data = data, mapping =  aes(\n      x = .data[[colnames(data)[1]]], \n      y = .data[[colnames(data)[2]]],\n      shape = .data[[class_var]]), \n      alpha = .7) + \n    scale_alpha_continuous(range = c(0,1), \n                           limits = c(0,1)) + \n    labs(subtitle = paste(\"Training accuracy:\", round(acc, 2)))\n}"},{"path":"classification-alternative-techniques.html","id":"iris-dataset","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1 Iris Dataset","text":"easier visualization, use two dimensions Iris dataset.original data.\nColor used show density.\nNote overplotting several points position.\nuse geom_jitter() instead geom_point().","code":"\nset.seed(1000)\ndata(iris)\niris <- as_tibble(iris)\n\nx <- iris |> dplyr::select(Sepal.Length, Sepal.Width, Species)\n# Note: package MASS overwrites the select function.\n\nx\n## # A tibble: 150 × 3\n##    Sepal.Length Sepal.Width Species\n##           <dbl>       <dbl> <fct>  \n##  1          5.1         3.5 setosa \n##  2          4.9         3   setosa \n##  3          4.7         3.2 setosa \n##  4          4.6         3.1 setosa \n##  5          5           3.6 setosa \n##  6          5.4         3.9 setosa \n##  7          4.6         3.4 setosa \n##  8          5           3.4 setosa \n##  9          4.4         2.9 setosa \n## 10          4.9         3.1 setosa \n## # ℹ 140 more rows\nggplot(x, aes(x = Sepal.Length, \n              y = Sepal.Width, \n              fill = Species)) +  \n  stat_density_2d(geom = \"polygon\", \n                  aes(alpha = after_stat(level))) +\n  geom_point()"},{"path":"classification-alternative-techniques.html","id":"nearest-neighbor-classifier-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1.1 Nearest Neighbor Classifier","text":"try several values \\(k\\).Increasing \\(k\\) smooths decision boundary. \\(k=1\\), see white\nareas around points flowers two classes spot.\n, algorithm randomly chooses class prediction resulting\nmeandering decision boundary. predictions area \nstable every time ask class, may get different\nclass.Note: crazy lines white areas artifact visualization. \nclassifier randomly selects class.","code":"\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 1)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (1 neighbor)\")\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 3)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (3 neighbors)\")\nmodel <- x |> caret::knn3(Species ~ ., data = _, k = 9)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"kNN (9 neighbors)\")"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1.2 Naive Bayes Classifier","text":"Use Gaussian naive Bayes classifier.GBN finds good model advantage hyperparameters needed.","code":"\nmodel <- x |> e1071::naiveBayes(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"Naive Bayes\")"},{"path":"classification-alternative-techniques.html","id":"linear-discriminant-analysis","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1.3 Linear Discriminant Analysis","text":"LDA finds linear decision boundaries.Linear decision boundaries work dataset LDA works well.","code":"\nmodel <- x |> MASS::lda(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"LDA\")"},{"path":"classification-alternative-techniques.html","id":"multinomial-logistic-regression","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1.4 Multinomial Logistic Regression","text":"Multinomial logistic regression extension logistic regression\nproblems two classes.","code":"\nmodel <- x |> nnet::multinom(Species ~., data = _)\n## # weights:  12 (6 variable)\n## initial  value 164.791843 \n## iter  10 value 62.715967\n## iter  20 value 59.808291\n## iter  30 value 55.445984\n## iter  40 value 55.375704\n## iter  50 value 55.346472\n## iter  60 value 55.301707\n## iter  70 value 55.253532\n## iter  80 value 55.243230\n## iter  90 value 55.230241\n## iter 100 value 55.212479\n## final  value 55.212479 \n## stopped after 100 iterations\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(titel = \"Multinomial Logistic Regression\")\n## Ignoring unknown labels:\n## • titel : \"Multinomial Logistic Regression\""},{"path":"classification-alternative-techniques.html","id":"decision-trees","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1.5 Decision Trees","text":"Compare different types decision trees.","code":"\nmodel <- x |> rpart::rpart(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"CART\")\nmodel <- x |> rpart::rpart(Species ~ ., data = _,\n  control = rpart::rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"CART (overfitting)\")\nmodel <- x |> C50::C5.0(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"C5.0\")"},{"path":"classification-alternative-techniques.html","id":"ensemble-random-forest","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1.6 Ensemble: Random Forest","text":"Use ensemble method.default settings Random forest, model seems overfit \ntraining data. data probably alleviate issue.","code":"\nmodel <- x |> randomForest::randomForest(Species ~ ., data = _)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"Random Forest\")"},{"path":"classification-alternative-techniques.html","id":"support-vector-machine","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1.7 Support Vector Machine","text":"Compare SVMs different kernel functions.linear SVM (without kernel) produces straight lines works well iris data.\nkernels also well, sigmoid kernel seems find \nstrange decision boundary indicates data \nlinear decision boundary\nprojected space.","code":"\nmodel <- x |> e1071::svm(Species ~ ., data = _, \n                         kernel = \"linear\", probability = TRUE)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (linear kernel)\")\nmodel <- x |> e1071::svm(Species ~ ., data = _, \n                         kernel = \"radial\", probability = TRUE)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (radial kernel)\")\nmodel <- x |> e1071::svm(Species ~ ., data = _, \n                         kernel = \"polynomial\", probability = TRUE)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (polynomial kernel)\")\nmodel <- x |> e1071::svm(Species ~ ., data = _, \n                         kernel = \"sigmoid\", probability = TRUE)\ndecisionplot(model, x, class_var = \"Species\") + \n  labs(title = \"SVM (sigmoid kernel)\")"},{"path":"classification-alternative-techniques.html","id":"single-layer-feed-forward-neural-networks","chapter":"4 Classification: Alternative Techniques","heading":"4.12.1.8 Single Layer Feed-forward Neural Networks","text":"Use simple network one hidden layer. try \ndifferent number \nneurons hidden layer.simple data set, 2 neurons produce best classifier.\nmodel starts overfit 6 neurons. ran ANN twice 100\nneurons, two different decision boundaries indicate variability \nANN many neurons high.","code":"\nset.seed(1234)\n\nmodel <- x |> nnet::nnet(Species ~ ., data = _, \n                        size = 1, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"ANN (1 neuron)\")\nmodel <- x |> nnet::nnet(Species ~ ., data = _, \n                        size = 2, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"ANN (2 neurons)\")\nmodel <- x |> nnet::nnet(Species ~ ., data = _, \n                        size = 4, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"ANN (4 neurons)\")\nmodel <- x |> nnet::nnet(Species ~ ., data = _, \n                        size = 6, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"ANN (6 neurons)\")\nmodel <- x |> nnet::nnet(Species ~ ., data = _, \n                        size = 20, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"ANN (20 neurons)\")\nmodel <- x |> nnet::nnet(Species ~ ., data = _, \n                        size = 100, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"ANN (100 neurons)\")\nmodel <- x |> nnet::nnet(Species ~ ., data = _, \n                        size = 100, trace = FALSE)\ndecisionplot(model, x, class_var  = \"Species\", \n             predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"ANN 2nd try (100 neurons)\")"},{"path":"classification-alternative-techniques.html","id":"circle-dataset","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2 Circle Dataset","text":"dataset challenging classification algorithms since\noptimal decision boundary circle around class center.","code":"\nset.seed(1000)\n\nx <- mlbench::mlbench.circle(500)\n\n# You can also experiment with the following datasets.\n#x <- mlbench::mlbench.cassini(500)\n#x <- mlbench::mlbench.spirals(500, sd = .1)\n#x <- mlbench::mlbench.smiley(500)\n\nx <- cbind(as.data.frame(x$x), factor(x$classes))\ncolnames(x) <- c(\"x\", \"y\", \"class\")\nx <- as_tibble(x)\nx\n## # A tibble: 500 × 3\n##          x       y class\n##      <dbl>   <dbl> <fct>\n##  1 -0.344   0.448  1    \n##  2  0.518   0.915  2    \n##  3 -0.772  -0.0913 1    \n##  4  0.382   0.412  1    \n##  5  0.0328  0.438  1    \n##  6 -0.865  -0.354  2    \n##  7  0.477   0.640  2    \n##  8  0.167  -0.809  2    \n##  9 -0.568  -0.281  1    \n## 10 -0.488   0.638  2    \n## # ℹ 490 more rows\nggplot(x, aes(x = x, y = y, color = class)) + \n  geom_point()"},{"path":"classification-alternative-techniques.html","id":"nearest-neighbor-classifier-2","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2.1 Nearest Neighbor Classifier","text":"Compare kNN classifiers different values \\(k\\).k-Nearest find smooth decision boundary, tends \noverfit training data low values \\(k\\).","code":"\nmodel <- x |> caret::knn3(class ~ ., data = _, k = 1)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"kNN (1 neighbor)\")\nmodel <- x |> caret::knn3(class ~ ., data = _, k = 10)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"kNN (10 neighbors)\")"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier-2","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2.2 Naive Bayes Classifier","text":"Gaussian naive Bayes classifier works well data.","code":"\nmodel <- x |> e1071::naiveBayes(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + \n  labs(title = \"naive Bayes\")"},{"path":"classification-alternative-techniques.html","id":"linear-discriminant-analysis-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2.3 Linear Discriminant Analysis","text":"LDA find good model since true decision boundary \nlinear.","code":"\nmodel <- x |> MASS::lda(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + labs(title = \"LDA\")"},{"path":"classification-alternative-techniques.html","id":"multinomial-logistic-regression-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2.4 Multinomial Logistic Regression","text":"Multinomial logistic regression extension logistic regression\nproblems two classes.Logistic regression also tries find linear\ndecision boundary fails.","code":"\nmodel <- x |> nnet::multinom(class ~., data = _)\n## # weights:  4 (3 variable)\n## initial  value 346.573590 \n## final  value 346.308371 \n## converged\ndecisionplot(model, x, class_var = \"class\") + \n  labs(titel = \"Multinomial Logistic Regression\")\n## Ignoring unknown labels:\n## • titel : \"Multinomial Logistic Regression\""},{"path":"classification-alternative-techniques.html","id":"decision-trees-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2.5 Decision Trees","text":"Compare different decision tree algorithms.Decision trees well restriction can create\ncuts parallel axes.","code":"\nmodel <- x |> rpart::rpart(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"CART\")\nmodel <- x |> rpart::rpart(class ~ ., data = _,\n  control = rpart::rpart.control(cp = 0, minsplit = 1))\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"CART (overfitting)\")\nmodel <- x |> C50::C5.0(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"C5.0\")"},{"path":"classification-alternative-techniques.html","id":"ensemble-random-forest-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2.6 Ensemble: Random Forest","text":"Try random forest dataset.","code":"\nlibrary(randomForest)\n## randomForest 4.7-1.2\n## Type rfNews() to see new features/changes/bug fixes.\n## \n## Attaching package: 'randomForest'\n## The following object is masked from 'package:dplyr':\n## \n##     combine\n## The following object is masked from 'package:ggplot2':\n## \n##     margin\nmodel <- x |> randomForest(class ~ ., data = _)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"Random Forest\")"},{"path":"classification-alternative-techniques.html","id":"support-vector-machine-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2.7 Support Vector Machine","text":"Compare SVMs different kernels.linear SVM work data set. SMV radial kernel performs\nwell, kernels issues finding linear decision boundary \nprojected space.","code":"\nmodel <- x |> e1071::svm(class ~ ., data = _, \n                         kernel = \"linear\", probability = TRUE)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (linear kernel)\")\n## Warning: Computation failed in `stat_contour()`.\n## Caused by error in `zero_range()`:\n## ! `x` must be length 1 or 2\nmodel <- x |> e1071::svm(class ~ ., data = _, \n                         kernel = \"radial\", probability = TRUE)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (radial kernel)\")\nmodel <- x |> e1071::svm(class ~ ., data = _, \n                         kernel = \"polynomial\", probability = TRUE)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (polynomial kernel)\")\nmodel <- x |> e1071::svm(class ~ ., data = _, \n                         kernel = \"sigmoid\", probability = TRUE)\ndecisionplot(model, x, class_var = \"class\") + \n  labs(title = \"SVM (sigmoid kernel)\")"},{"path":"classification-alternative-techniques.html","id":"single-layer-feed-forward-neural-networks-1","chapter":"4 Classification: Alternative Techniques","heading":"4.12.2.8 Single Layer Feed-forward Neural Networks","text":"Use simple network one hidden layer. try \ndifferent number \nneurons hidden layer.plots show network 4 6 neurons performs well, \nlarger number neurons leads overfitting training data.","code":"\nmodel <- x |> nnet::nnet(class ~ ., data = _, size = 1, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"ANN (1 neuron)\")\nmodel <- x |> nnet::nnet(class ~ ., data = _, size = 2, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"ANN (2 neurons)\")\nmodel <- x |> nnet::nnet(class ~ ., data = _, size = 4, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"ANN (4 neurons)\")\nmodel <- x |> nnet::nnet(class ~ ., data = _, size = 6, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"ANN (6 neurons)\")\nmodel <- x |> nnet::nnet(class ~ ., data = _, size = 20, trace = FALSE)\ndecisionplot(model, x, class_var = \"class\", \n  predict_type = c(\"class\", \"raw\")) + labs(title = \"ANN (20 neurons)\")"},{"path":"classification-alternative-techniques.html","id":"more-information-on-classification-with-r","chapter":"4 Classification: Alternative Techniques","heading":"4.13 More Information on Classification with R","text":"Package caret: http://topepo.github.io/caret/index.htmlTidymodels (machine learning tidyverse):\nhttps://www.tidymodels.org/R taskview machine learning:\nhttp://cran.r-project.org/web/views/MachineLearning.html","code":""},{"path":"classification-alternative-techniques.html","id":"exercises-2","chapter":"4 Classification: Alternative Techniques","heading":"4.14 Exercises*","text":"use Palmer penguin data exercises.Create R markdown file code following .Apply least 3 different classification models data.Compare models simple baseline model. model\nperforms best? perform significantly better \nmodels?","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm\n##   <chr>   <chr>              <dbl>         <dbl>\n## 1 Adelie  Torgersen           39.1          18.7\n## 2 Adelie  Torgersen           39.5          17.4\n## 3 Adelie  Torgersen           40.3          18  \n## 4 Adelie  Torgersen           NA            NA  \n## 5 Adelie  Torgersen           36.7          19.3\n## 6 Adelie  Torgersen           39.3          20.6\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"association-analysis-basic-concepts.html","id":"association-analysis-basic-concepts","chapter":"5 Association Analysis: Basic Concepts","heading":"5 Association Analysis: Basic Concepts","text":"chapter introduces association rules mining using APRIORI\nalgorithm. addition, analyzing sets association rules\nusing visualization techniques demonstrated.corresponding chapter \ndata mining textbook available online:\nChapter 5: Association Analysis: Basic Concepts Algorithms.","code":""},{"path":"association-analysis-basic-concepts.html","id":"packages-used-in-this-chapter-3","chapter":"5 Association Analysis: Basic Concepts","heading":"Packages Used in this Chapter","text":"packages used chapter :arules (Hahsler et al. 2025)arulesViz (Hahsler 2025)mlbench (Leisch Dimitriadou 2024)palmerpenguins (Horst, Hill, Gorman 2022)tidyverse (Wickham 2023b)","code":"\npkgs <- c(\"arules\", \"arulesViz\", \"mlbench\", \n          \"palmerpenguins\", \"tidyverse\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"association-analysis-basic-concepts.html","id":"preliminaries","chapter":"5 Association Analysis: Basic Concepts","heading":"5.1 Preliminaries","text":"Association rule mining\nplays vital role discovering hidden patterns relationships within large\ntransactional datasets. Applications range exploratory data analysis marketing building rule-based\nclassifiers.\nAgrawal, Imielinski, Swami (1993) introduced problem\nmining association rules transaction data follows (definition taken Hahsler, Grün, Hornik (2005)):Let \\(= \\{i_1,i_2,...,i_n\\}\\) set \\(n\\) binary attributes called items. Let \\(D = \\{t_1,t_2,...,t_m\\}\\) \nset transactions called database. transaction \\(D\\) unique transaction ID \ncontains subset items \\(\\). rule defined implication form \\(X \\Rightarrow Y\\) \n\\(X,Y \\subseteq \\) \\(X \\cap Y = \\emptyset\\) called itemsets. itemsets rules several quality measures can\ndefined. important measures support confidence. support \\(supp(X)\\) \nitemset \\(X\\) defined proportion transactions data set contain itemset.\nItemsets support surpasses user-defined threshold \\(\\sigma\\) called frequent itemsets. \nconfidence rule defined \\(conf(X \\Rightarrow Y) = supp(X \\cup Y)/supp(X)\\). Association rules rules\n\\(supp(X \\cup Y) \\ge \\sigma\\) \\(conf(X) \\ge \\delta\\) \\(\\sigma\\) \\(\\delta\\) user-defined thresholds.\nfound set association rules used reason data.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 5. Association Analysis: Basic Concepts \nAlgorithms","code":""},{"path":"association-analysis-basic-concepts.html","id":"the-arules-package","chapter":"5 Association Analysis: Basic Concepts","heading":"5.1.1 The arules Package","text":"Association rule mining R implemented package arules.information arules package try: help(package=\"arules\")\nvignette(\"arules\") (also available \nCRAN)arules uses S4 object system implement classes methods.\nStandard R objects use S3 object\nsystem use formal class\ndefinitions usually implemented list class\nattribute. arules many R packages use S4 object\nsystem based formal class\ndefinitions member variables methods (similar \nobject-oriented programming languages like Java C++). important\ndifferences using S4 objects compared usual S3 objects :coercion (casting): (, \"class_name\")help classes: class? class_name","code":"\nlibrary(tidyverse)\nlibrary(arules)\nlibrary(arulesViz)"},{"path":"association-analysis-basic-concepts.html","id":"transactions","chapter":"5 Association Analysis: Basic Concepts","heading":"5.1.2 Transactions","text":"","code":""},{"path":"association-analysis-basic-concepts.html","id":"create-transactions","chapter":"5 Association Analysis: Basic Concepts","heading":"5.1.2.1 Create Transactions","text":"use Zoo dataset mlbench.data data.frame need converted set \ntransactions row represents transaction column \ntranslated items. done using constructor\ntransactions(). Zoo data set means consider\nanimals transactions different traits (features) become\nitems animal . example animal antelope \nitem hair transaction.conversion gives warning discrete features (factor\nlogical) can directly translated items. Continuous\nfeatures need discretized first.column 13?Possible solution: Make legs /legsAlternatives:Use unique value item:Use discretize function (see\n? discretize\ndiscretization code Chapter\n2):Convert data set transactions","code":"\ndata(Zoo, package = \"mlbench\")\nhead(Zoo)\n##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n## bear      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## boar      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## buffalo   TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n##          predator toothed backbone breathes venomous  fins\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE  TRUE\n## bear         TRUE    TRUE     TRUE     TRUE    FALSE FALSE\n## boar         TRUE    TRUE     TRUE     TRUE    FALSE FALSE\n## buffalo     FALSE    TRUE     TRUE     TRUE    FALSE FALSE\n##          legs  tail domestic catsize   type\n## aardvark    4 FALSE    FALSE    TRUE mammal\n## antelope    4  TRUE    FALSE    TRUE mammal\n## bass        0  TRUE    FALSE   FALSE   fish\n## bear        4 FALSE    FALSE    TRUE mammal\n## boar        4  TRUE    FALSE    TRUE mammal\n## buffalo     4  TRUE    FALSE    TRUE mammal\ntrans <- transactions(Zoo)\n## Warning: Column(s) 13 not logical or factor. Applying\n## default discretization (see '? discretizeDF').\nsummary(Zoo[13])\n##       legs     \n##  Min.   :0.00  \n##  1st Qu.:2.00  \n##  Median :4.00  \n##  Mean   :2.84  \n##  3rd Qu.:4.00  \n##  Max.   :8.00\nggplot(Zoo, aes(legs)) + geom_bar()\nZoo$legs |> table()\n## \n##  0  2  4  5  6  8 \n## 23 27 38  1 10  2\nZoo_has_legs <- Zoo |> mutate(legs = legs > 0)\nggplot(Zoo_has_legs, aes(legs)) + geom_bar()\nZoo_has_legs$legs |> table()\n## \n## FALSE  TRUE \n##    23    78\nZoo_unique_leg_values <- Zoo |> mutate(legs = factor(legs))\nZoo_unique_leg_values$legs |> head()\n## [1] 4 4 0 4 4 4\n## Levels: 0 2 4 5 6 8\nZoo_discretized_legs <- Zoo |> mutate(\n  legs = discretize(legs, breaks = 2, method=\"interval\")\n)\ntable(Zoo_discretized_legs$legs)\n## \n## [0,4) [4,8] \n##    50    51\ntrans <- transactions(Zoo_has_legs)\ntrans\n## transactions in sparse format with\n##  101 transactions (rows) and\n##  23 items (columns)"},{"path":"association-analysis-basic-concepts.html","id":"inspect-transactions","chapter":"5 Association Analysis: Basic Concepts","heading":"5.1.2.2 Inspect Transactions","text":"Look created items. still called column names since \ntransactions actually stored large sparse logical matrix (see\n).Compare original features (column names) ZooLook (first) transactions matrix. 1 indicates presence\nitem.Look transactions sets itemsPlot binary matrix. Dark dots represent 1s.Look relative frequency (=support) items data set. \nlook 10 frequent items.Alternative encoding: Also create items FALSE (use factor)","code":"\nsummary(trans)\n## transactions as itemMatrix in sparse format with\n##  101 rows (elements/itemsets/transactions) and\n##  23 columns (items) and a density of 0.3612 \n## \n## most frequent items:\n## backbone breathes     legs     tail  toothed  (Other) \n##       83       80       78       75       61      462 \n## \n## element (itemset/transaction) length distribution:\n## sizes\n##  3  4  5  6  7  8  9 10 11 12 \n##  3  2  6  5  8 21 27 25  3  1 \n## \n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    3.00    8.00    9.00    8.31   10.00   12.00 \n## \n## includes extended item information - examples:\n##     labels variables levels\n## 1     hair      hair   TRUE\n## 2 feathers  feathers   TRUE\n## 3     eggs      eggs   TRUE\n## \n## includes extended transaction information - examples:\n##   transactionID\n## 1      aardvark\n## 2      antelope\n## 3          bass\ncolnames(trans)\n##  [1] \"hair\"               \"feathers\"          \n##  [3] \"eggs\"               \"milk\"              \n##  [5] \"airborne\"           \"aquatic\"           \n##  [7] \"predator\"           \"toothed\"           \n##  [9] \"backbone\"           \"breathes\"          \n## [11] \"venomous\"           \"fins\"              \n## [13] \"legs\"               \"tail\"              \n## [15] \"domestic\"           \"catsize\"           \n## [17] \"type=mammal\"        \"type=bird\"         \n## [19] \"type=reptile\"       \"type=fish\"         \n## [21] \"type=amphibian\"     \"type=insect\"       \n## [23] \"type=mollusc.et.al\"\ncolnames(Zoo)\n##  [1] \"hair\"     \"feathers\" \"eggs\"     \"milk\"     \"airborne\"\n##  [6] \"aquatic\"  \"predator\" \"toothed\"  \"backbone\" \"breathes\"\n## [11] \"venomous\" \"fins\"     \"legs\"     \"tail\"     \"domestic\"\n## [16] \"catsize\"  \"type\"\nas(trans, \"matrix\")[1:3,]\n##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n##          predator toothed backbone breathes venomous  fins\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE  TRUE\n##           legs  tail domestic catsize type=mammal type=bird\n## aardvark  TRUE FALSE    FALSE    TRUE        TRUE     FALSE\n## antelope  TRUE  TRUE    FALSE    TRUE        TRUE     FALSE\n## bass     FALSE  TRUE    FALSE   FALSE       FALSE     FALSE\n##          type=reptile type=fish type=amphibian type=insect\n## aardvark        FALSE     FALSE          FALSE       FALSE\n## antelope        FALSE     FALSE          FALSE       FALSE\n## bass            FALSE      TRUE          FALSE       FALSE\n##          type=mollusc.et.al\n## aardvark              FALSE\n## antelope              FALSE\n## bass                  FALSE\ninspect(trans[1:3])\n##     items         transactionID\n## [1] {hair,                     \n##      milk,                     \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      catsize,                  \n##      type=mammal}      aardvark\n## [2] {hair,                     \n##      milk,                     \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      tail,                     \n##      catsize,                  \n##      type=mammal}      antelope\n## [3] {eggs,                     \n##      aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      type=fish}        bass\nimage(trans)\nitemFrequencyPlot(trans,topN = 20)\nggplot(\n  tibble(\n    Support = sort(itemFrequency(trans, type = \"absolute\"), \n                   decreasing = TRUE),\n    Item = seq_len(ncol(trans))\n  ), aes(x = Item, y = Support)) + \n  geom_line()\nsapply(Zoo_has_legs, class)\n##      hair  feathers      eggs      milk  airborne   aquatic \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##  predator   toothed  backbone  breathes  venomous      fins \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##      legs      tail  domestic   catsize      type \n## \"logical\" \"logical\" \"logical\" \"logical\"  \"factor\"\nZoo_factors <- Zoo_has_legs |> \n  mutate(across(where(is.logical), factor))\nsapply(Zoo_factors, class)\n##     hair feathers     eggs     milk airborne  aquatic \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \n## predator  toothed backbone breathes venomous     fins \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \n##     legs     tail domestic  catsize     type \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\"\nsummary(Zoo_factors)\n##     hair     feathers     eggs       milk     airborne \n##  FALSE:58   FALSE:81   FALSE:42   FALSE:60   FALSE:77  \n##  TRUE :43   TRUE :20   TRUE :59   TRUE :41   TRUE :24  \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##   aquatic    predator   toothed    backbone   breathes \n##  FALSE:65   FALSE:45   FALSE:40   FALSE:18   FALSE:21  \n##  TRUE :36   TRUE :56   TRUE :61   TRUE :83   TRUE :80  \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##   venomous     fins       legs       tail     domestic \n##  FALSE:93   FALSE:84   FALSE:23   FALSE:26   FALSE:88  \n##  TRUE : 8   TRUE :17   TRUE :78   TRUE :75   TRUE :13  \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##   catsize              type   \n##  FALSE:57   mammal       :41  \n##  TRUE :44   bird         :20  \n##             reptile      : 5  \n##             fish         :13  \n##             amphibian    : 4  \n##             insect       : 8  \n##             mollusc.et.al:10\ntrans_factors <- transactions(Zoo_factors)\ntrans_factors\n## transactions in sparse format with\n##  101 transactions (rows) and\n##  39 items (columns)\nitemFrequencyPlot(trans_factors, topN = 20)\n## Select transactions that contain a certain item\ntrans_insects <- trans_factors[trans %in% \"type=insect\"]\ntrans_insects\n## transactions in sparse format with\n##  8 transactions (rows) and\n##  39 items (columns)\ninspect(trans_insects)\n##     items             transactionID\n## [1] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=FALSE,               \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          flea    \n## [2] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          gnat    \n## [3] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=TRUE,                \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=TRUE,                \n##      catsize=FALSE,                \n##      type=insect}          honeybee\n## [4] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          housefly\n## [5] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=TRUE,                \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          ladybird\n## [6] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          moth    \n## [7] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=FALSE,               \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          termite \n## [8] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=TRUE,                \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          wasp"},{"path":"association-analysis-basic-concepts.html","id":"vertical-layout-transaction-id-lists","chapter":"5 Association Analysis: Basic Concepts","heading":"5.1.2.3 Vertical Layout (Transaction ID Lists)","text":"default layout transactions horizontal layout (.e. \ntransaction row). vertical layout represents transaction data\nlist transaction IDs item (= transaction ID lists).","code":"\nvertical <- as(trans, \"tidLists\")\nas(vertical, \"matrix\")[1:10, 1:5]\n##          aardvark antelope  bass  bear  boar\n## hair         TRUE     TRUE FALSE  TRUE  TRUE\n## feathers    FALSE    FALSE FALSE FALSE FALSE\n## eggs        FALSE    FALSE  TRUE FALSE FALSE\n## milk         TRUE     TRUE FALSE  TRUE  TRUE\n## airborne    FALSE    FALSE FALSE FALSE FALSE\n## aquatic     FALSE    FALSE  TRUE FALSE FALSE\n## predator     TRUE    FALSE  TRUE  TRUE  TRUE\n## toothed      TRUE     TRUE  TRUE  TRUE  TRUE\n## backbone     TRUE     TRUE  TRUE  TRUE  TRUE\n## breathes     TRUE     TRUE FALSE  TRUE  TRUE"},{"path":"association-analysis-basic-concepts.html","id":"frequent-itemset-generation","chapter":"5 Association Analysis: Basic Concepts","heading":"5.2 Frequent Itemset Generation","text":"dataset already huge number possible itemsetsFind frequent itemsets (target=“frequent”) default settings.Default minimum support .1 (10%). Note: use small\ndata set. larger datasets default minimum support might \nlow may run memory. probably want start \nhigher minimum support like .5 (50%) work way .order find itemsets effect 5 animals need go \nsupport 5%.Sort supportLook frequent itemsets many items (set breaks manually since\nAutomatically chosen breaks look bad)","code":"\n2^ncol(trans)\n## [1] 8388608\nits <- apriori(trans, parameter=list(target = \"frequent\"))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##          NA    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen            target  ext\n##      0.1      1     10 frequent itemsets TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [18 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, parameter = list(target =\n## \"frequent\")): Mining stopped (maxlen reached). Only\n## patterns up to a length of 10 returned!\n##  done [0.00s].\n## sorting transactions ... done [0.00s].\n## writing ... [1465 set(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nits\n## set of 1465 itemsets\n5/nrow(trans)\n## [1] 0.0495\nits <- apriori(trans, parameter=list(target = \"frequent\", \n                                     support = 0.05))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##          NA    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen            target  ext\n##     0.05      1     10 frequent itemsets TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, parameter = list(target =\n## \"frequent\", support = 0.05)): Mining stopped (maxlen\n## reached). Only patterns up to a length of 10 returned!\n##  done [0.00s].\n## sorting transactions ... done [0.00s].\n## writing ... [2537 set(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nits\n## set of 2537 itemsets\nits <- sort(its, by = \"support\")\nits |> head(n = 10) |> inspect()\n##      items                      support count\n## [1]  {backbone}                 0.8218  83   \n## [2]  {breathes}                 0.7921  80   \n## [3]  {legs}                     0.7723  78   \n## [4]  {tail}                     0.7426  75   \n## [5]  {backbone, tail}           0.7327  74   \n## [6]  {breathes, legs}           0.7228  73   \n## [7]  {backbone, breathes}       0.6832  69   \n## [8]  {backbone, legs}           0.6337  64   \n## [9]  {backbone, breathes, legs} 0.6337  64   \n## [10] {toothed}                  0.6040  61\nggplot(tibble(`Itemset Size` = factor(size(its))), \n       aes(`Itemset Size`)) + \n  geom_bar()\nits[size(its) > 8] |> inspect()\n##      items         support count\n## [1]  {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.23762    24\n## [2]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       catsize,                  \n##       type=mammal} 0.15842    16\n## [3]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       type=mammal} 0.14851    15\n## [4]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.13861    14\n## [5]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [6]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [7]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [8]  {milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [9]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize}     0.12871    13\n## [10] {hair,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [11] {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal} 0.12871    13\n## [12] {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       domestic,                 \n##       catsize,                  \n##       type=mammal} 0.05941     6\n## [13] {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       domestic,                 \n##       type=mammal} 0.05941     6\n## [14] {feathers,                 \n##       eggs,                     \n##       airborne,                 \n##       predator,                 \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       type=bird}   0.05941     6"},{"path":"association-analysis-basic-concepts.html","id":"rule-generation","chapter":"5 Association Analysis: Basic Concepts","heading":"5.3 Rule Generation","text":"use APRIORI algorithm (see\n? apriori)Look rules highest liftCreate rules using alternative encoding (“FALSE” item)","code":"\nrules <- apriori(trans, \n                 parameter = list(support = 0.05, \n                                         confidence = 0.9))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##         0.9    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen target  ext\n##     0.05      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, parameter = list(support = 0.05,\n## confidence = 0.9)): Mining stopped (maxlen reached). Only\n## patterns up to a length of 10 returned!\n##  done [0.00s].\n## writing ... [7174 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nlength(rules)\n## [1] 7174\nrules |> head() |> inspect()\n##     lhs                     rhs        support confidence\n## [1] {type=insect}        => {eggs}     0.07921 1.0       \n## [2] {type=insect}        => {legs}     0.07921 1.0       \n## [3] {type=insect}        => {breathes} 0.07921 1.0       \n## [4] {type=mollusc.et.al} => {eggs}     0.08911 0.9       \n## [5] {type=fish}          => {fins}     0.12871 1.0       \n## [6] {type=fish}          => {aquatic}  0.12871 1.0       \n##     coverage lift  count\n## [1] 0.07921  1.712  8   \n## [2] 0.07921  1.295  8   \n## [3] 0.07921  1.262  8   \n## [4] 0.09901  1.541  9   \n## [5] 0.12871  5.941 13   \n## [6] 0.12871  2.806 13\nrules |> head() |> quality()\n##   support confidence coverage  lift count\n## 1 0.07921        1.0  0.07921 1.712     8\n## 2 0.07921        1.0  0.07921 1.295     8\n## 3 0.07921        1.0  0.07921 1.262     8\n## 4 0.08911        0.9  0.09901 1.541     9\n## 5 0.12871        1.0  0.12871 5.941    13\n## 6 0.12871        1.0  0.12871 2.806    13\nrules <- sort(rules, by = \"lift\")\nrules |> head(n = 10) |> inspect()\n##      lhs            rhs         support confidence coverage  lift count\n## [1]  {eggs,                                                            \n##       fins}      => {type=fish} 0.12871          1  0.12871 7.769    13\n## [2]  {eggs,                                                            \n##       aquatic,                                                         \n##       fins}      => {type=fish} 0.12871          1  0.12871 7.769    13\n## [3]  {eggs,                                                            \n##       predator,                                                        \n##       fins}      => {type=fish} 0.08911          1  0.08911 7.769     9\n## [4]  {eggs,                                                            \n##       toothed,                                                         \n##       fins}      => {type=fish} 0.12871          1  0.12871 7.769    13\n## [5]  {eggs,                                                            \n##       fins,                                                            \n##       tail}      => {type=fish} 0.12871          1  0.12871 7.769    13\n## [6]  {eggs,                                                            \n##       backbone,                                                        \n##       fins}      => {type=fish} 0.12871          1  0.12871 7.769    13\n## [7]  {eggs,                                                            \n##       aquatic,                                                         \n##       predator,                                                        \n##       fins}      => {type=fish} 0.08911          1  0.08911 7.769     9\n## [8]  {eggs,                                                            \n##       aquatic,                                                         \n##       toothed,                                                         \n##       fins}      => {type=fish} 0.12871          1  0.12871 7.769    13\n## [9]  {eggs,                                                            \n##       aquatic,                                                         \n##       fins,                                                            \n##       tail}      => {type=fish} 0.12871          1  0.12871 7.769    13\n## [10] {eggs,                                                            \n##       aquatic,                                                         \n##       backbone,                                                        \n##       fins}      => {type=fish} 0.12871          1  0.12871 7.769    13\nr <- apriori(trans_factors)\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##         0.8    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen target  ext\n##      0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[39 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [34 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans_factors): Mining stopped (maxlen\n## reached). Only patterns up to a length of 10 returned!\n##  done [0.04s].\n## writing ... [1517191 rule(s)] done [0.21s].\n## creating S4 object  ... done [1.11s].\nr\n## set of 1517191 rules\nprint(object.size(r), unit = \"Mb\")\n## 110.2 Mb\ninspect(r[1:10])\n##      lhs                rhs              support confidence\n## [1]  {}              => {feathers=FALSE} 0.8020  0.8020    \n## [2]  {}              => {backbone=TRUE}  0.8218  0.8218    \n## [3]  {}              => {fins=FALSE}     0.8317  0.8317    \n## [4]  {}              => {domestic=FALSE} 0.8713  0.8713    \n## [5]  {}              => {venomous=FALSE} 0.9208  0.9208    \n## [6]  {domestic=TRUE} => {predator=FALSE} 0.1089  0.8462    \n## [7]  {domestic=TRUE} => {aquatic=FALSE}  0.1188  0.9231    \n## [8]  {domestic=TRUE} => {legs=TRUE}      0.1188  0.9231    \n## [9]  {domestic=TRUE} => {breathes=TRUE}  0.1188  0.9231    \n## [10] {domestic=TRUE} => {backbone=TRUE}  0.1188  0.9231    \n##      coverage lift  count\n## [1]  1.0000   1.000 81   \n## [2]  1.0000   1.000 83   \n## [3]  1.0000   1.000 84   \n## [4]  1.0000   1.000 88   \n## [5]  1.0000   1.000 93   \n## [6]  0.1287   1.899 11   \n## [7]  0.1287   1.434 12   \n## [8]  0.1287   1.195 12   \n## [9]  0.1287   1.165 12   \n## [10] 0.1287   1.123 12\nr |> head(n = 10, by = \"lift\") |> inspect()\n##      lhs                  rhs         support confidence coverage  lift count\n## [1]  {breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [2]  {eggs=TRUE,                                                             \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [3]  {milk=FALSE,                                                            \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [4]  {breathes=FALSE,                                                        \n##       fins=TRUE,                                                             \n##       legs=FALSE}      => {type=fish}  0.1287          1   0.1287 7.769    13\n## [5]  {aquatic=TRUE,                                                          \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [6]  {hair=FALSE,                                                            \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [7]  {eggs=TRUE,                                                             \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [8]  {milk=FALSE,                                                            \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [9]  {toothed=TRUE,                                                          \n##       breathes=FALSE,                                                        \n##       fins=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13\n## [10] {breathes=FALSE,                                                        \n##       fins=TRUE,                                                             \n##       tail=TRUE}       => {type=fish}  0.1287          1   0.1287 7.769    13"},{"path":"association-analysis-basic-concepts.html","id":"calculate-additional-interest-measures","chapter":"5 Association Analysis: Basic Concepts","heading":"5.3.1 Calculate Additional Interest Measures","text":"Add measures rulesFind rules score high Phi correlation","code":"\ninterestMeasure(rules[1:10], measure = c(\"phi\", \"gini\"),\n  trans = trans)\n##       phi   gini\n## 1  1.0000 0.2243\n## 2  1.0000 0.2243\n## 3  0.8138 0.1485\n## 4  1.0000 0.2243\n## 5  1.0000 0.2243\n## 6  1.0000 0.2243\n## 7  0.8138 0.1485\n## 8  1.0000 0.2243\n## 9  1.0000 0.2243\n## 10 1.0000 0.2243\nquality(rules) <- cbind(quality(rules),\n  interestMeasure(rules, measure = c(\"phi\", \"gini\"),\n    trans = trans))\nrules |> head(by = \"phi\") |> inspect()\n##     lhs            rhs         support confidence coverage  lift count phi   gini\n## [1] {eggs,                                                                       \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13   1 0.2243\n## [2] {eggs,                                                                       \n##      aquatic,                                                                    \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13   1 0.2243\n## [3] {eggs,                                                                       \n##      toothed,                                                                    \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13   1 0.2243\n## [4] {eggs,                                                                       \n##      fins,                                                                       \n##      tail}      => {type=fish}  0.1287          1   0.1287 7.769    13   1 0.2243\n## [5] {eggs,                                                                       \n##      backbone,                                                                   \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13   1 0.2243\n## [6] {eggs,                                                                       \n##      aquatic,                                                                    \n##      toothed,                                                                    \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13   1 0.2243"},{"path":"association-analysis-basic-concepts.html","id":"mine-using-templates","chapter":"5 Association Analysis: Basic Concepts","heading":"5.3.2 Mine Using Templates","text":"Sometimes beneficial specify items \nrule. apriori can use parameter appearance specify \n(see\n? APappearance).\nfollowing restrict rules animal type RHS \nitem LHS.Saving rules CSV-file opened Excel tools.write(rules, file = \"rules.csv\", quote = TRUE)","code":"\ntype <- grep(\"type=\", itemLabels(trans), value = TRUE)\ntype\n## [1] \"type=mammal\"        \"type=bird\"         \n## [3] \"type=reptile\"       \"type=fish\"         \n## [5] \"type=amphibian\"     \"type=insect\"       \n## [7] \"type=mollusc.et.al\"\nrules_type <- apriori(trans, appearance= list(rhs = type))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##         0.8    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen target  ext\n##      0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[7 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [18 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, appearance = list(rhs = type)):\n## Mining stopped (maxlen reached). Only patterns up to a\n## length of 10 returned!\n##  done [0.00s].\n## writing ... [571 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nrules_type |> sort(by = \"lift\") |> head() |> inspect()\n##     lhs            rhs         support confidence coverage  lift count\n## [1] {eggs,                                                            \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13\n## [2] {eggs,                                                            \n##      aquatic,                                                         \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13\n## [3] {eggs,                                                            \n##      toothed,                                                         \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13\n## [4] {eggs,                                                            \n##      fins,                                                            \n##      tail}      => {type=fish}  0.1287          1   0.1287 7.769    13\n## [5] {eggs,                                                            \n##      backbone,                                                        \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13\n## [6] {eggs,                                                            \n##      aquatic,                                                         \n##      toothed,                                                         \n##      fins}      => {type=fish}  0.1287          1   0.1287 7.769    13"},{"path":"association-analysis-basic-concepts.html","id":"compact-representation-of-frequent-itemsets","chapter":"5 Association Analysis: Basic Concepts","heading":"5.4 Compact Representation of Frequent Itemsets","text":"Find maximal frequent itemsets (superset frequent)Find closed frequent itemsets (superset frequent)","code":"\nits_max <- its[is.maximal(its)]\nits_max\n## set of 22 itemsets\nits_max |> head(by = \"support\") |> inspect()\n##     items         support count\n## [1] {hair,                     \n##      milk,                     \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      tail,                     \n##      catsize,                  \n##      type=mammal} 0.12871    13\n## [2] {eggs,                     \n##      aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      type=fish}   0.08911     9\n## [3] {aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes}    0.07921     8\n## [4] {aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      catsize}     0.06931     7\n## [5] {eggs,                     \n##      venomous}    0.05941     6\n## [6] {predator,                 \n##      venomous}    0.05941     6\nits_closed <- its[is.closed(its)]\nits_closed\n## set of 230 itemsets\nits_closed |> head(by = \"support\") |> inspect()\n##     items            support count\n## [1] {backbone}       0.8218  83   \n## [2] {breathes}       0.7921  80   \n## [3] {legs}           0.7723  78   \n## [4] {tail}           0.7426  75   \n## [5] {backbone, tail} 0.7327  74   \n## [6] {breathes, legs} 0.7228  73\ncounts <- c(\n  frequent=length(its),\n  closed=length(its_closed),\n  maximal=length(its_max)\n)\n\nggplot(as_tibble(counts, rownames = \"Itemsets\"),\n  aes(Itemsets, counts)) + geom_bar(stat = \"identity\")"},{"path":"association-analysis-basic-concepts.html","id":"association-rule-visualization","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5 Association Rule Visualization*","text":"Visualization powerful approach analyse large sets \nmined association rules frequent itemsets. present options\ncreate static visualizations inspect rule sets interactively.","code":""},{"path":"association-analysis-basic-concepts.html","id":"static-visualizations","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.1 Static Visualizations","text":"Load arulesViz library.Create rules Zoo dataset converting legs variable binary\nindicator.","code":"\nlibrary(arulesViz)\nlibrary(tidyverse)\ndata(Zoo, package = \"mlbench\")\nZoo_has_legs <- Zoo |> mutate(legs = legs > 0)\ntrans <- transactions(Zoo_has_legs)\ntrans\n## transactions in sparse format with\n##  101 transactions (rows) and\n##  23 items (columns)\nrules <- apriori(trans, \n                 parameter = list(support = 0.05, \n                                  confidence = 0.9))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##         0.9    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen target  ext\n##     0.05      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10\n## Warning in apriori(trans, parameter = list(support = 0.05,\n## confidence = 0.9)): Mining stopped (maxlen reached). Only\n## patterns up to a length of 10 returned!\n##  done [0.00s].\n## writing ... [7174 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nquality(rules) <- cbind(quality(rules),\n  interestMeasure(rules, measure = c(\"phi\", \"gini\"),\n    trans = trans))\n\nrules\n## set of 7174 rules"},{"path":"association-analysis-basic-concepts.html","id":"scatterplot","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.1.1 Scatterplot","text":"default plot association rules support/confidence scatterplot.rules represented point. Note jitter (randomly move points)\nadded show many\nrules confidence support value. Without jitter:","code":"\nplot(rules)\n## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\nplot(rules, control = list(jitter = 0))\nplot(rules, shading = \"order\")\n## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter."},{"path":"association-analysis-basic-concepts.html","id":"grouped-matrix-plot","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.1.2 Grouped Matrix Plot","text":"grouped matrix plot tries group rules similar relationship rule’s LHS RHS \nrepresents groups support lift using balloon plot.\nGrouping performed using clustering. Groups organized \ninteresting rules appear top left corner.interesting rules thre relationship fins, eggs 5 \nitems type fish.plot can also used interactively using parameter\nengine = \"interactive\".","code":"\nset.seed(1234)\nplot(rules, method = \"grouped matrix\")\n## Registered S3 methods overwritten by 'registry':\n##   method               from \n##   print.registry_field proxy\n##   print.registry_entry proxy"},{"path":"association-analysis-basic-concepts.html","id":"gaph-based-visualization","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.1.3 Gaph-based Visualization","text":"Graph-based techniques visualize association rules using vertices edges,\nvertices typically represent items itemsets edges indicate relationships\nterms rules. Interest measures typically added plot labels \nedges color width arrows displaying edges.","code":"\nplot(rules, method = \"graph\")\n## Warning: Too many rules supplied. Only plotting the best\n## 100 using 'lift' (change control parameter max if needed).\nplot(rules |> head(by = \"phi\", n = 100), method = \"graph\")"},{"path":"association-analysis-basic-concepts.html","id":"interactive-visualizations","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.2 Interactive Visualizations","text":"Interactive visualizations let user explore large number \nrules itemsets learn structure data. overview article\narulesViz: Interactive Visualization Association Rules R\nprovides indepth discussion interactive association rule visualization.use association rules mined Iris dataset \nfollowing examples.Convert data transactions.Note conversion gives warning indicate potentially\nunwanted conversion happens. features numeric \nneed discretized. conversion automatically applies\nfrequency-based discretization 3 classes numeric feature,\nhowever, use may want use different discretization strategy.Next, mine association rules.","code":"\ndata(iris)\nsummary(iris)\n##   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  \n##  Median :5.80   Median :3.00   Median :4.35   Median :1.3  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \niris_trans <- transactions(iris)\n## Warning: Column(s) 1, 2, 3, 4 not logical or factor.\n## Applying default discretization (see '? discretizeDF').\niris_trans |> head() |> inspect()\n##     items                      transactionID\n## [1] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       1\n## [2] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[2.9,3.2),                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       2\n## [3] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       3\n## [4] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[2.9,3.2),                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       4\n## [5] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       5\n## [6] {Sepal.Length=[5.4,6.3),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       6\nrules <- apriori(iris_trans, parameter = list(support = 0.1, \n                                              confidence = 0.8))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##         0.8    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen target  ext\n##      0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 15 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[15 item(s), 150 transaction(s)] done [0.00s].\n## sorting and recoding items ... [15 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 done [0.00s].\n## writing ... [144 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nrules\n## set of 144 rules"},{"path":"association-analysis-basic-concepts.html","id":"interactive-inspect-with-sorting-filtering-and-paging","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.2.1 Interactive Inspect With Sorting, Filtering and Paging","text":"interactive table lets user sort filter rules\neffective exploration tool. can quickly find highest\nlift rules filter interesting items LHS RHS rules.","code":"\ninspectDT(rules,options = list(scrollX = TRUE))"},{"path":"association-analysis-basic-concepts.html","id":"scatter-plot","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.2.2 Scatter Plot","text":"Plot rules scatter plot using interactive html widget. avoid\noverplotting, jitter added automatically. Set jitter = 0 disable\njitter. Hovering rules shows rule information. Note:\nplotly/javascript well many points, plot selects\ntop 1000 rules warning rules supplied.","code":"\nplot(rules, engine = \"html\")\n## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter."},{"path":"association-analysis-basic-concepts.html","id":"matrix-visualization","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.2.3 Matrix Visualization","text":"rules organized matrix columns represent unique LHS itemsets \nrows RHS items. Hovering rules shows rule information.","code":"\nplot(rules, method = \"matrix\", engine = \"html\") "},{"path":"association-analysis-basic-concepts.html","id":"visualization-as-graph","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.2.4 Visualization as Graph","text":"Plot rules interactive graph items rectangular vertices rules circular vertices.\nHovering vertex shows additional information. devices, mouse wheel\nlets user zoom graph.\nNote: used\njavascript library well many graph nodes, plot\nselects top 100 rules (warning).","code":"\nplot(rules, method = \"graph\", engine = \"html\")\n## Warning: Too many rules supplied. Only plotting the best\n## 100 using 'lift' (change control parameter max if needed)."},{"path":"association-analysis-basic-concepts.html","id":"interactive-rule-explorer","chapter":"5 Association Analysis: Basic Concepts","heading":"5.5.2.5 Interactive Rule Explorer","text":"can specify rule set dataset. explore rules can \nmined iris, use: ruleExplorer(iris)rule explorer creates interactive Shiny application can \nused locally deployed server sharing. deployed version \nruleExplorer available\n(using\nshinyapps.io).","code":""},{"path":"association-analysis-basic-concepts.html","id":"exercises-3","chapter":"5 Association Analysis: Basic Concepts","heading":"5.6 Exercises*","text":"use Palmer penguin data exercises.Translate penguin data transaction data :conversion report warnings?following first three transactions mean?Next, use ruleExplorer() function analyze association rules\ncreated transaction data set.Use default settings parameters.\nUsing Data Table, association rule highest lift. LHS, RHS,\nsupport, confidence lift mean?Use default settings parameters.\nUsing Data Table, association rule highest lift. LHS, RHS,\nsupport, confidence lift mean?Use Graph visualization. Use select id highlight different species \ndifferent islands hover rules. see?Use Graph visualization. Use select id highlight different species \ndifferent islands hover rules. see?","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm\n##   <chr>   <chr>              <dbl>         <dbl>\n## 1 Adelie  Torgersen           39.1          18.7\n## 2 Adelie  Torgersen           39.5          17.4\n## 3 Adelie  Torgersen           40.3          18  \n## 4 Adelie  Torgersen           NA            NA  \n## 5 Adelie  Torgersen           36.7          19.3\n## 6 Adelie  Torgersen           39.3          20.6\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <dbl>, sex <chr>, year <dbl>\ntrans <- transactions(penguins)\n## Warning: Column(s) 1, 2, 3, 4, 5, 6, 7, 8 not logical or\n## factor. Applying default discretization (see '?\n## discretizeDF').\n## Warning in discretize(x = c(2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, : The calculated breaks are: 2007, 2008, 2009, 2009\n##   Only unique breaks are used reducing the number of intervals. Look at ? discretize for details.\ntrans\n## transactions in sparse format with\n##  344 transactions (rows) and\n##  22 items (columns)\ninspect(trans[1:3])\n##     items                             transactionID\n## [1] {species=Adelie,                               \n##      island=Torgersen,                             \n##      bill_length_mm=[32.1,40.8),                   \n##      bill_depth_mm=[18.3,21.5],                    \n##      flipper_length_mm=[172,192),                  \n##      body_mass_g=[3.7e+03,4.55e+03),               \n##      sex=male,                                     \n##      year=[2007,2008)}                            1\n## [2] {species=Adelie,                               \n##      island=Torgersen,                             \n##      bill_length_mm=[32.1,40.8),                   \n##      bill_depth_mm=[16.2,18.3),                    \n##      flipper_length_mm=[172,192),                  \n##      body_mass_g=[3.7e+03,4.55e+03),               \n##      sex=female,                                   \n##      year=[2007,2008)}                            2\n## [3] {species=Adelie,                               \n##      island=Torgersen,                             \n##      bill_length_mm=[32.1,40.8),                   \n##      bill_depth_mm=[16.2,18.3),                    \n##      flipper_length_mm=[192,209),                  \n##      body_mass_g=[2.7e+03,3.7e+03),                \n##      sex=female,                                   \n##      year=[2007,2008)}                            3"},{"path":"association-analysis-advanced-concepts.html","id":"association-analysis-advanced-concepts","chapter":"6 Association Analysis: Advanced Concepts","heading":"6 Association Analysis: Advanced Concepts","text":"chapter discusses advanced concepts association analysis.\nFirst, look categorical continuous attributes converted\nitems. look integrating item hierarchies analysis.\nFinally, sequence pattern mining introduced.","code":""},{"path":"association-analysis-advanced-concepts.html","id":"packages-used-in-this-chapter-4","chapter":"6 Association Analysis: Advanced Concepts","heading":"Packages Used in this Chapter","text":"packages used chapter :arules (Hahsler et al. 2025)arulesSequences (Buchta Hahsler 2024)tidyverse (Wickham 2023b)","code":"\npkgs <- c(\"arules\", \"arulesSequences\", \"tidyverse\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"association-analysis-advanced-concepts.html","id":"handling-categorical-attributes","chapter":"6 Association Analysis: Advanced Concepts","heading":"6.1 Handling Categorical Attributes","text":"Categorical attributes nominal ordinal variables.\nR factors ordinal. \ntranslated series binary items (one level constructed ⁠variable\nname = level⁠). Items represent order ordered factors lose\norder information. Note nominal variables need encoded \nfactors (characters numbers) converting transactions.special case Boolean variables (logical), TRUE value \nconverted item name variable FALSE\nvalues item created.give example next section.","code":""},{"path":"association-analysis-advanced-concepts.html","id":"handling-continuous-attributes","chapter":"6 Association Analysis: Advanced Concepts","heading":"6.2 Handling Continuous Attributes","text":"Continuous variables directly represented items need \ndiscretized first (see Discretization Chapter 2).\nitem resulting discretization might age>18 \ncolumn contains TRUE FALSE. Alternatively, can factor\nlevels age<=18, ⁠50=>age>18⁠ age>50. automatically\nconverted 3 items, one level. Discretization described functions\ndiscretize() discretizeDF() discretize columns data.frame.give short example using iris dataset. add extra logical column\nshow Boolean attributes converted items.first step \ndiscretize continuous attributes (marked <dbl> table ).\ndiscretize two Petal features.Next, convert dataset transactions.conversion creates warning still two undiscretized columns\ndata. warning indicates default discretization used\nautomatically.see continuous variables discretized different ranges\ncreate item. example Petal.Width two items Petal.Width=narrow\nPetal.Width=wide. automatically discretized variables show intervals.\nSepal.Length=[4.3,5.4) means item used flowers \nsepal length 4.3 5.4 cm.species converted three items, one class. logical\nvariable Versicolor created single item used \nvariable TRUE.","code":"\ndata(iris)\n\n## add a Boolean attribute\niris$Versicolor <- iris$Species == \"versicolor\"\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n##   Versicolor\n## 1      FALSE\n## 2      FALSE\n## 3      FALSE\n## 4      FALSE\n## 5      FALSE\n## 6      FALSE\nlibrary(tidyverse)\nlibrary(arules)\n\niris_disc <- iris %>% \n  mutate(Petal.Length = discretize(Petal.Length, \n                          method = \"frequency\", \n                          breaks = 3, \n                          labels = c(\"short\", \"medium\", \"long\")),\n         Petal.Width = discretize(Petal.Width,\n                          method = \"frequency\", \n                          breaks = 2, \n                          labels = c(\"narrow\", \"wide\"))\n         )\n  \n\nhead(iris_disc)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5        short      narrow  setosa\n## 2          4.9         3.0        short      narrow  setosa\n## 3          4.7         3.2        short      narrow  setosa\n## 4          4.6         3.1        short      narrow  setosa\n## 5          5.0         3.6        short      narrow  setosa\n## 6          5.4         3.9        short      narrow  setosa\n##   Versicolor\n## 1      FALSE\n## 2      FALSE\n## 3      FALSE\n## 4      FALSE\n## 5      FALSE\n## 6      FALSE\ntrans <- transactions(iris_disc)\n## Warning: Column(s) 1, 2 not logical or factor. Applying\n## default discretization (see '? discretizeDF').\ntrans\n## transactions in sparse format with\n##  150 transactions (rows) and\n##  15 items (columns)\nitemLabels(trans)\n##  [1] \"Sepal.Length=[4.3,5.4)\" \"Sepal.Length=[5.4,6.3)\"\n##  [3] \"Sepal.Length=[6.3,7.9]\" \"Sepal.Width=[2,2.9)\"   \n##  [5] \"Sepal.Width=[2.9,3.2)\"  \"Sepal.Width=[3.2,4.4]\" \n##  [7] \"Petal.Length=short\"     \"Petal.Length=medium\"   \n##  [9] \"Petal.Length=long\"      \"Petal.Width=narrow\"    \n## [11] \"Petal.Width=wide\"       \"Species=setosa\"        \n## [13] \"Species=versicolor\"     \"Species=virginica\"     \n## [15] \"Versicolor\""},{"path":"association-analysis-advanced-concepts.html","id":"handling-concept-hierarchies","chapter":"6 Association Analysis: Advanced Concepts","heading":"6.3 Handling Concept Hierarchies","text":"Often item hierarchy available transactions used association rule mining. example supermarket dataset items like “bread” “beagle” might belong item group (category) “baked goods.”\nTransactions can store item hierarchies additional columns itemInfo data.frame.","code":""},{"path":"association-analysis-advanced-concepts.html","id":"aggregation-1","chapter":"6 Association Analysis: Advanced Concepts","heading":"6.3.1 Aggregation","text":"perform analysis group level item hierarchy, aggregate() produces new object items aggregated given group level. group-level item present one items group present original object. rules aggregated, aggregation lead aggregated group item lhs rhs, group item removed lhs. Rules itemsets, unique aggregation, also removed. Note also quality measures applicable new rules thus removed. measures required, aggregate transactions mining rules.use Groceries data set example. contains 1 month (30 days) real-world point--sale transaction data typical local grocery outlet. items\n169 products categories.dataset also contains two aggregation levels.aggregate level1 stored Groceries. items level2 label\nbecome single item name. reduces number items \n55 level2 categoriesWe can now compare original transaction aggregated transaction.example, citrus fruit first transaction translated category fruit.\nNote order items transaction important, might change\naggregation.now easy mine rules aggregated data.can add aggregation existing dataset constructing\niteminfo data.frame adding transactions. See ? hierarchy\ndetails.","code":"\ndata(\"Groceries\")\nGroceries\n## transactions in sparse format with\n##  9835 transactions (rows) and\n##  169 items (columns)\nhead(itemInfo(Groceries))\n##              labels  level2           level1\n## 1       frankfurter sausage meat and sausage\n## 2           sausage sausage meat and sausage\n## 3        liver loaf sausage meat and sausage\n## 4               ham sausage meat and sausage\n## 5              meat sausage meat and sausage\n## 6 finished products sausage meat and sausage\nGroceries_level2 <- aggregate(Groceries, by = \"level2\")\nGroceries_level2\n## transactions in sparse format with\n##  9835 transactions (rows) and\n##  55 items (columns)\nhead(itemInfo(Groceries_level2)) ## labels are alphabetically sorted!\n##             labels           level2           level1\n## 1        baby food        baby food      canned food\n## 2             bags             bags         non-food\n## 3  bakery improver  bakery improver   processed food\n## 4 bathroom cleaner bathroom cleaner        detergent\n## 5             beef             beef meat and sausage\n## 6             beer             beer           drinks\ninspect(head(Groceries, 3))\n##     items                 \n## [1] {citrus fruit,        \n##      semi-finished bread, \n##      margarine,           \n##      ready soups}         \n## [2] {tropical fruit,      \n##      yogurt,              \n##      coffee}              \n## [3] {whole milk}\ninspect(head(Groceries_level2, 3))\n##     items                    \n## [1] {bread and backed goods, \n##      fruit,                  \n##      soups/sauces,           \n##      vinegar/oils}           \n## [2] {coffee,                 \n##      dairy produce,          \n##      fruit}                  \n## [3] {dairy produce}\nrules <- apriori(Groceries_level2, support = 0.005)\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##         0.8    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen target  ext\n##    0.005      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 49 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[55 item(s), 9835 transaction(s)] done [0.00s].\n## sorting and recoding items ... [47 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 done [0.00s].\n## writing ... [243 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nrules |> head(3, by = \"support\") |> inspect()\n##     lhs                          rhs             support confidence coverage  lift count\n## [1] {bread and backed goods,                                                            \n##      cheese,                                                                            \n##      fruit}                   => {dairy produce} 0.02481     0.8385  0.02959 1.893   244\n## [2] {bread and backed goods,                                                            \n##      cheese,                                                                            \n##      vegetables}              => {dairy produce} 0.02379     0.8239  0.02888 1.860   234\n## [3] {cheese,                                                                            \n##      fruit,                                                                             \n##      vegetables}              => {dairy produce} 0.02267     0.8479  0.02674 1.914   223"},{"path":"association-analysis-advanced-concepts.html","id":"multi-level-analysis","chapter":"6 Association Analysis: Advanced Concepts","heading":"6.3.2 Multi-level Analysis","text":"analyze relationships individual items item groups time, addAggregate() can used create new transactions object contains , original items group-level items.added group-level items marked * name. Now can mine rules\nincluding items multiple levels.Mining rules group-level items added \ncreate many spurious rules typewith confidence 1.\nalso happen mine itemsets. filterAggregate()\ncan used filter spurious rules itemsets.Using multi-level mining can reduce number rules help analyze \ncustomers differentiate products group.","code":"\nGroceries_multilevel <- addAggregate(Groceries, \"level2\")\nGroceries_multilevel |> head(n=3) |> inspect()\n##     items                     \n## [1] {citrus fruit,            \n##      semi-finished bread,     \n##      margarine,               \n##      ready soups,             \n##      bread and backed goods*, \n##      fruit*,                  \n##      soups/sauces*,           \n##      vinegar/oils*}           \n## [2] {tropical fruit,          \n##      yogurt,                  \n##      coffee,                  \n##      coffee*,                 \n##      dairy produce*,          \n##      fruit*}                  \n## [3] {whole milk,              \n##      dairy produce*}\nrules <- apriori(Groceries_multilevel,\n  parameter = list(support = 0.005))\n## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport maxtime\n##         0.8    0.1    1 none FALSE            TRUE       5\n##  support minlen maxlen target  ext\n##    0.005      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 49 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[224 item(s), 9835 transaction(s)] done [0.01s].\n## sorting and recoding items ... [167 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 done [0.03s].\n## writing ... [21200 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nrules\n## set of 21200 rules⁠item A => group of item A⁠\nrules <- filterAggregate(rules)\nrules\n## set of 838 rules\nrules |> head(n = 3, by = \"lift\") |> inspect()\n##     lhs                           rhs                        support confidence coverage  lift count\n## [1] {whole milk,                                                                                    \n##      whipped/sour cream,                                                                            \n##      bread and backed goods*,                                                                       \n##      cheese*}                  => {vegetables*}             0.005186     0.8095 0.006406 2.965    51\n## [2] {sausage,                                                                                       \n##      poultry*}                 => {vegetables*}             0.005084     0.8065 0.006304 2.954    50\n## [3] {other vegetables,                                                                              \n##      soda,                                                                                          \n##      fruit*,                                                                                        \n##      sausage*}                 => {bread and backed goods*} 0.005287     0.8525 0.006202 2.467    52"},{"path":"association-analysis-advanced-concepts.html","id":"sequential-patterns","chapter":"6 Association Analysis: Advanced Concepts","heading":"6.4 Sequential Patterns","text":"frequent sequential pattern mining algorithm cSPADE (Zaki 2000) implemented \narules extension package arulesSequences.Sequential pattern mining starts sequences events.\nsequence identified sequence ID \nevent set items happen together.\norder events specified using\nevent IDs. goal find subsequences items events \nfollow frequently. called frequent sequential pattern.look small example dataset comes package\narulesSequences.dataset contains four sequences (see sequenceID) event IDs\ninteger numbers provide order events sequence.\narulesSequences, set sequences implemented regular transaction\nset, transaction event. temporal information added\nextra columns transaction’s transactionInfo() data.frame.Mine frequent sequence patterns\nusing cspade similar using apriori.\nset support find patterns occur 50% \nsequences.example, pattern 17 shows D event, often followed \nevent containing B F turn followed event\ncontaining .cspade algorithm supports many additional parameters control gaps\nwindows. Details can found manual page cspade.Rules, similar regular association rules can generated\nfrequent sequence patterns using ruleInduction().usual measures confidence lift used.","code":"\nlibrary(arulesSequences)\n## \n## Attaching package: 'arulesSequences'\n## The following object is masked from 'package:arules':\n## \n##     itemsets\ndata(zaki)\n\ninspect(zaki)\n##      items        sequenceID eventID SIZE\n## [1]  {C, D}       1          10      2   \n## [2]  {A, B, C}    1          15      3   \n## [3]  {A, B, F}    1          20      3   \n## [4]  {A, C, D, F} 1          25      4   \n## [5]  {A, B, F}    2          15      3   \n## [6]  {E}          2          20      1   \n## [7]  {A, B, F}    3          10      3   \n## [8]  {D, G, H}    4          10      3   \n## [9]  {B, F}       4          20      2   \n## [10] {A, G, H}    4          25      3\nfsp <- cspade(zaki, parameter = list(support = .5))\nfsp |> inspect()\n##     items support \n##   1 <{A}>    1.00 \n##   2 <{B}>    1.00 \n##   3 <{D}>    0.50 \n##   4 <{F}>    1.00 \n##   5 <{A,   \n##       F}>    0.75 \n##   6 <{B,   \n##       F}>    1.00 \n##   7 <{D},  \n##      {F}>    0.50 \n##   8 <{D},  \n##      {B,   \n##       F}>    0.50 \n##   9 <{A,   \n##       B,   \n##       F}>    0.75 \n##  10 <{A,   \n##       B}>    0.75 \n##  11 <{D},  \n##      {B}>    0.50 \n##  12 <{B},  \n##      {A}>    0.50 \n##  13 <{D},  \n##      {A}>    0.50 \n##  14 <{F},  \n##      {A}>    0.50 \n##  15 <{D},  \n##      {F},  \n##      {A}>    0.50 \n##  16 <{B,   \n##       F},  \n##      {A}>    0.50 \n##  17 <{D},  \n##      {B,   \n##       F},  \n##      {A}>    0.50 \n##  18 <{D},  \n##      {B},  \n##      {A}>    0.50 \n## \nrules <- ruleInduction(fsp, confidence = .8)\nrules |> inspect()\n##    lhs      rhs   support confidence lift \n##  1 <{D}> => <{F}>     0.5          1    1 \n##  2 <{D}> => <{B,      0.5          1    1 \n##               F}>    \n##  3 <{D}> => <{B}>     0.5          1    1 \n##  4 <{D}> => <{A}>     0.5          1    1 \n##  5 <{D},             \n##     {F}> => <{A}>     0.5          1    1 \n##  6 <{D},             \n##     {B,              \n##      F}> => <{A}>     0.5          1    1 \n##  7 <{D},             \n##     {B}> => <{A}>     0.5          1    1 \n## "},{"path":"cluster-analysis.html","id":"cluster-analysis","chapter":"7 Cluster Analysis","heading":"7 Cluster Analysis","text":"chapter introduces cluster analysis using K-means, hierarchical clustering\nDBSCAN. discuss choose number clusters \nevaluate quality clusterings. addition, introduce \nclustering algorithms clustering influenced outliers.corresponding chapter \ndata mining textbook available online:\nChapter 7: Cluster Analysis: Basic Concepts Algorithms.","code":""},{"path":"cluster-analysis.html","id":"packages-used-in-this-chapter-5","chapter":"7 Cluster Analysis","heading":"Packages Used in this Chapter","text":"packages used chapter :cluster (Maechler et al. 2025)dbscan (Hahsler Piekenbrock 2025)e1071 (Meyer et al. 2024)factoextra (Kassambara Mundt 2020)fpc (Hennig 2024)GGally (Schloerke et al. 2025)kernlab (Karatzoglou, Smola, Hornik 2024)mclust (Fraley, Raftery, Scrucca 2024)mlbench (Leisch Dimitriadou 2024)scatterpie (Yu 2025)seriation (Hahsler, Buchta, Hornik 2025)tidyverse (Wickham 2023b)","code":"\npkgs <- c(\"cluster\", \"dbscan\", \"e1071\", \"factoextra\", \"fpc\", \n          \"GGally\", \"kernlab\", \"mclust\", \"mlbench\", \"scatterpie\", \n          \"seriation\", \"tidyverse\")\n  \npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"cluster-analysis.html","id":"overview","chapter":"7 Cluster Analysis","heading":"7.1 Overview","text":"Cluster analysis clustering\ntask grouping set objects way objects group (called cluster) similar (sense) groups (clusters).Clustering also called unsupervised learning, tries directly learns structure data\nrely availability correct answer class label supervised learning .\nClustering often used exploratory analysis preprocess data grouping.can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 7. Cluster Analysis: Basic Concepts \nAlgorithms","code":""},{"path":"cluster-analysis.html","id":"data-preparation","chapter":"7 Cluster Analysis","heading":"7.1.1 Data Preparation","text":"use small clean toy dataset called Ruspini \nincluded R package cluster.Ruspini data set, consisting 75 points four groups \npopular illustrating clustering techniques. simple data\nset well separated clusters. original dataset points\nordered group. can shuffle data (rows) using sample_frac\nsamples default 100%.","code":"\nlibrary(tidyverse)\ndata(ruspini, package = \"cluster\")\nruspini <- as_tibble(ruspini) |> \n  sample_frac()\nruspini\n## # A tibble: 75 × 2\n##        x     y\n##    <int> <int>\n##  1    44   149\n##  2    13    49\n##  3     5    63\n##  4    36    72\n##  5    31    60\n##  6    66    23\n##  7    34   141\n##  8    97   122\n##  9    86   132\n## 10   101   115\n## # ℹ 65 more rows"},{"path":"cluster-analysis.html","id":"data-cleaning","chapter":"7 Cluster Analysis","heading":"7.1.2 Data cleaning","text":"clustering algorithms necessary handle missing values\noutliers (e.g., remove observations). details see Section\n“Outlier removal” . data set missing values strong\noutlier looks like clear groups.","code":"\nggplot(ruspini, aes(x = x, y = y)) + geom_point()\nsummary(ruspini)\n##        x               y        \n##  Min.   :  4.0   Min.   :  4.0  \n##  1st Qu.: 31.5   1st Qu.: 56.5  \n##  Median : 52.0   Median : 96.0  \n##  Mean   : 54.9   Mean   : 92.0  \n##  3rd Qu.: 76.5   3rd Qu.:141.5  \n##  Max.   :117.0   Max.   :156.0"},{"path":"cluster-analysis.html","id":"scale-data","chapter":"7 Cluster Analysis","heading":"7.1.3 Scale data","text":"Clustering algorithms use distances variables largest\nnumber range dominate distance calculation. summary shows\nissue Ruspini dataset , x y,\nroughly 0 150. data analysts still scale\ncolumn data zero mean unit standard deviation\n(z-scores).Note: standard scale() function scales whole data\nmatrix implement function single vector apply \nnumeric columns.scaling, z-scores fall range \\([-3,3]\\) (z-scores\nmeasured standard deviations mean), \\(0\\) means\naverage.","code":"\n## I use this till tidyverse implements a scale function\nscale_numeric <- function(x) {\n  x |> mutate(across(where(is.numeric), \n                function(y) (y - mean(y, na.rm = TRUE)) / sd(y, na.rm = TRUE)))\n}\nruspini_scaled <- ruspini |> \n  scale_numeric()\nsummary(ruspini_scaled)\n##        x                 y          \n##  Min.   :-1.6681   Min.   :-1.8074  \n##  1st Qu.:-0.7665   1st Qu.:-0.7295  \n##  Median :-0.0944   Median : 0.0816  \n##  Mean   : 0.0000   Mean   : 0.0000  \n##  3rd Qu.: 0.7088   3rd Qu.: 1.0158  \n##  Max.   : 2.0366   Max.   : 1.3136"},{"path":"cluster-analysis.html","id":"k-means","chapter":"7 Cluster Analysis","heading":"7.2 K-means","text":"k-means implicitly\nassumes Euclidean distances. use \\(k = 4\\) clusters run \nalgorithm 10 times random initialized centroids. best result \nreturned.km R object implemented list.clustering vector just list element\ncontaining cluster assignment data row can accessed\nusing km$cluster. add cluster assignment column \noriginal dataset (make factor since represents nominal label).Add centroids plot. centroids scaled, need unscale \nplot original data. second geom_points uses\noriginal data specifies centroids dataset.factoextra package provides also good visualization object labels\nellipses clusters.","code":"\nkm <- kmeans(ruspini_scaled, centers = 4, nstart = 10)\nkm\n## K-means clustering with 4 clusters of sizes 20, 23, 17, 15\n## \n## Cluster means:\n##         x       y\n## 1 -1.1386 -0.5560\n## 2 -0.3595  1.1091\n## 3  1.4194  0.4693\n## 4  0.4607 -1.4912\n## \n## Clustering vector:\n##  [1] 2 1 1 1 1 4 2 3 3 3 3 4 2 3 2 2 1 1 3 3 1 4 1 4 2 4 3 3\n## [29] 4 4 3 2 2 1 1 2 1 2 2 1 3 2 4 4 2 3 2 2 4 2 1 3 1 3 1 2\n## [57] 3 3 1 1 4 4 2 1 4 2 2 1 2 2 1 4 3 4 2\n## \n## Within cluster sum of squares by cluster:\n## [1] 2.705 2.659 3.641 1.082\n##  (between_SS / total_SS =  93.2 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"       \n## [4] \"withinss\"     \"tot.withinss\" \"betweenss\"   \n## [7] \"size\"         \"iter\"         \"ifault\"\nstr(km)\n## List of 9\n##  $ cluster     : int [1:75] 2 1 1 1 1 4 2 3 3 3 ...\n##  $ centers     : num [1:4, 1:2] -1.139 -0.36 1.419 0.461 -0.556 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n##   .. ..$ : chr [1:2] \"x\" \"y\"\n##  $ totss       : num 148\n##  $ withinss    : num [1:4] 2.71 2.66 3.64 1.08\n##  $ tot.withinss: num 10.1\n##  $ betweenss   : num 138\n##  $ size        : int [1:4] 20 23 17 15\n##  $ iter        : int 3\n##  $ ifault      : int 0\n##  - attr(*, \"class\")= chr \"kmeans\"\nruspini_clustered <- ruspini |> \n  add_column(cluster = factor(km$cluster))\nruspini_clustered\n## # A tibble: 75 × 3\n##        x     y cluster\n##    <int> <int> <fct>  \n##  1    44   149 2      \n##  2    13    49 1      \n##  3     5    63 1      \n##  4    36    72 1      \n##  5    31    60 1      \n##  6    66    23 4      \n##  7    34   141 2      \n##  8    97   122 3      \n##  9    86   132 3      \n## 10   101   115 3      \n## # ℹ 65 more rows\nggplot(ruspini_clustered, aes(x = x, y = y)) + \n  geom_point(aes(color = cluster))\nunscale <- function(x, original_data) {\n  if (ncol(x) != ncol(original_data))\n    stop(\"Function needs matching columns!\")\n  x * matrix(apply(original_data, MARGIN = 2, sd, na.rm = TRUE), \n             byrow = TRUE, nrow = nrow(x), ncol = ncol(x)) + \n    matrix(apply(original_data, MARGIN = 2, mean, na.rm = TRUE), \n           byrow = TRUE, nrow = nrow(x), ncol = ncol(x))\n}\n\ncentroids <- km$centers %>% \n                unscale(original_data = ruspini) %>% \n                as_tibble(rownames = \"cluster\")\ncentroids\n## # A tibble: 4 × 3\n##   cluster     x     y\n##   <chr>   <dbl> <dbl>\n## 1 1        20.2  65.0\n## 2 2        43.9 146. \n## 3 3        98.2 115. \n## 4 4        68.9  19.4\nggplot(ruspini_clustered, aes(x = x, y = y)) + \n  geom_point(aes(color = cluster)) +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), \n             shape = 3, size = 10)\nlibrary(factoextra)\nfviz_cluster(km, data = ruspini_scaled, centroids = TRUE, \n             repel = TRUE, ellipse.type = \"norm\")"},{"path":"cluster-analysis.html","id":"inspect-clusters","chapter":"7 Cluster Analysis","heading":"7.2.1 Inspect Clusters","text":"inspect clusters created 4-cluster k-means solution. \nfollowing code can adapted used clustering methods.Inspect centroids horizontal bar charts organized cluster.\ngroup plots cluster, change data format \n“long”-format using pivot operation. use colors match \nclusters scatter plots.","code":"\nggplot(pivot_longer(centroids, \n                    cols = c(x, y), \n                    names_to = \"feature\"),\n  #aes(x = feature, y = value, fill = cluster)) +\n  aes(x = value, y = feature, fill = cluster)) +\n  geom_bar(stat = \"identity\") +\n  facet_grid(cols = vars(cluster))"},{"path":"cluster-analysis.html","id":"extract-a-single-cluster","chapter":"7 Cluster Analysis","heading":"7.2.2 Extract a Single Cluster","text":"need filter rows corresponding cluster index. \nnext example calculates summary statistics plots data\npoints cluster 1.happens try cluster 8 centers?","code":"\ncluster1 <- ruspini_clustered |> \n  filter(cluster == 1)\ncluster1\n## # A tibble: 20 × 3\n##        x     y cluster\n##    <int> <int> <fct>  \n##  1    13    49 1      \n##  2     5    63 1      \n##  3    36    72 1      \n##  4    31    60 1      \n##  5    27    72 1      \n##  6     9    77 1      \n##  7    28    76 1      \n##  8    24    58 1      \n##  9    27    55 1      \n## 10    18    61 1      \n## 11    15    75 1      \n## 12    13    69 1      \n## 13    12    88 1      \n## 14    10    59 1      \n## 15    19    65 1      \n## 16    32    61 1      \n## 17    22    74 1      \n## 18     4    53 1      \n## 19    28    60 1      \n## 20    30    52 1\nsummary(cluster1)\n##        x              y        cluster\n##  Min.   : 4.0   Min.   :49.0   1:20   \n##  1st Qu.:12.8   1st Qu.:58.8   2: 0   \n##  Median :20.5   Median :62.0   3: 0   \n##  Mean   :20.1   Mean   :65.0   4: 0   \n##  3rd Qu.:28.0   3rd Qu.:72.5          \n##  Max.   :36.0   Max.   :88.0\nggplot(cluster1, aes(x = x, y = y)) + geom_point() +\n  coord_cartesian(xlim = c(0, 120), ylim = c(0, 160))\nfviz_cluster(kmeans(ruspini_scaled, centers = 8), data = ruspini_scaled,\n  centroids = TRUE,  geom = \"point\", ellipse.type = \"norm\")\n## Too few points to calculate an ellipse"},{"path":"cluster-analysis.html","id":"agglomerative-hierarchical-clustering","chapter":"7 Cluster Analysis","heading":"7.3 Agglomerative Hierarchical Clustering","text":"Hierarchical clustering starts distance matrix. dist() defaults\nmethod = \"euclidean\".Notes:Distance matrices become large\nquickly (space complexity \\(O(n^2)\\) \\(n\\) number \ndata points). possible calculate store matrix \nsmall medium data sets (maybe hundred thousand data points) main\nmemory. data large can use sampling reduce \nnumber points cluster.data needs scaled since compute distances.","code":""},{"path":"cluster-analysis.html","id":"creating-a-dendrogram","chapter":"7 Cluster Analysis","heading":"7.3.1 Creating a Dendrogram","text":"hclust() implements agglomerative hierarchical\nclustering.\nlinkage function defines distance two clusters (sets points)\ncalculated. specify complete-link method distance two groups\nmeasured distance two farthest apart points two groups.Hierarchical clustering return cluster assignments \ndendrogram object shows objects x-axis successively joined together\ngoing along y-axis. y-axis represents distance groups points\njoined together. standard plot function displays dendrogram.package factoextra provides ggplot function plot dendrograms.plotting options dendrograms, including plotting parts large\ndendrograms can found .","code":"\nd <- dist(ruspini_scaled)\nhc <- hclust(d, method = \"complete\")\nplot(hc)\nfviz_dend(hc)"},{"path":"cluster-analysis.html","id":"extracting-a-partitional-clustering","chapter":"7 Cluster Analysis","heading":"7.3.2 Extracting a Partitional Clustering","text":"Partitional clusterings cluster assignments can extracted dendrogram\ncutting dendrogram horizontally using cutree(). cut four parts\nadd cluster id data.4 different clusters can shown dendrogram using different colors.can also color scatterplot.package factoextra provides alternative visualization. Since needs\npartition object first argument, create one simple list.results use single-link method join clusters.\nSingle-link measures distance two groups \ndistance two closest points two groups.important difference complete-link single-link \ncomplete-link prefers globular clusters single-link shows chaining\n(see staircase pattern dendrogram).","code":"\nclusters <- cutree(hc, k = 4)\ncluster_complete <- ruspini_scaled |>\n  add_column(cluster = factor(clusters))\ncluster_complete\n## # A tibble: 75 × 3\n##         x      y cluster\n##     <dbl>  <dbl> <fct>  \n##  1 -0.357  1.17  1      \n##  2 -1.37  -0.883 2      \n##  3 -1.64  -0.596 2      \n##  4 -0.619 -0.411 2      \n##  5 -0.783 -0.658 2      \n##  6  0.365 -1.42  3      \n##  7 -0.685  1.01  1      \n##  8  1.38   0.615 4      \n##  9  1.02   0.821 4      \n## 10  1.51   0.472 4      \n## # ℹ 65 more rows\nfviz_dend(hc, k = 4)\nggplot(cluster_complete, aes(x, y, color = cluster)) +\n  geom_point()\nfviz_cluster(list(data = ruspini_scaled, \n                  cluster = cutree(hc, k = 4)), \n             geom = \"point\")\nhc_single <- hclust(d, method = \"single\")\nfviz_dend(hc_single, k = 4)\nfviz_cluster(list(data = ruspini_scaled, \n                  cluster = cutree(hc_single, k = 4)), \n             geom = \"point\")"},{"path":"cluster-analysis.html","id":"dbscan","chapter":"7 Cluster Analysis","heading":"7.4 DBSCAN","text":"DBSCAN stands “Density-Based\nSpatial Clustering Applications Noise.” groups together\npoints closely packed together treats points low-density\nregions outliers. DBSCAN implemented package dbscan.","code":"\nlibrary(dbscan)\n## \n## Attaching package: 'dbscan'\n## The following object is masked from 'package:stats':\n## \n##     as.dendrogram"},{"path":"cluster-analysis.html","id":"dbscan-parameters","chapter":"7 Cluster Analysis","heading":"7.4.1 DBSCAN Parameters","text":"DBSCAN two parameters\ninteract . Changing one typically\nmeans one also adjusted.minPts defines many points needed epsilon\nneighborhood make point core point cluster. often chosen\nsmoothing parameter, larger values smooth density estimation \nalso ignore smaller structures less minPts.eps radius epsilon neighborhood around point \nnumber points counted.Users typically first select minPts. use minPts = 4.\ndecide epsilon, knee kNN distance plot often used.\npoints sorted kNN distance. idea points \nlow kNN distance located dense area become cluster.\nPoints large kNN distance low density area \nlikely represent outliers noise.Note minPts contains point , k-nearest neighbor\ndistance calculation .\nplot uses therefor k = minPts - 1.knee visible around eps = .32 (shown manually added red line).\npoints left intersection \nk-nearest neighbor distance line red line points \ncore points clustering specified parameters.","code":"\nkNNdistplot(ruspini_scaled, minPts = 4)\nabline(h = .32, col = \"red\")"},{"path":"cluster-analysis.html","id":"cluster-using-dbscan","chapter":"7 Cluster Analysis","heading":"7.4.2 Cluster using DBSCAN","text":"Clustering dbscan() returns dbscan object.cluster element cluster assignment cluster labels. \nspecial cluster label 0 means point noise assigned \ncluster.DBSCAN found 5 noise points. fviz_cluster() can \nused create ggplot visualization.Different values minPts eps can lead vastly different clusterings.\nOften, misspecification leads points noise points single cluster.\nAlso, clusters well separated, DBSCAN hard\ntime splitting .","code":"\ndb <- dbscan(ruspini_scaled, eps = .32, minPts = 4)\ndb\n## DBSCAN clustering for 75 objects.\n## Parameters: eps = 0.32, minPts = 4\n## Using euclidean distances and borderpoints = TRUE\n## The clustering contains 4 cluster(s) and 5 noise points.\n## \n##  0  1  2  3  4 \n##  5 23 20 15 12 \n## \n## Available fields: cluster, eps, minPts, metric,\n##                   borderPoints\nstr(db)\n## List of 5\n##  $ cluster     : int [1:75] 1 2 2 2 2 3 1 4 0 4 ...\n##  $ eps         : num 0.32\n##  $ minPts      : num 4\n##  $ metric      : chr \"euclidean\"\n##  $ borderPoints: logi TRUE\n##  - attr(*, \"class\")= chr [1:2] \"dbscan_fast\" \"dbscan\"\nggplot(ruspini_scaled |> add_column(cluster = factor(db$cluster)),\n  aes(x, y, color = cluster)) + geom_point()\nfviz_cluster(db, ruspini_scaled, geom = \"point\")"},{"path":"cluster-analysis.html","id":"cluster-evaluation","chapter":"7 Cluster Analysis","heading":"7.5 Cluster Evaluation","text":"","code":""},{"path":"cluster-analysis.html","id":"unsupervised-cluster-evaluation","chapter":"7 Cluster Analysis","heading":"7.5.1 Unsupervised Cluster Evaluation","text":"also often called internal cluster evaluation since use\nextra external labels.evaluate k-means clustering .","code":""},{"path":"cluster-analysis.html","id":"visual-methods","chapter":"7 Cluster Analysis","heading":"7.5.1.1 Visual Methods","text":"data 2 dimensions, scatter plot show clusters look right.First, calculate distance matrix \ninspect distance matrix first 5 objects.Matrix visualizations reordering provided package seriation.\nMatrix visualization creates\nfalse-color image value matrix pixel color representing value.advanced version plot called dissimilarity plot. reorders rows \ncolumns based cluster labels adds lines cluster borders. also\npresents average distance values diagonal make structure clusters\neasier evaluate.","code":"\nggplot(ruspini_scaled, \n       aes(x, y, color = factor(km$cluster))) + \n  geom_point()\nd <- dist(ruspini_scaled)\n\nas.matrix(d)[1:5, 1:5]\n##       1      2      3      4      5\n## 1 0.000 2.2910 2.1801 1.6026 1.8765\n## 2 2.291 0.0000 0.3891 0.8897 0.6319\n## 3 2.180 0.3891 0.0000 1.0330 0.8546\n## 4 1.603 0.8897 1.0330 0.0000 0.2959\n## 5 1.876 0.6319 0.8546 0.2959 0.0000\nlibrary(seriation)\n## \n## Attaching package: 'seriation'\n## The following object is masked from 'package:lattice':\n## \n##     panel.lines\npimage(d, main = \"Unordered\")\npimage(d, order = order(km$cluster), main = \"Reordered by cluster\")\ndissplot(d, km$cluster)"},{"path":"cluster-analysis.html","id":"evaluation-metrics","chapter":"7 Cluster Analysis","heading":"7.5.1.2 Evaluation Metrics","text":"two popular quality metrics within-cluster sum \nsquares (WCSS) used optimization objective \n\\(k\\)-means \naverage silhouette\nwidth. Look \nwithin.cluster.ss avg.silwidth .comprehensive set evaluation metric calculated cluster.stats() \npackage fpc.notes code:load package fpc using library() since mask dbscan() function package dbscan. Instead use namespace operator ::.clustering (second argument ) supplied vector\nintegers (cluster IDs) factor (make sure, can use .integer()).metrics easy identify name. important metrics :cluster.size: vector cluster sizeswithin.cluster.ss: within clusters sum squares (k-means objective function).avg.silwidth: average silhouette widthpearsongamma: correlation distances 0-1-vector cluster incidence matrix.numbers NULL. measures available supervised evaluation.\nRead man page cluster.stats() explanation available indices.can compare different clusterings.4 clusters, k-means hierarchical clustering produce\nexactly clustering. two different hierarchical methods \n5 clusters produce smaller WCSS, actually worse given three \nmeasures.Next, look silhouette using \nsilhouette plot.Note: silhouette plot show correctly R Studio \nmany objects (bars missing). work open \nnew plotting device windows(), x11() quartz().ggplot visualization using factoextra","code":"\n# library(fpc)\nfpc::cluster.stats(d, as.integer(km$cluster))\n## $n\n## [1] 75\n## \n## $cluster.number\n## [1] 4\n## \n## $cluster.size\n## [1] 20 23 17 15\n## \n## $min.cluster.size\n## [1] 15\n## \n## $noisen\n## [1] 0\n## \n## $diameter\n## [1] 1.1193 1.1591 1.4627 0.8359\n## \n## $average.distance\n## [1] 0.4824 0.4286 0.5806 0.3564\n## \n## $median.distance\n## [1] 0.4492 0.3934 0.5024 0.3380\n## \n## $separation\n## [1] 1.1577 0.7676 0.7676 1.1577\n## \n## $average.toother\n## [1] 2.157 2.149 2.293 2.308\n## \n## $separation.matrix\n##       [,1]   [,2]   [,3]  [,4]\n## [1,] 0.000 1.2199 1.3397 1.158\n## [2,] 1.220 0.0000 0.7676 1.958\n## [3,] 1.340 0.7676 0.0000 1.308\n## [4,] 1.158 1.9577 1.3084 0.000\n## \n## $ave.between.matrix\n##       [,1]  [,2]  [,3]  [,4]\n## [1,] 0.000 1.887 2.772 1.874\n## [2,] 1.887 0.000 1.925 2.750\n## [3,] 2.772 1.925 0.000 2.220\n## [4,] 1.874 2.750 2.220 0.000\n## \n## $average.between\n## [1] 2.219\n## \n## $average.within\n## [1] 0.463\n## \n## $n.between\n## [1] 2091\n## \n## $n.within\n## [1] 684\n## \n## $max.diameter\n## [1] 1.463\n## \n## $min.separation\n## [1] 0.7676\n## \n## $within.cluster.ss\n## [1] 10.09\n## \n## $clus.avg.silwidths\n##      1      2      3      4 \n## 0.7211 0.7455 0.6813 0.8074 \n## \n## $avg.silwidth\n## [1] 0.7368\n## \n## $g2\n## NULL\n## \n## $g3\n## NULL\n## \n## $pearsongamma\n## [1] 0.8416\n## \n## $dunn\n## [1] 0.5248\n## \n## $dunn2\n## [1] 3.228\n## \n## $entropy\n## [1] 1.373\n## \n## $wb.ratio\n## [1] 0.2086\n## \n## $ch\n## [1] 323.6\n## \n## $cwidegap\n## [1] 0.2612 0.3153 0.4150 0.2352\n## \n## $widestgap\n## [1] 0.415\n## \n## $sindex\n## [1] 0.8583\n## \n## $corrected.rand\n## NULL\n## \n## $vi\n## NULL\nsapply(\n  list(\n    km_4 = km$cluster,\n    hc_compl_4 = cutree(hc, k = 4),\n    hc_compl_5 = cutree(hc, k = 5),\n    hc_single_5 = cutree(hc_single, k = 5)\n  ),\n  FUN = function(x)\n    fpc::cluster.stats(d, x))[c(\"within.cluster.ss\", \n                                \"avg.silwidth\", \n                                \"pearsongamma\", \n                                \"dunn\"), ]\n##                   km_4   hc_compl_4 hc_compl_5 hc_single_5\n## within.cluster.ss 10.09  10.09      8.314      7.791      \n## avg.silwidth      0.7368 0.7368     0.6642     0.6886     \n## pearsongamma      0.8416 0.8416     0.8042     0.816      \n## dunn              0.5248 0.5248     0.1988     0.358\nlibrary(cluster)\n## \n## Attaching package: 'cluster'\n## The following object is masked _by_ '.GlobalEnv':\n## \n##     ruspini\nplot(silhouette(km$cluster, d))\nfviz_silhouette(silhouette(km$cluster, d))\n##   cluster size ave.sil.width\n## 1       1   20          0.72\n## 2       2   23          0.75\n## 3       3   17          0.68\n## 4       4   15          0.81"},{"path":"cluster-analysis.html","id":"determining-the-correct-number-of-clusters","chapter":"7 Cluster Analysis","heading":"7.5.2 Determining the Correct Number of Clusters","text":"user needs specify number clusters clustering algorithms.\nDetermining number clusters data set\ntherefor important task can cluster data. apply different methods\nscaled Ruspini data set.","code":"\nggplot(ruspini_scaled, aes(x, y)) + geom_point()\n## We will use different methods and try 1-10 clusters.\nset.seed(1234)\nks <- 2:10"},{"path":"cluster-analysis.html","id":"elbow-method-within-cluster-sum-of-squares","chapter":"7 Cluster Analysis","heading":"7.5.2.1 Elbow Method: Within-Cluster Sum of Squares","text":"method often used k-means calculate within-cluster sum squares different numbers \nclusters look knee \nelbow \nplot. use nstart = 5 restart k-means 5 times return best\nsolution.","code":"\nWCSS <- sapply(ks, FUN = function(k) {\n  kmeans(ruspini_scaled, centers = k, nstart = 5)$tot.withinss\n  })\n\nggplot(tibble(ks, WCSS), aes(ks, WCSS)) + \n  geom_line() +\n  geom_vline(xintercept = 4, color = \"red\", linetype = 2)"},{"path":"cluster-analysis.html","id":"average-silhouette-width","chapter":"7 Cluster Analysis","heading":"7.5.2.2 Average Silhouette Width","text":"Another popular method (often preferred clustering methods starting distance matrix)\nplot average silhouette width different numbers clusters \nlook maximum plot.","code":"\nASW <- sapply(ks, FUN=function(k) {\n  fpc::cluster.stats(d, \n                     kmeans(ruspini_scaled, \n                            centers = k, \n                            nstart = 5)$cluster)$avg.silwidth\n  })\n\nbest_k <- ks[which.max(ASW)]\nbest_k\n## [1] 4\nggplot(tibble(ks, ASW), aes(ks, ASW)) + \n  geom_line() +\n  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"},{"path":"cluster-analysis.html","id":"similarity-matrix-visualization","chapter":"7 Cluster Analysis","heading":"7.5.2.3 Similarity Matrix Visualization","text":"can visualize similarity matrix reordered cluster labels visually determine number clusters \ngood fit data.\nPackage seriation provides dissimilarity plot make easy.plot reorders rows \ncolumns based cluster labels adds lines cluster borders. also\npresents average distance values diagonal make structure clusters\neasier evaluate. , see clear structure \nfour clusters.Next, look two examples wherethe number clusters\nmisspecified.","code":"\nggdissplot(d, labels = km$cluster, \n         options = list(main = \"k-means with k=4\"))\nggdissplot(d, \n         labels = kmeans(ruspini_scaled, centers = 3, nstart = 5)$cluster)\nggdissplot(d, \n         labels = kmeans(ruspini_scaled, centers = 9, nstart = 5)$cluster)"},{"path":"cluster-analysis.html","id":"dunn-index","chapter":"7 Cluster Analysis","heading":"7.5.2.4 Dunn Index","text":"Dunn index another\ninternal measure given smallest separation clusters scaled \nlargest cluster diameter.","code":"\nDI <- sapply(ks, FUN = function(k) {\n  fpc::cluster.stats(d, \n                     kmeans(ruspini_scaled, centers = k, \n                            nstart = 5)$cluster)$dunn\n})\n\nbest_k <- ks[which.max(DI)]\nggplot(tibble(ks, DI), aes(ks, DI)) + \n  geom_line() +\n  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"},{"path":"cluster-analysis.html","id":"gap-statistic","chapter":"7 Cluster Analysis","heading":"7.5.2.5 Gap Statistic","text":"Gap statisticCompares change within-cluster dispersion expected \nnull model (see clusGap()). default method choose \nsmallest k value Gap(k) 1 standard error\naway first local maximum.many methods indices proposed determine \nnumber clusters. See, e.g., package\nNbClust.","code":"\nlibrary(cluster)\nk <- clusGap(ruspini_scaled, \n             FUN = kmeans,  \n             nstart = 10, \n             K.max = 10)\nk\n## Clustering Gap statistic [\"clusGap\"] from call:\n## clusGap(x = ruspini_scaled, FUNcluster = kmeans, K.max = 10, nstart = 10)\n## B=100 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n##  --> Number of clusters (method 'firstSEmax', SE.factor=1): 4\n##        logW E.logW      gap  SE.sim\n##  [1,] 3.498  3.464 -0.03387 0.03824\n##  [2,] 3.074  3.151  0.07724 0.03605\n##  [3,] 2.678  2.903  0.22549 0.03861\n##  [4,] 2.106  2.712  0.60533 0.03793\n##  [5,] 1.987  2.574  0.58708 0.03599\n##  [6,] 1.864  2.456  0.59181 0.03442\n##  [7,] 1.732  2.352  0.61966 0.03577\n##  [8,] 1.660  2.257  0.59679 0.03758\n##  [9,] 1.612  2.168  0.55592 0.04024\n## [10,] 1.563  2.090  0.52667 0.04291\nplot(k)"},{"path":"cluster-analysis.html","id":"clustering-tendency","chapter":"7 Cluster Analysis","heading":"7.5.3 Clustering Tendency","text":"clustering algorithms always produce clustering, even \ndata contain cluster structure. typically good check\ncluster tendency attempting cluster data.use smiley data.","code":"\nlibrary(mlbench)\nshapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)$x\ncolnames(shapes) <- c(\"x\", \"y\")\nshapes <- as_tibble(shapes)"},{"path":"cluster-analysis.html","id":"scatter-plots","chapter":"7 Cluster Analysis","heading":"7.5.3.1 Scatter plots","text":"first step visual inspection using scatter plots.Cluster tendency typically indicated several separated point\nclouds. Often appropriate number clusters can also visually\nobtained counting number point clouds. see four clusters,\nmouth convex/spherical thus pose problems \nalgorithms like k-means.data two features can use pairs plot\n(scatterplot matrix) look scatterplot first two principal\ncomponents using PCA.","code":"\nggplot(shapes, aes(x = x, y = y)) + geom_point()"},{"path":"cluster-analysis.html","id":"visual-analysis-for-cluster-tendency-assessment-vat","chapter":"7 Cluster Analysis","heading":"7.5.3.2 Visual Analysis for Cluster Tendency Assessment (VAT)","text":"VAT reorders objects show potential clustering tendency \nblock structure (dark blocks along main diagonal). scale data\nusing Euclidean distance.iVAT uses largest distances possible paths two\nobjects instead direct distances make block structure\nbetter visible.","code":"\nlibrary(seriation)\n\nd_shapes <- dist(scale(shapes))\nggVAT(d_shapes)\nggiVAT(d_shapes)"},{"path":"cluster-analysis.html","id":"hopkins-statistic","chapter":"7 Cluster Analysis","heading":"7.5.3.3 Hopkins statistic","text":"factoextra can also create VAT plot calculate Hopkins\nstatistic assess\nclustering tendency. Hopkins statistic, sample size \\(n\\) \ndrawn data compares nearest neighbor distribution\nsimulated dataset drawn random uniform distribution (see\ndetailed\nexplanation).\nvalues >.5 indicates usually clustering tendency.plots show strong cluster structure 4 clusters.","code":"\nget_clust_tendency(shapes, n = 10)\n## $hopkins_stat\n## [1] 0.9498\n## \n## $plot"},{"path":"cluster-analysis.html","id":"data-without-clustering-tendency","chapter":"7 Cluster Analysis","heading":"7.5.3.4 Data Without Clustering Tendency","text":"point clouds visible, just noise.little clustering structure visible indicating low\nclustering tendency clustering performed data.\nHowever, k-means can used partition data \\(k\\) regions \nroughly equivalent size. can used data-driven\ndiscretization space.","code":"\ndata_random <- tibble(x = runif(500), y = runif(500))\nggplot(data_random, aes(x, y)) + geom_point()\nd_random <- dist(data_random)\nggVAT(d_random)\nggiVAT(d_random)\nget_clust_tendency(data_random, n = 10, graph = FALSE)\n## $hopkins_stat\n## [1] 0.4642\n## \n## $plot\n## NULL"},{"path":"cluster-analysis.html","id":"k-means-on-data-without-clustering-tendency","chapter":"7 Cluster Analysis","heading":"7.5.3.5 k-means on Data Without Clustering Tendency","text":"happens perform k-means data inherent\nclustering structure?k-means discretizes space similarly sized regions.","code":"\nkm <- kmeans(data_random, centers = 4)\n\nrandom_clustered<- data_random |> \n  add_column(cluster = factor(km$cluster))\nggplot(random_clustered, aes(x = x, y = y, color = cluster)) + \n  geom_point()"},{"path":"cluster-analysis.html","id":"supervised-cluster-evaluation","chapter":"7 Cluster Analysis","heading":"7.5.4 Supervised Cluster Evaluation","text":"Also called external cluster validation since uses ground truth information.\n, \nuser idea data grouped. known\nclass label provided clustering algorithm.use artificial data set known groups.First, prepare data hide known class label used\nevaluation clustering.Find optimal number Clusters k-meansUse within sum squares (look knee)Looks like 7 clusters?mouth issue k-means. use hierarchical clustering single-linkage mouth \nnon-convex chaining may help.Find optimal number clustersThe maximum clearly 4 clusters.Compare ground truth adjusted Rand index\n(ARI, also\ncorrected Rand index),\nvariation information (VI)\nindex,\nmutual information (MI),\nentropy\n\npurity.cluster_stats() computes ARI VI comparative measures. define\nfunctions purity entropy clustering given ground truth :calculate measures add comparison two random “clusterings” \n4 6 clusters.Notes:Comparing ground truth produces perfect scores.Hierarchical clustering found perfect clustering.Entropy purity heavily impacted number clusters\n(clusters improve metric) fact mouth 41.8%\ndata points becoming automatically majority class purity.adjusted rand index shows clearly random clusterings\nrelationship ground truth (close 0). \nhelpful property explains ARI popular\nmeasure.Read manual page fpc::cluster.stats() explanation available indices.","code":"\nlibrary(mlbench)\nset.seed(1234)\nshapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)\nplot(shapes)\ntruth <- as.integer(shapes$class)\nshapes <- shapes$x\ncolnames(shapes) <- c(\"x\", \"y\")\n\nshapes <- shapes |> scale() |> as_tibble()\n\nggplot(shapes, aes(x, y)) + geom_point()\nks <- 2:20\nWCSS <- sapply(ks, FUN = function(k) {\n  kmeans(shapes, centers = k, nstart = 10)$tot.withinss\n})\n\nggplot(tibble(ks, WCSS), aes(ks, WCSS)) + geom_line()\nkm <- kmeans(shapes, centers = 7, nstart = 10)\n\nggplot(shapes |> add_column(cluster = factor(km$cluster)), \n       aes(x, y, color = cluster)) +\n  geom_point()\nd <- dist(shapes)\nhc <- hclust(d, method = \"single\")\nASW <- sapply(ks, FUN = function(k) {\n  fpc::cluster.stats(d, cutree(hc, k))$avg.silwidth\n})\n\nggplot(tibble(ks, ASW), aes(ks, ASW)) + geom_line()\nhc_4 <- cutree(hc, 4)\n\nggplot(shapes |> add_column(cluster = factor(hc_4)), \n       aes(x, y, color = cluster)) +\n  geom_point()\npurity <- function(cluster, truth, show_table = FALSE) {\n  if (length(cluster) != length(truth))\n    stop(\"Cluster vector and ground truth vectors are not of the same length!\")\n  \n  # tabulate\n  tbl <- table(cluster, truth)\n  if(show_table)\n    print(tbl)\n  \n  # find majority class\n  majority <- apply(tbl, 1, max)\n  sum(majority) / length(cluster)\n}\n\nentropy <- function(cluster, truth, show_table = FALSE) {\n  if (length(cluster) != length(truth))\n    stop(\"Cluster vector and ground truth vectors are not of the same length!\")\n  \n  # calculate membership probability of cluster to class\n  tbl <- table(cluster, truth)\n  p <- sweep(tbl, 2, colSums(tbl), \"/\")\n  \n  if(show_table)\n    print(p)\n\n  # calculate cluster entropy\n  e <- -p * log(p, 2)\n  e <- rowSums(e, na.rm = TRUE)\n  \n  # weighted sum over clusters\n  w <- table(cluster) / length(cluster)\n  sum(w * e)\n}\nrandom_4 <- sample(1:4, nrow(shapes), replace = TRUE)\nrandom_6 <- sample(1:6, nrow(shapes), replace = TRUE)\n\nr <- rbind(\n  truth = c(\n    unlist(fpc::cluster.stats(d, truth, \n                              truth, compareonly = TRUE)),\n    purity = purity(truth, truth),\n    entropy = entropy(truth, truth)\n  ),\n  \n  kmeans_7 = c(\n    unlist(fpc::cluster.stats(d, km$cluster, \n                              truth, compareonly = TRUE)),\n    purity = purity(km$cluster, truth),\n    entropy = entropy(km$cluster, truth)\n    ),\n  hc_4 = c(\n    unlist(fpc::cluster.stats(d, hc_4, \n                              truth, compareonly = TRUE)),\n    purity = purity(hc_4, truth),\n    entropy = entropy(hc_4, truth)\n    ),\n  random_4 = c(\n    unlist(fpc::cluster.stats(d, random_4, \n                              truth, compareonly = TRUE)),\n    purity = purity(random_4, truth),\n    entropy = entropy(random_4, truth)\n    ),\n  random_6 = c(\n    unlist(fpc::cluster.stats(d, random_6, \n                              truth, compareonly = TRUE)),\n    purity = purity(random_6, truth),\n    entropy = entropy(random_6, truth)\n    )\n  )\nr\n##          corrected.rand     vi purity entropy\n## truth          1.000000 0.0000  1.000  0.0000\n## kmeans_7       0.638229 0.5709  1.000  0.2088\n## hc_4           1.000000 0.0000  1.000  0.0000\n## random_4      -0.003235 2.6832  0.418  1.9895\n## random_6      -0.002125 3.0763  0.418  1.7129"},{"path":"cluster-analysis.html","id":"more-clustering-algorithms","chapter":"7 Cluster Analysis","heading":"7.6 More Clustering Algorithms*","text":"Note: methods covered Chapter 8 textbook.","code":""},{"path":"cluster-analysis.html","id":"partitioning-around-medoids-pam","chapter":"7 Cluster Analysis","heading":"7.6.1 Partitioning Around Medoids (PAM)","text":"PAM tries solve \n\\(k\\)-medoids problem. problem similar \\(k\\)-means, uses\nmedoids instead centroids represent clusters. Like hierarchical\nclustering, typically works precomputed distance matrix. \nadvantage can use distance metric just Euclidean\ndistances. Note: medoid central data point \nmiddle cluster. PAM lot computationally expensive compared \nk-means.Extract clustering medoids visualization.Alternative visualization using fviz_cluster().","code":"\nlibrary(cluster)\n\nd <- dist(ruspini_scaled)\nstr(d)\n##  'dist' num [1:2775] 2.29 2.18 1.6 1.88 2.69 ...\n##  - attr(*, \"Size\")= int 75\n##  - attr(*, \"Diag\")= logi FALSE\n##  - attr(*, \"Upper\")= logi FALSE\n##  - attr(*, \"method\")= chr \"Euclidean\"\n##  - attr(*, \"call\")= language dist(x = ruspini_scaled)\np <- pam(d, k = 4)\np\n## Medoids:\n##      ID   \n## [1,]  1  1\n## [2,] 55 55\n## [3,] 22 22\n## [4,] 19 19\n## Clustering vector:\n##  [1] 1 2 2 2 2 3 1 4 4 4 4 3 1 4 1 1 2 2 4 4 2 3 2 3 1 3 4 4\n## [29] 3 3 4 1 1 2 2 1 2 1 1 2 4 1 3 3 1 4 1 1 3 1 2 4 2 4 2 1\n## [57] 4 4 2 2 3 3 1 2 3 1 1 2 1 1 2 3 4 3 1\n## Objective function:\n##  build   swap \n## 0.4423 0.3187 \n## \n## Available components:\n## [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\" \n## [5] \"isolation\"  \"clusinfo\"   \"silinfo\"    \"diss\"      \n## [9] \"call\"\nruspini_clustered <- ruspini_scaled |> \n  add_column(cluster = factor(p$cluster))\n\nmedoids <- as_tibble(ruspini_scaled[p$medoids, ], \n                     rownames = \"cluster\")\nmedoids\n## # A tibble: 4 × 3\n##   cluster      x      y\n##   <chr>    <dbl>  <dbl>\n## 1 1       -0.357  1.17 \n## 2 2       -1.18  -0.555\n## 3 3        0.463 -1.46 \n## 4 4        1.45   0.554\nggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + \n  geom_point() +\n  geom_point(data = medoids, aes(x = x, y = y, color = cluster), \n             shape = 3, size = 10)\n## __Note:__ `fviz_cluster` needs the original data.\nfviz_cluster(c(p, list(data = ruspini_scaled)), \n             geom = \"point\", \n             ellipse.type = \"norm\")"},{"path":"cluster-analysis.html","id":"gaussian-mixture-models","chapter":"7 Cluster Analysis","heading":"7.6.2 Gaussian Mixture Models","text":"Gaussian mixture\nmodels\nassume data set result drawing data set \nGaussian distributions distribution represents cluster.\nEstimation algorithms try identify location parameters \ndistributions thus can used find clusters. Mclust() uses\nBayesian Information Criterion (BIC) find number clusters\n(model selection). BIC uses likelihood penalty term guard\noverfitting.Rerun fixed number 4 clusters","code":"\nlibrary(mclust)\n## Package 'mclust' version 6.1.1\n## Type 'citation(\"mclust\")' for citing this R package in publications.\n## \n## Attaching package: 'mclust'\n## The following object is masked from 'package:purrr':\n## \n##     map\nm <- Mclust(ruspini_scaled)\nsummary(m)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEI (diagonal, equal volume and shape) model with 5\n## components: \n## \n##  log-likelihood  n df    BIC    ICL\n##          -91.26 75 16 -251.6 -251.7\n## \n## Clustering table:\n##  1  2  3  4  5 \n## 23 20 15 14  3\nplot(m, what = \"classification\")\nm <- Mclust(ruspini_scaled, G=4)\nsummary(m)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEI (diagonal, equal volume and shape) model with 4\n## components: \n## \n##  log-likelihood  n df    BIC    ICL\n##          -101.6 75 13 -259.3 -259.3\n## \n## Clustering table:\n##  1  2  3  4 \n## 23 20 15 17\nplot(m, what = \"classification\")"},{"path":"cluster-analysis.html","id":"spectral-clustering","chapter":"7 Cluster Analysis","heading":"7.6.3 Spectral Clustering","text":"Spectral clustering\nworks embedding data points partitioning problem \nsubspace k largest eigenvectors normalized affinity/kernel\nmatrix. uses simple clustering method like k-means.","code":"\nlibrary(\"kernlab\")\n## \n## Attaching package: 'kernlab'\n## The following object is masked from 'package:arulesSequences':\n## \n##     size\n## The following object is masked from 'package:scales':\n## \n##     alpha\n## The following object is masked from 'package:arules':\n## \n##     size\n## The following object is masked from 'package:purrr':\n## \n##     cross\n## The following object is masked from 'package:ggplot2':\n## \n##     alpha\ncluster_spec <- specc(as.matrix(ruspini_scaled), centers = 4)\ncluster_spec\n## Spectral Clustering object of class \"specc\" \n## \n##  Cluster memberships: \n##  \n## 4 3 3 3 3 2 4 1 1 1 1 2 4 1 4 4 3 3 1 1 3 2 3 2 4 2 1 1 2 2 1 4 4 3 3 4 3 4 4 3 1 4 2 2 4 1 4 4 2 4 3 1 3 1 3 4 1 1 3 3 2 2 4 3 2 4 4 3 4 4 3 2 1 2 4 \n##  \n## Gaussian Radial Basis kernel function. \n##  Hyperparameter : sigma =  20.8835033729148 \n## \n## Centers:  \n##         [,1]    [,2]\n## [1,]  1.4194  0.4693\n## [2,]  0.4607 -1.4912\n## [3,] -1.1386 -0.5560\n## [4,] -0.3595  1.1091\n## \n## Cluster size:  \n## [1] 17 15 20 23\n## \n## Within-cluster sum of squares:  \n## [1] 18.93 54.85 11.34 45.48\nggplot(ruspini_scaled |> \n         add_column(cluster = factor(cluster_spec)),\n       aes(x, y, color = cluster)) + \n  geom_point()"},{"path":"cluster-analysis.html","id":"deep-clustering-methods","chapter":"7 Cluster Analysis","heading":"7.6.4 Deep Clustering Methods","text":"Deep clustering often used high-dimensional data.\nuses deep autoencoder learn cluster-friendly representation\ndata applying standard clustering algorithm (e.g., k-means)\nembedded data.\nautoencoder\ntypically implemented using keras\nautoencoder’s loss function modified include\nclustering loss make embedding clusterable.","code":""},{"path":"cluster-analysis.html","id":"fuzzy-c-means-clustering","chapter":"7 Cluster Analysis","heading":"7.6.5 Fuzzy C-Means Clustering","text":"fuzzy clustering\nversion k-means clustering problem. data point degree\nmembership cluster.Plot membership (shown small pie charts)","code":"\nlibrary(\"e1071\")\n## \n## Attaching package: 'e1071'\n## The following object is masked from 'package:ggplot2':\n## \n##     element\ncluster_cmeans <- cmeans(as.matrix(ruspini_scaled), centers = 4)\ncluster_cmeans\n## Fuzzy c-means clustering with 4 clusters\n## \n## Cluster centers:\n##         x       y\n## 1 -1.1371 -0.5550\n## 2 -0.3763  1.1143\n## 3  0.4552 -1.4760\n## 4  1.5048  0.5161\n## \n## Memberships:\n##               1         2         3         4\n##  [1,] 0.0009643 9.977e-01 0.0004512 8.879e-04\n##  [2,] 0.9148680 3.002e-02 0.0405062 1.461e-02\n##  [3,] 0.8877281 4.917e-02 0.0431101 1.999e-02\n##  [4,] 0.7683893 9.312e-02 0.0971203 4.137e-02\n##  [5,] 0.8900960 3.663e-02 0.0549604 1.831e-02\n##  [6,] 0.0038612 1.664e-03 0.9921767 2.298e-03\n##  [7,] 0.0376056 9.294e-01 0.0133136 1.973e-02\n##  [8,] 0.0032212 7.444e-03 0.0047479 9.846e-01\n##  [9,] 0.0394179 1.267e-01 0.0461290 7.877e-01\n## [10,] 0.0002500 5.072e-04 0.0004110 9.988e-01\n## [11,] 0.0075741 1.388e-02 0.0135387 9.650e-01\n## [12,] 0.0254632 9.526e-03 0.9531429 1.187e-02\n## [13,] 0.0419832 9.187e-01 0.0155189 2.380e-02\n## [14,] 0.1075095 1.739e-01 0.1773899 5.412e-01\n## [15,] 0.0483850 9.103e-01 0.0168055 2.448e-02\n## [16,] 0.0371539 9.245e-01 0.0146294 2.371e-02\n## [17,] 0.9426524 2.538e-02 0.0220711 9.896e-03\n## [18,] 0.8953110 5.308e-02 0.0336348 1.798e-02\n## [19,] 0.0006092 1.324e-03 0.0009436 9.971e-01\n## [20,] 0.0106794 1.929e-02 0.0192355 9.508e-01\n## [21,] 0.9045073 4.507e-02 0.0339770 1.645e-02\n## [22,] 0.0001096 5.052e-05 0.9997656 7.424e-05\n## [23,] 0.9731699 9.545e-03 0.0127747 4.511e-03\n## [24,] 0.0030791 1.396e-03 0.9934965 2.029e-03\n## [25,] 0.0713882 7.016e-01 0.0509709 1.760e-01\n## [26,] 0.0205931 1.087e-02 0.9503616 1.818e-02\n## [27,] 0.0392620 9.618e-02 0.0536215 8.109e-01\n## [28,] 0.1283082 2.177e-01 0.1837576 4.703e-01\n## [29,] 0.0249349 1.136e-02 0.9472520 1.646e-02\n## [30,] 0.0384838 2.343e-02 0.8921802 4.591e-02\n## [31,] 0.0130801 2.683e-02 0.0205434 9.395e-01\n## [32,] 0.0033849 9.924e-01 0.0014973 2.767e-03\n## [33,] 0.0151633 9.597e-01 0.0078901 1.728e-02\n## [34,] 0.9302684 2.248e-02 0.0358179 1.143e-02\n## [35,] 0.9920941 3.143e-03 0.0034030 1.360e-03\n## [36,] 0.0112519 9.693e-01 0.0059278 1.354e-02\n## [37,] 0.9540725 2.256e-02 0.0155206 7.844e-03\n## [38,] 0.0920675 7.514e-01 0.0519053 1.047e-01\n## [39,] 0.0758512 8.622e-01 0.0256682 3.625e-02\n## [40,] 0.9626044 1.710e-02 0.0138101 6.486e-03\n## [41,] 0.0058692 1.130e-02 0.0099614 9.729e-01\n## [42,] 0.0204611 9.395e-01 0.0114699 2.852e-02\n## [43,] 0.0241504 1.011e-02 0.9523623 1.337e-02\n## [44,] 0.0173389 9.017e-03 0.9591007 1.454e-02\n## [45,] 0.0015600 9.964e-01 0.0007051 1.322e-03\n## [46,] 0.0184319 3.390e-02 0.0318375 9.158e-01\n## [47,] 0.0120649 9.754e-01 0.0047499 7.761e-03\n## [48,] 0.0333622 8.915e-01 0.0199935 5.510e-02\n## [49,] 0.0498123 1.727e-02 0.9125422 2.038e-02\n## [50,] 0.0672233 7.551e-01 0.0448213 1.329e-01\n## [51,] 0.8288352 9.817e-02 0.0452768 2.771e-02\n## [52,] 0.0061528 1.483e-02 0.0087249 9.703e-01\n## [53,] 0.9343415 2.687e-02 0.0272619 1.153e-02\n## [54,] 0.0011602 2.456e-03 0.0018406 9.945e-01\n## [55,] 0.9989320 4.474e-04 0.0004367 1.839e-04\n## [56,] 0.0044839 9.885e-01 0.0022374 4.752e-03\n## [57,] 0.0034560 8.073e-03 0.0050417 9.834e-01\n## [58,] 0.0229230 4.091e-02 0.0405213 8.956e-01\n## [59,] 0.8727657 4.260e-02 0.0633412 2.130e-02\n## [60,] 0.9712833 1.357e-02 0.0102381 4.907e-03\n## [61,] 0.0287389 1.083e-02 0.9470184 1.341e-02\n## [62,] 0.0203875 1.111e-02 0.9492342 1.926e-02\n## [63,] 0.0256390 9.467e-01 0.0103553 1.729e-02\n## [64,] 0.8604239 5.522e-02 0.0593849 2.497e-02\n## [65,] 0.0082410 3.347e-03 0.9839900 4.423e-03\n## [66,] 0.0114703 9.753e-01 0.0048169 8.408e-03\n## [67,] 0.0103266 9.787e-01 0.0041349 6.878e-03\n## [68,] 0.9397726 2.104e-02 0.0290836 1.010e-02\n## [69,] 0.0095409 9.763e-01 0.0046348 9.540e-03\n## [70,] 0.0046268 9.889e-01 0.0021822 4.269e-03\n## [71,] 0.8665154 3.837e-02 0.0740308 2.108e-02\n## [72,] 0.0183428 1.016e-02 0.9537438 1.775e-02\n## [73,] 0.0654674 1.100e-01 0.1188607 7.056e-01\n## [74,] 0.0031669 1.347e-03 0.9936326 1.853e-03\n## [75,] 0.0247839 9.272e-01 0.0139396 3.405e-02\n## \n## Closest hard clustering:\n##  [1] 2 1 1 1 1 3 2 4 4 4 4 3 2 4 2 2 1 1 4 4 1 3 1 3 2 3 4 4\n## [29] 3 3 4 2 2 1 1 2 1 2 2 1 4 2 3 3 2 4 2 2 3 2 1 4 1 4 1 2\n## [57] 4 4 1 1 3 3 2 1 3 2 2 1 2 2 1 3 4 3 2\n## \n## Available components:\n## [1] \"centers\"     \"size\"        \"cluster\"     \"membership\" \n## [5] \"iter\"        \"withinerror\" \"call\"\nlibrary(\"scatterpie\")\n## scatterpie v0.2.6 Learn more at https://yulab-smu.top/\nggplot()  +\n  geom_scatterpie(\n    data = cbind(ruspini_scaled, cluster_cmeans$membership),\n    aes(x = x, y = y), \n    cols = colnames(cluster_cmeans$membership), \n    legend_name = \"Membership\") + \n  coord_equal()"},{"path":"cluster-analysis.html","id":"scale-issues-in-clustering","chapter":"7 Cluster Analysis","heading":"7.7 Scale Issues in Clustering*","text":"demonstrate issue, use unscaled Ruspini dataset assume \nmeasure x millimeters y meters. multiplying x 1000.cluster data now, algorithm fails!clusters form vertical bands across whole plot.\nreason large numeric differences x-axis\noverpower relatively small differences y-axis distances \ncalculated. issue can avoided scaling variables first.","code":"\nruspini_scale_issue <- ruspini |> mutate(x = x * 1000)\nsummary(ruspini_scale_issue)\n##        x                y        \n##  Min.   :  4000   Min.   :  4.0  \n##  1st Qu.: 31500   1st Qu.: 56.5  \n##  Median : 52000   Median : 96.0  \n##  Mean   : 54880   Mean   : 92.0  \n##  3rd Qu.: 76500   3rd Qu.:141.5  \n##  Max.   :117000   Max.   :156.0\nkm <- kmeans(ruspini_scale_issue, centers = 4)\nruspini_clustered <- ruspini_scale_issue |> \n  add_column(cluster = factor(km$cluster))\n\nggplot(ruspini_clustered, aes(x = x, y = y)) + \n  geom_point(aes(color = cluster))"},{"path":"cluster-analysis.html","id":"outliers-in-clustering","chapter":"7 Cluster Analysis","heading":"7.8 Outliers in Clustering*","text":"clustering algorithms perform complete assignment (.e., data\npoints need assigned cluster). Outliers affect \nclustering.show effect, add clear outlier large \\(x\\) value Ruspini dataset.outlier presents problem k-means, even scale data. Scaling data\noutlier make direction outlier (case \\(x\\))\nimportant.Often, outliers become clusters.\ntempting deal issue\nadding additional clusters, one per outlier.\nwork!result one additional cluster.seems work, happens clusters \nRuspini dataset nicely spread \\(y\\) axis. outlier suppresses\neffect variability \\(x\\) direction. top two clusters \nfollowing plot clustering without outlier point\nshows clustering just uses \\(y\\) axis cut data completely\nignors \\(x\\).work!\nneed identify \nremove outliers scaling data.\nMethods identify outliers summary statistics, visual inspection,\noutlier scores like Local Outlier Factor (LOF) discussed following\nsections.","code":"\nlibrary(dbscan)\nruspini_outlier <- ruspini |> add_case(x=10000, y=100)\nggplot(ruspini_outlier, aes(x = x, y = y)) + \n  geom_point()\nruspini_scaled_outlier <- ruspini_outlier |> scale_numeric()\n\nkm <- kmeans(ruspini_scaled_outlier, centers = 4, nstart = 10)\nruspini_scaled_outlier_km <- ruspini_scaled_outlier|>\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + \n  geom_point() +\n  geom_point(data = centroids, \n             aes(x = x, y = y, color = cluster), \n             shape = 3, size = 10)\nkm <- kmeans(ruspini_scaled_outlier, centers = 5, nstart = 10)\nruspini_scaled_outlier_km <- ruspini_scaled_outlier|>\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + \n  geom_point() +\n  geom_point(data = centroids, \n             aes(x = x, y = y, color = cluster), \n             shape = 3, size = 10)\nggplot(ruspini_scaled_outlier_km |> filter(row_number() <= n()-1), aes(x = x, y = y, color = cluster)) + \n  geom_point()"},{"path":"cluster-analysis.html","id":"visual-identification-of-outliers","chapter":"7 Cluster Analysis","heading":"7.8.1 Visual Identification of Outliers","text":"Outliers can identified using summary statistics, histograms,\nscatterplots (pairs plots), boxplots, etc. use pairs plot\n(diagonal contains smoothed histograms). outlier visible \nsingle separate point scatter plot long tail \nsmoothed histogram x. Remember, scaled data, expect observations \nfall range \\([-3,3]\\).tope left panel plot shows therw outlier x axis.\nx value regular points way less 2500 can use information\nidentify outliers.\ncan find row number byand manually remove scaling data.","code":"\nlibrary(\"GGally\")\nggpairs(ruspini_outlier, progress = FALSE)\nwhich(ruspini_outlier$x > 2500)\n## [1] 76"},{"path":"cluster-analysis.html","id":"local-outlier-factor","chapter":"7 Cluster Analysis","heading":"7.8.2 Local Outlier Factor","text":"Local Outlier\nFactor (LOF) related\nconcepts DBSCAN can help identify potential outliers.\nLOF compares density data point neighboring data points.\ndata point much lower density neighbors, high LOF score considered outlier.useful identify outliers remove strong outliers\nprior clustering. density based method identify outlier \nLOF (Local Outlier\nFactor). related dbscan compares density around point\ndensities around neighbors (specify \nneighborhood size \\(k\\)). LOF value regular data point 1. \nlarger LOF value gets, likely point outlier.calculate\nLOF, local neighborhood size (MinPts DBSCAN) density estimation\nneeds chosen. use 10 since expect clusters \n10 points. Note LOF uses distances, need scale data first.Plot points sorted increasing LOF look knee.choose threshold 2.analyze found outliers. often interesting important\ndata points.model regular data, perform clustering regular data points\nwithout outliers. Note data needs rescaled outlier\nremoved. remove scaling issues caused outlier.may repeat identification, removal rescaling steps several\ntimes find outliers different scale.many outlier removal strategies available. See, e.g.,\npackage outliers.","code":"\nruspini_outlier_scaled <- ruspini_outlier |> scale_numeric()\n\nlof <- lof(ruspini_outlier_scaled, minPts= 10)\nlof\n##  [1]  0.9286  1.3222  0.9521  1.1313  0.9664  1.0068  1.0342\n##  [8]  0.9541  1.3508  0.9915  1.0356  0.9313  0.9289  1.5467\n## [15]  1.0078  1.0820  1.1303  1.1708  0.9082  1.0356  1.1593\n## [22]  0.9838  1.0139  0.9313  1.0623  1.1230  0.9915  1.8401\n## [29]  1.6549  0.9828  1.1291  0.9570  1.0128  1.0483  0.9200\n## [36]  1.0299  1.1362  1.0523  0.9999  1.1179  1.0078  1.0289\n## [43]  1.2959  1.3537  1.0122  1.0080  1.0097  1.1418  1.0781\n## [50]  1.3697  1.2170  1.2333  0.9845  1.0081  1.0028  0.9289\n## [57]  1.0423  0.9915  0.9200  1.1800  1.0938  0.9468  1.0533\n## [64]  1.0159  0.9451  0.9567  1.0003  0.9667  1.2781  1.0128\n## [71]  1.0785  1.1230  1.8383  0.9626  0.9935 55.3714\nruspini_outlier_scaled  <- ruspini_outlier_scaled  |> add_column(lof = lof)\n\nggplot(ruspini_outlier_scaled, \n       aes(x, y, color = lof)) +\n  geom_point() + \n  scale_color_gradient(low = \"gray\", high = \"red\")\nggplot(tibble(index = seq_len(length(lof)), lof = sort(lof)), \n       aes(index, lof)) +\n  geom_line()\nggplot(ruspini_outlier_scaled |> add_column(outlier = lof >= 2), \n       aes(x, y, color = outlier)) +\n  geom_point()\nruspini_scaled_clean <- ruspini_outlier_scaled |> \n  filter(lof < 2) |> \n  select(x, y) |> \n  scale_numeric()\n\nkm <- kmeans(ruspini_scaled_clean, centers = 4, nstart = 10)\nruspini_scaled_clean_km <- ruspini_scaled_clean|>\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_clean_km, aes(x = x, y = y, color = cluster)) + \n  geom_point() +\n  geom_point(data = centroids, \n             aes(x = x, y = y, color = cluster), \n             shape = 3, size = 10)"},{"path":"cluster-analysis.html","id":"exercises-4","chapter":"7 Cluster Analysis","heading":"7.9 Exercises*","text":"use Palmer penguin data exercises.Create R markdown file code discussion following .features use clustering? missing values?\nDiscuss answers. need scale data clustering? ?distance measure use reflect similarities penguins?\nSee Measures Similarity Dissimilarity Chapter 2.Apply k-means clustering. Use appropriate method determine number\nclusters. Compare clustering using unscaled data \nscaled data. difference? Visualize describe results.Apply hierarchical clustering.\nCreate dendrogram discuss means.Apply DBSCAN. choose parameters? well work?","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm\n##   <chr>   <chr>              <dbl>         <dbl>\n## 1 Adelie  Torgersen           39.1          18.7\n## 2 Adelie  Torgersen           39.5          17.4\n## 3 Adelie  Torgersen           40.3          18  \n## 4 Adelie  Torgersen           NA            NA  \n## 5 Adelie  Torgersen           36.7          19.3\n## 6 Adelie  Torgersen           39.3          20.6\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"data-exploration-and-visualization.html","id":"data-exploration-and-visualization","chapter":"A Data Exploration and Visualization","heading":"A Data Exploration and Visualization","text":"following code covers important part \ndata exploration. space reasons, chapter moved \nprinted textbook \nData Exploration Web Chapter.","code":""},{"path":"data-exploration-and-visualization.html","id":"packages-used-in-this-chapter-6","chapter":"A Data Exploration and Visualization","heading":"Packages Used in this Chapter","text":"packages used chapter :arules (Hahsler et al. 2025)GGally (Schloerke et al. 2025)ggcorrplot (Kassambara 2023)hexbin (Carr et al. 2024)palmerpenguins (Horst, Hill, Gorman 2022)plotly (Sievert et al. 2025)seriation (Hahsler, Buchta, Hornik 2025)tidyverse (Wickham 2023b)use iris dataset.","code":"\npkgs <- c(\"arules\", \"GGally\", \n          \"ggcorrplot\", \"hexbin\", \"palmerpenguins\", \"plotly\", \"seriation\", \"tidyverse\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)\nlibrary(tidyverse)\ndata(iris)\niris <- as_tibble(iris)\niris\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # ℹ 140 more rows"},{"path":"data-exploration-and-visualization.html","id":"exploring-data","chapter":"A Data Exploration and Visualization","heading":"A.1 Exploring Data","text":"","code":""},{"path":"data-exploration-and-visualization.html","id":"basic-statistics","chapter":"A Data Exploration and Visualization","heading":"A.1.1 Basic statistics","text":"Get summary statistics (using base R)Get mean standard deviation sepal length.Data missing values result statistics NA. Adding \nparameter na.rm = TRUE can used statistics functions \nignore missing values.Outliers typically smallest largest values feature.\nmake mean robust outliers, can trim 10% \nobservations end distribution.Sepal length outliers, trimmed mean almost\nidentical.calculate summary set features (e.g., numeric\nfeatures), tidyverse provides across((.numeric), fun).median absolute deviation (MAD) another measure dispersion.","code":"\nsummary(iris)\n##   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3  \n##  Median :5.80   Median :3.00   Median :4.35   Median :1.3  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \niris |> \n  summarize(avg_Sepal.Length = mean(Sepal.Length), \n            sd_Sepal.Length = sd(Sepal.Length))\n## # A tibble: 1 × 2\n##   avg_Sepal.Length sd_Sepal.Length\n##              <dbl>           <dbl>\n## 1             5.84           0.828\nmean(c(1, 2, NA, 3, 4, 5))\n## [1] NA\nmean(c(1, 2, NA, 3, 4, 5),  na.rm = TRUE)\n## [1] 3\niris |>\n  summarize(\n    avg_Sepal.Length = mean(Sepal.Length),\n    trimmed_avg_Sepal.Length = mean(Sepal.Length, trim = .1)\n  )\n## # A tibble: 1 × 2\n##   avg_Sepal.Length trimmed_avg_Sepal.Length\n##              <dbl>                    <dbl>\n## 1             5.84                     5.81\niris |> summarize(across(where(is.numeric), mean))\n## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         5.84        3.06         3.76        1.20\niris |> summarize(across(where(is.numeric), sd))\n## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1        0.828       0.436         1.77       0.762\niris |> summarize(across(where(is.numeric), \n            list(min = min, \n                 median = median, \n                 max = max)))\n## # A tibble: 1 × 12\n##   Sepal.Length_min Sepal.Length_median Sepal.Length_max\n##              <dbl>               <dbl>            <dbl>\n## 1              4.3                 5.8              7.9\n## # ℹ 9 more variables: Sepal.Width_min <dbl>,\n## #   Sepal.Width_median <dbl>, Sepal.Width_max <dbl>,\n## #   Petal.Length_min <dbl>, Petal.Length_median <dbl>,\n## #   Petal.Length_max <dbl>, Petal.Width_min <dbl>,\n## #   Petal.Width_median <dbl>, Petal.Width_max <dbl>\niris |> summarize(across(where(is.numeric), mad))\n## # A tibble: 1 × 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         1.04       0.445         1.85        1.04"},{"path":"data-exploration-and-visualization.html","id":"grouped-operations-and-calculations","chapter":"A Data Exploration and Visualization","heading":"A.1.2 Grouped Operations and Calculations","text":"can use nominal feature form groups calculate\ngroup-wise statistics continuous features. often use\ngroup-wise averages see differ groups.see species Virginica highest average , \nSepal.Width.statistical difference groups can tested using ANOVA\n(analysis \nvariance).summary shows significant difference \nSepal.Length groups. TukeyHDS evaluates differences\npairs groups. case, significantly different.\ndata contains two groups, t.test can used.","code":"\niris |> \n  group_by(Species) |> \n  summarize(across(Sepal.Length, mean))\n## # A tibble: 3 × 2\n##   Species    Sepal.Length\n##   <fct>             <dbl>\n## 1 setosa             5.01\n## 2 versicolor         5.94\n## 3 virginica          6.59\niris |> \n  group_by(Species) |> \n  summarize(across(where(is.numeric), mean))\n## # A tibble: 3 × 5\n##   Species  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <fct>           <dbl>       <dbl>        <dbl>       <dbl>\n## 1 setosa           5.01        3.43         1.46       0.246\n## 2 versico…         5.94        2.77         4.26       1.33 \n## 3 virgini…         6.59        2.97         5.55       2.03\nres.aov <- aov(Sepal.Length ~ Species, data = iris)\nsummary(res.aov)\n##              Df Sum Sq Mean Sq F value Pr(>F)    \n## Species       2   63.2   31.61     119 <2e-16 ***\n## Residuals   147   39.0    0.27                   \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(res.aov)\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = Sepal.Length ~ Species, data = iris)\n## \n## $Species\n##                       diff    lwr    upr p adj\n## versicolor-setosa    0.930 0.6862 1.1738     0\n## virginica-setosa     1.582 1.3382 1.8258     0\n## virginica-versicolor 0.652 0.4082 0.8958     0"},{"path":"data-exploration-and-visualization.html","id":"tabulate-data","chapter":"A Data Exploration and Visualization","heading":"A.1.3 Tabulate data","text":"can count number flowers species.base R, can also done using count(iris$Species).following examples, discretize data using cut.Cross tabulation used find two discrete features \nrelated.table contains number rows contain combination \nvalues (e.g., number flowers short Sepal.Length \nspecies Setosa 47). cells large counts\nothers low counts, might \nrelationship. iris data, see species Setosa mostly \nshort Sepal.Length, Versicolor Virginica longer sepals.Creating cross table tidyverse little involved uses\npivot operations grouping.can use statistical test determine significant\nrelationship two features. Pearson’s chi-squared\ntest independence\nperformed null hypothesis joint distribution \ncell counts 2-dimensional contingency table product \nrow column marginals. null hypothesis h0 independence \nrows columns.small p-value indicates null hypothesis independence\nneeds rejected. small counts (cells counts <5),\nFisher’s exact\ntest better.","code":"\niris |> \n  group_by(Species) |> \n  summarize(n())\n## # A tibble: 3 × 2\n##   Species    `n()`\n##   <fct>      <int>\n## 1 setosa        50\n## 2 versicolor    50\n## 3 virginica     50\niris_ord <- iris |> \n  mutate(across(where(is.numeric),  \n    function(x) cut(x, 3, labels = c(\"short\", \"medium\", \"long\"), \n                    ordered = TRUE)))\n\niris_ord\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##    <ord>        <ord>       <ord>        <ord>       <fct>  \n##  1 short        medium      short        short       setosa \n##  2 short        medium      short        short       setosa \n##  3 short        medium      short        short       setosa \n##  4 short        medium      short        short       setosa \n##  5 short        medium      short        short       setosa \n##  6 short        long        short        short       setosa \n##  7 short        medium      short        short       setosa \n##  8 short        medium      short        short       setosa \n##  9 short        medium      short        short       setosa \n## 10 short        medium      short        short       setosa \n## # ℹ 140 more rows\nsummary(iris_ord)\n##  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##  short :59    short :47   short :50    short :50  \n##  medium:71    medium:88   medium:54    medium:54  \n##  long  :20    long  :15   long  :46    long  :46  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50\ntbl <- iris_ord |> \n  select(Sepal.Length, Species) |> \n  table()\ntbl\n##             Species\n## Sepal.Length setosa versicolor virginica\n##       short      47         11         1\n##       medium      3         36        32\n##       long        0          3        17\niris_ord |>\n  select(Species, Sepal.Length) |>\n### Relationship Between Nominal and Ordinal Features\n  pivot_longer(cols = Sepal.Length) |>\n  group_by(Species, value) |> \n  count() |> \n  ungroup() |>\n  pivot_wider(names_from = Species, values_from = n)\n## # A tibble: 3 × 4\n##   value  setosa versicolor virginica\n##   <ord>   <int>      <int>     <int>\n## 1 short      47         11         1\n## 2 medium      3         36        32\n## 3 long       NA          3        17\ntbl |> \n  chisq.test()\n## \n##  Pearson's Chi-squared test\n## \n## data:  tbl\n## X-squared = 112, df = 4, p-value <2e-16\nfisher.test(tbl)\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tbl\n## p-value <2e-16\n## alternative hypothesis: two.sided"},{"path":"data-exploration-and-visualization.html","id":"percentiles-quantiles","chapter":"A Data Exploration and Visualization","heading":"A.1.4 Percentiles (Quantiles)","text":"Quantiles cutting points\ndividing range probability distribution continuous\nintervals equal probability. example, median \nempirical 50% quantile dividing observations 50% \nobservations smaller median 50% \nlarger median.default quartiles calculated. 25% typically called Q1, 50% \ncalled Q2 median 75% called Q3.interquartile range measure variability robust\noutliers. defined length Q3 - Q2 covers 50%\ndata middle.","code":"\niris |> \n  pull(Petal.Length) |> \n  quantile()\n##   0%  25%  50%  75% 100% \n## 1.00 1.60 4.35 5.10 6.90\niris |> \n  summarize(IQR = \n  quantile(Petal.Length, probs = 0.75) - \n    quantile(Petal.Length, probs = 0.25))\n## # A tibble: 1 × 1\n##     IQR\n##   <dbl>\n## 1   3.5"},{"path":"data-exploration-and-visualization.html","id":"correlation","chapter":"A Data Exploration and Visualization","heading":"A.1.5 Correlation","text":"","code":""},{"path":"data-exploration-and-visualization.html","id":"pearson-correlation","chapter":"A Data Exploration and Visualization","heading":"A.1.5.1 Pearson Correlation","text":"Correlation can used ratio/interval scaled features. typically\nthink Pearson correlation\ncoefficient\nfeatures (columns).cor calculates correlation matrix pairwise correlations \nfeatures. Correlation matrices symmetric, different \ndistances, whole matrix stored.correlation Petal.Length Petal.Width can visualized\nusing scatter plot.geom_smooth adds regression line fitting linear model (lm).\npoints close line indicating strong linear dependence\n(.e., high correlation).can calculate individual correlations specifying two vectors.Note: lets use columns using just names \n(iris, cor(Petal.Length, Petal.Width)) \ncor(iris$Petal.Length, iris$Petal.Width).Finally, can test correlation significantly different \nzero.small p-value (less 0.05) indicates observed correlation\nsignificantly different zero. can also seen fact\n95% confidence interval span zero.Sepal.Length Sepal.Width show little correlation:","code":"\ncc <- iris |> \n  select(-Species) |> \n  cor()\ncc\n##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length       1.0000     -0.1176       0.8718\n## Sepal.Width       -0.1176      1.0000      -0.4284\n## Petal.Length       0.8718     -0.4284       1.0000\n## Petal.Width        0.8179     -0.3661       0.9629\n##              Petal.Width\n## Sepal.Length      0.8179\n## Sepal.Width      -0.3661\n## Petal.Length      0.9629\n## Petal.Width       1.0000\nggplot(iris, aes(Petal.Length, Petal.Width)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n## `geom_smooth()` using formula = 'y ~ x'\nwith(iris, cor(Petal.Length, Petal.Width))\n## [1] 0.9629\nwith(iris, cor.test(Petal.Length, Petal.Width))\n## \n##  Pearson's product-moment correlation\n## \n## data:  Petal.Length and Petal.Width\n## t = 43, df = 148, p-value <2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.9491 0.9730\n## sample estimates:\n##    cor \n## 0.9629\nggplot(iris, aes(Sepal.Length, Sepal.Width)) + \n  geom_point() +   \n  geom_smooth(method = \"lm\") \n## `geom_smooth()` using formula = 'y ~ x'\nwith(iris, cor(Sepal.Length, Sepal.Width)) \n## [1] -0.1176\nwith(iris, cor.test(Sepal.Length, Sepal.Width))\n## \n##  Pearson's product-moment correlation\n## \n## data:  Sepal.Length and Sepal.Width\n## t = -1.4, df = 148, p-value = 0.2\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.27269  0.04351\n## sample estimates:\n##     cor \n## -0.1176"},{"path":"data-exploration-and-visualization.html","id":"rank-correlation","chapter":"A Data Exploration and Visualization","heading":"A.1.5.2 Rank Correlation","text":"Rank correlation used ordinal features correlation \nlinear. show , first convert continuous features \nIris dataset ordered factors (ordinal) three levels using\nfunction cut.Two measures rank correlation Kendall’s Tau Spearman’s Rho.Kendall’s Tau Rank Correlation\nCoefficient\nmeasures agreement two rankings (.e., ordinal features).Note: use xtfrm transform ordered factors \nranks, .e., numbers representing order.Spearman’s\nRho\nequal Pearson correlation rank values two\nfeatures.Spearman’s Rho much faster compute large datasets \nKendall’s Tau.Comparing rank correlation results Pearson correlation \noriginal data shows similar. indicates \ndiscretizing data result loss much information.","code":"\niris_ord <- iris |> \n  mutate(across(where(is.numeric), \n    function(x) cut(x, 3, \n                    labels = c(\"short\", \"medium\", \"long\"), \n                    ordered = TRUE)))\n\niris_ord\n## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##    <ord>        <ord>       <ord>        <ord>       <fct>  \n##  1 short        medium      short        short       setosa \n##  2 short        medium      short        short       setosa \n##  3 short        medium      short        short       setosa \n##  4 short        medium      short        short       setosa \n##  5 short        medium      short        short       setosa \n##  6 short        long        short        short       setosa \n##  7 short        medium      short        short       setosa \n##  8 short        medium      short        short       setosa \n##  9 short        medium      short        short       setosa \n## 10 short        medium      short        short       setosa \n## # ℹ 140 more rows\nsummary(iris_ord)\n##  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##  short :59    short :47   short :50    short :50  \n##  medium:71    medium:88   medium:54    medium:54  \n##  long  :20    long  :15   long  :46    long  :46  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50\niris_ord |> \n  pull(Sepal.Length)\n##   [1] short  short  short  short  short  short  short \n##   [8] short  short  short  short  short  short  short \n##  [15] medium medium short  short  medium short  short \n##  [22] short  short  short  short  short  short  short \n##  [29] short  short  short  short  short  short  short \n##  [36] short  short  short  short  short  short  short \n##  [43] short  short  short  short  short  short  short \n##  [50] short  long   medium long   short  medium medium\n##  [57] medium short  medium short  short  medium medium\n##  [64] medium medium medium medium medium medium medium\n##  [71] medium medium medium medium medium medium long  \n##  [78] medium medium medium short  short  medium medium\n##  [85] short  medium medium medium medium short  short \n##  [92] medium medium short  medium medium medium medium\n##  [99] short  medium medium medium long   medium medium\n## [106] long   short  long   medium long   medium medium\n## [113] long   medium medium medium medium long   long  \n## [120] medium long   medium long   medium medium long  \n## [127] medium medium medium long   long   long   medium\n## [134] medium medium long   medium medium medium long  \n## [141] medium long   medium long   medium medium medium\n## [148] medium medium medium\n## Levels: short < medium < long\niris_ord |> \n  select(-Species) |> \n  sapply(xtfrm) |> \n  cor(method = \"kendall\")\n##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length       1.0000     -0.1438       0.7419\n## Sepal.Width       -0.1438      1.0000      -0.3299\n## Petal.Length       0.7419     -0.3299       1.0000\n## Petal.Width        0.7295     -0.3154       0.9198\n##              Petal.Width\n## Sepal.Length      0.7295\n## Sepal.Width      -0.3154\n## Petal.Length      0.9198\n## Petal.Width       1.0000\niris_ord |> \n  select(-Species) |> \n  sapply(xtfrm) |> \n  cor(method = \"spearman\")\n##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length       1.0000     -0.1570       0.7938\n## Sepal.Width       -0.1570      1.0000      -0.3663\n## Petal.Length       0.7938     -0.3663       1.0000\n## Petal.Width        0.7843     -0.3517       0.9399\n##              Petal.Width\n## Sepal.Length      0.7843\n## Sepal.Width      -0.3517\n## Petal.Length      0.9399\n## Petal.Width       1.0000\niris |> \n  select(-Species) |> \n  cor()\n##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length       1.0000     -0.1176       0.8718\n## Sepal.Width       -0.1176      1.0000      -0.4284\n## Petal.Length       0.8718     -0.4284       1.0000\n## Petal.Width        0.8179     -0.3661       0.9629\n##              Petal.Width\n## Sepal.Length      0.8179\n## Sepal.Width      -0.3661\n## Petal.Length      0.9629\n## Petal.Width       1.0000"},{"path":"data-exploration-and-visualization.html","id":"density","chapter":"A Data Exploration and Visualization","heading":"A.1.6 Density","text":"Density estimation\nestimate probability density function\n(distribution) continuous variable observed data.Just plotting data using points helpful single\nfeature.","code":"\nggplot(iris, aes(x = Petal.Length, y = 0)) + geom_point()"},{"path":"data-exploration-and-visualization.html","id":"histograms","chapter":"A Data Exploration and Visualization","heading":"A.1.6.1 Histograms","text":"histograms shows \ndistribution counting many values fall within bin \nvisualizing counts bar chart. use geom_rug place marks\noriginal data points bottom histogram.Two-dimensional distributions can visualized using 2-d binning \nhexagonal bins.","code":"\nggplot(iris, aes(x = Petal.Length)) +\n  geom_histogram() +\n  geom_rug(alpha = 1/2)\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_bin2d(bins = 10) +\n  geom_jitter(color = \"red\")\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_hex(bins = 10) +\n  geom_jitter(color = \"red\")"},{"path":"data-exploration-and-visualization.html","id":"kernel-density-estimate-kde","chapter":"A Data Exploration and Visualization","heading":"A.1.6.2 Kernel Density Estimate (KDE)","text":"Kernel density\nestimation \nused estimate probability density function (distribution) \nfeature. works replacing value kernel function (often\nGaussian) adding . result estimated\nprobability density function looks like smoothed version \nhistogram. bandwidth (bw) kernel controls amount \nsmoothing.Kernel density estimates can also done two dimensions.","code":"\nggplot(iris, aes(Petal.Length)) +\n  geom_density(bw = .2) +\n  geom_rug(alpha = 1/2)\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_density_2d_filled() +\n  geom_jitter()"},{"path":"data-exploration-and-visualization.html","id":"visualization","chapter":"A Data Exploration and Visualization","heading":"A.2 Visualization","text":"Visualization uses several components convey information:Symbols: Circles, dots, lines, bars, …Position: Axes (labels units) origin (0/0) important.Length, Size Area: used faithfully represents\ninformation.Color: color overpower another. Limit 3 4 colors.Angle: Human eyes good comparing angles!components plot need convey information. E.g., use\ncolor just make colorful.\ngood visualizations show important patterns clearly.space reasons, chapter data exploration visualization\nmoved printed textbook can now found \nData Exploration Web Chapter.important R-base plot functions plot(), barplot(), hist(), pairs().\n, use mainly flexible ggplot2 library. gg ggplot2\nstands Grammar Graphics introduced Wilkinson (2005). \nmain idea every graph built basic components:data,coordinate system, andvisual marks representing data (geoms).ggplot2, components combined using + operator.Since typically use Cartesian coordinate system, ggplot uses \ndefault. geom_ function uses stat_ function calculate\nvisualizes. example, geom_bar uses stat_count create\nbar chart counting often value appears data (see\n? geom_bar). geom_point just uses stat \"identity\" display\npoints using coordinates .Additional components like main title, axis labels, different scales can\nalso added. great introduction can\nfound Section Data\nVisualization (Wickham, Çetinkaya-Rundel, Grolemund 2023),\nuseful RStudio’s Data Visualization Cheatsheet.Next, go basic visualizations used working data.","code":"\nggplot(data, mapping = aes(x = ..., y = ..., color = ...)) +\n  geom_point()"},{"path":"data-exploration-and-visualization.html","id":"histogram","chapter":"A Data Exploration and Visualization","heading":"A.2.1 Histogram","text":"Histograms show distribution single continuous feature.\nx-axis cuts variable discrete buckets y-axis\nshows many observations fall bucket.\nplot \nused inspect continuous variable :Detect outliers (.e., single observation far right plot)\nrecording mistakes (e.g., large frequency 0 may indicate missing\nvalues mistake converted 0).Understand distribution. single normal distribution \nseveral peaks can indicate multiple groups data.ggplot equivalent uses histogram geometry.plot shows just single normal distribution, least two peaks indicating \ndata may mixture two three different groups.know Iris data set contains three groups (types iris) \nvariable indicating group. group information can easily\nadded aesthetic histogram.bars appear stacked green blocks top pf blue blocks.\ndisplay three distributions behind , change position\nplacement make bars slightly translucent using alpha.","code":"\nhist(iris$Petal.Width)\nggplot(iris, aes(Petal.Width)) + geom_histogram(bins = 20)\nggplot(iris, aes(Petal.Width)) + \n         geom_histogram(bins = 20, aes(fill = Species))\nggplot(iris, aes(Petal.Width)) + \n         geom_histogram(bins = 20, aes(fill = Species), alpha = .5, position = 'identity')"},{"path":"data-exploration-and-visualization.html","id":"boxplot","chapter":"A Data Exploration and Visualization","heading":"A.2.2 Boxplot","text":"Boxplots used compare distribution feature \ndifferent groups. horizontal line middle boxes \ngroup-wise medians, boxes span interquartile range. whiskers\n(vertical lines) span typically 1.4 times interquartile range.\nPoints fall outside range typically outliers shown \ndots.Iris data outliers group Virginica shows \nsingle dot. can also see box Setosa much lower \ntwo, indicating group much smaller sepal length.group-wise medians can also calculated directly.compare distribution four features using ggplot boxplot,\nfirst transform data long format (.e., feature\nvalues combined single column).visualization useful features roughly \nrange. data can scaled first compare distributions.","code":"\nggplot(iris, aes(Species, Sepal.Length)) + \n  geom_boxplot()\niris |> group_by(Species) |> \n  summarize(across(where(is.numeric), median))\n## # A tibble: 3 × 5\n##   Species  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##   <fct>           <dbl>       <dbl>        <dbl>       <dbl>\n## 1 setosa            5           3.4         1.5          0.2\n## 2 versico…          5.9         2.8         4.35         1.3\n## 3 virgini…          6.5         3           5.55         2\nlibrary(tidyr)\niris_long <- iris |> \n  mutate(id = row_number()) |> \n  pivot_longer(1:4)\n\nggplot(iris_long, aes(name, value)) + \n  geom_boxplot() +\n  labs(y = \"Original value\")\nlibrary(tidyr)\n\n\nscale_numeric <- function(x) \n  x |> \n  mutate(across(where(is.numeric), \n                function(y) (y - mean(y, na.rm = TRUE)) / sd(y, na.rm = TRUE)))\n\niris_long_scaled <- iris |> \n  scale_numeric() |> \n  mutate(id = row_number()) |> pivot_longer(1:4)\n\nggplot(iris_long_scaled, aes(name, value)) + \n  geom_boxplot() +\n  labs(y = \"Scaled value\")"},{"path":"data-exploration-and-visualization.html","id":"scatter-plot-1","chapter":"A Data Exploration and Visualization","heading":"A.2.3 Scatter plot","text":"Scatter plots show relationship two continuous features.ggplot uses point geometry.see Setosa flowers well separated two flowers. \neasy identify simple threshold 2.5 cm petal length.\ntwo species closer mix somewhat together.\nOutliers single points far points. data set \ncontain outliers.can also identify correlation variables using plot. ,\nsee Sepal length tends increase petal length increases\nindicating positive linear correlation variables.\nrelationship can visualized adding regression line using geom_smooth \nlinear model method (“lm”). confidence interval\nregression also shown.\ncan suppressed using se = FALSE.can also perform group-wise linear regression adding color\naesthetic also geom_smooth.see within groups Versicolor Virginica much stronger\nlinear relationship group Setosa.can achieved using color aesthetic ggplot call,\napplies geoms.","code":"\nplot(iris$Petal.Length, iris$Sepal.Length, col = iris$Species)\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + \n  geom_point(aes(color = Species))\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + \n  geom_point(aes(color = Species)) +  \n  geom_smooth(method = \"lm\")\n## `geom_smooth()` using formula = 'y ~ x'\nggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + \n  geom_point(aes(color = Species)) +  \n  geom_smooth(method = \"lm\", aes(color = Species))\n## `geom_smooth()` using formula = 'y ~ x'"},{"path":"data-exploration-and-visualization.html","id":"scatter-plot-matrix","chapter":"A Data Exploration and Visualization","heading":"A.2.4 Scatter Plot Matrix","text":"scatter plot matrix show relationship pairs features\narranging panels matrix. First, lets look regular\nR-base plot.package GGally provides way sophisticated visualization.Additional plots\n(histograms, density estimates box plots) correlation\ncoefficients shown different panels. See Data Quality\nsection description interpret different panels.","code":"\npairs(iris, col = iris$Species)\nlibrary(\"GGally\")\nggpairs(iris,  aes(color = Species), progress = FALSE)\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## `binwidth`."},{"path":"data-exploration-and-visualization.html","id":"matrix-visualization-1","chapter":"A Data Exploration and Visualization","heading":"A.2.5 Matrix Visualization","text":"Matrix visualization shows values matrix using color scale.need long format tidyverse.Smaller values darker. Package seriation provides simpler\nplotting function.can scale features z-scores make better comparable.reveals red blue blocks. row flower flowers\nIris dataset sorted species. blue blocks top\n50 flowers show flowers smaller average \nSepal.Width red blocks show bottom 50 flowers \nlarger features.Often, reordering data matrices help visualization. reordering\ntechnique called seriation. Ir reorders rows columns place\nsimilar points closer together.see rows (flowers) organized blue red\nfeatures reordered move Sepal.Width way \nright different features.","code":"\niris_matrix <- iris |> select(-Species) |> as.matrix()\niris_long <- as_tibble(iris_matrix) |> \n  mutate(id = row_number()) |> \n  pivot_longer(1:4)\n\nhead(iris_long)\n## # A tibble: 6 × 3\n##      id name         value\n##   <int> <chr>        <dbl>\n## 1     1 Sepal.Length   5.1\n## 2     1 Sepal.Width    3.5\n## 3     1 Petal.Length   1.4\n## 4     1 Petal.Width    0.2\n## 5     2 Sepal.Length   4.9\n## 6     2 Sepal.Width    3\nggplot(iris_long, aes(x = name, y = id)) + \n  geom_tile(aes(fill = value))\nlibrary(seriation)\nggpimage(iris_matrix, prop = FALSE)\niris_scaled <- scale(iris_matrix)\nggpimage(iris_scaled, prop = FALSE)\nggpimage(iris_scaled, order = seriate(iris_scaled), prop = FALSE)"},{"path":"data-exploration-and-visualization.html","id":"correlation-matrix","chapter":"A Data Exploration and Visualization","heading":"A.2.6 Correlation Matrix","text":"correlation matrix contains correlation features.Package ggcorrplot provides ggplot-based visualization correlation matrices.Package seriation provides method reorder variables \ncorrelation matrix groups highly correlated variables become\neasier see.assume flowers grow, measurements increase.\nvisualization shows sepal length, petal length, petal width \nhighly correlated .\nInterestingly, sepal width negative correlation \nvariables seems contradict initial assumption. However, calculating\ncorrelation matrices group independently show \njust artifact resulting group Setosa large sepal width.Correlations can also calculates objects transposing \ndata matrix.Object--object correlations can used measure similarity.\ndark red blocks indicate different species.","code":"\ncm1 <- iris |> \n  select(-Species) |> \n  as.matrix() |> \n  cor()\ncm1\n##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length       1.0000     -0.1176       0.8718\n## Sepal.Width       -0.1176      1.0000      -0.4284\n## Petal.Length       0.8718     -0.4284       1.0000\n## Petal.Width        0.8179     -0.3661       0.9629\n##              Petal.Width\n## Sepal.Length      0.8179\n## Sepal.Width      -0.3661\n## Petal.Length      0.9629\n## Petal.Width       1.0000\nlibrary(ggcorrplot)\nggcorrplot(cm1)\ncm1 |> permute(order = \"AOE\") |> ggcorrplot()\niris_by_group <- split(iris, f = iris$Species)\nlapply(iris_by_group, FUN = function(x) x |> \n         select(-Species) |> \n         as.matrix() |> \n         cor() |> \n         permute(order = \"AOE\") |> \n         ggcorrplot()\n)\n## $setosa## \n## $versicolor## \n## $virginica\ncm2 <- iris |> \n  select(-Species) |> \n  as.matrix() |> \n  t() |> \n  cor()\n\nggcorrplot(cm2)"},{"path":"data-exploration-and-visualization.html","id":"parallel-coordinates-plot","chapter":"A Data Exploration and Visualization","heading":"A.2.7 Parallel Coordinates Plot","text":"Parallel coordinate plots can visualize several features single\nplot. Lines connect values object (flower).plot can improved reordering variables place correlated\nfeatures next .","code":"\nlibrary(GGally)\nggparcoord(iris, columns = 1:4, groupColumn = 5)\no <- seriate(as.dist(1-cor(iris[,1:4])), method = \"BBURCG\")\nget_order(o)\n## Petal.Length  Petal.Width Sepal.Length  Sepal.Width \n##            3            4            1            2\nggparcoord(iris, \n           columns = as.integer(get_order(o)), \n           groupColumn = 5)"},{"path":"data-exploration-and-visualization.html","id":"star-plot","chapter":"A Data Exploration and Visualization","heading":"A.2.8 Star Plot","text":"Star plots type radar chart\nvisualize three quantitative variables represented axes starting \nplot’s origin. R-base offers simple star plot. plot \nfirst 5 flowers species.","code":"\nflowers_5 <- iris[c(1:5, 51:55, 101:105), ]\nflowers_5\n## # A tibble: 15 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          7           3.2          4.7         1.4 versic…\n##  7          6.4         3.2          4.5         1.5 versic…\n##  8          6.9         3.1          4.9         1.5 versic…\n##  9          5.5         2.3          4           1.3 versic…\n## 10          6.5         2.8          4.6         1.5 versic…\n## 11          6.3         3.3          6           2.5 virgin…\n## 12          5.8         2.7          5.1         1.9 virgin…\n## 13          7.1         3            5.9         2.1 virgin…\n## 14          6.3         2.9          5.6         1.8 virgin…\n## 15          6.5         3            5.8         2.2 virgin…\nstars(flowers_5[, 1:4], ncol = 5)"},{"path":"data-exploration-and-visualization.html","id":"more-visualizations","chapter":"A Data Exploration and Visualization","heading":"A.2.9 More Visualizations","text":"well organized collection visualizations code can found \nR Graph Gallery.","code":""},{"path":"data-exploration-and-visualization.html","id":"exercises-5","chapter":"A Data Exploration and Visualization","heading":"A.3 Exercises*","text":"R package palmerpenguins contains measurements penguin different\nspecies Palmer Archipelago, Antarctica. Install package.\nprovides CSV file can read following way:Create RStudio new R Markdown document.\nApply code sections chapter data set answer \nfollowing questions.Group penguins species, island sex. can find ?Create histograms boxplots continuous variable. Interpret \ndistributions.Create scatterplots scatterplot matrix. Can identify correlations?Create reordered correlation matrix visualization. visualizations show?Make sure markdown document contains now well formatted report.\nUse Knit button create HTML document.","code":"\nlibrary(\"palmerpenguins\")\npenguins <- read_csv(path_to_file(\"penguins.csv\"))\n## Rows: 344 Columns: 8\n## ── Column specification ────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): species, island, sex\n## dbl (5): bill_length_mm, bill_depth_mm, flipper_length_m...\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm\n##   <chr>   <chr>              <dbl>         <dbl>\n## 1 Adelie  Torgersen           39.1          18.7\n## 2 Adelie  Torgersen           39.5          17.4\n## 3 Adelie  Torgersen           40.3          18  \n## 4 Adelie  Torgersen           NA            NA  \n## 5 Adelie  Torgersen           36.7          19.3\n## 6 Adelie  Torgersen           39.3          20.6\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"appendix_regression.html","id":"appendix_regression","chapter":"B Regression","heading":"B Regression","text":"Regression important statistical method covered\nfreely available Appendix D\ndata mining textbook.extra chapter, introduce regression problem\nmultiple linear regression. addition,\nalternative models like regression trees regularized regression\ndiscussed.","code":""},{"path":"appendix_regression.html","id":"packages-used-in-this-chapter-7","chapter":"B Regression","heading":"Packages Used in this Chapter","text":"packages used chapter :lars (Hastie Efron 2022)nnet (Ripley 2025)rpart (Therneau Atkinson 2025)rpart.plot (Milborrow 2025)","code":"\npkgs <- c('lars', 'rpart', 'rpart.plot', 'nnet')\n  \npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"appendix_regression.html","id":"introduction-1","chapter":"B Regression","heading":"B.1 Introduction","text":"Recall classification predicts one small set discrete (.e., nominal) labels (e.g., yes ,\nsmall, medium large).\nRegression also supervised learning problem, goal \npredict value continuous value given set predictors.\nstart popular linear regression later discuss alternative\nregression methods.Linear regression models value dependent variable \\(y\\) (also called\nresponse) \nlinear function independent variables \\(X_1, X_2, ..., X_p\\)\n(also called regressors, predictors, exogenous variables, explanatory variables,\ncovariates).\nuse one explanatory variable, often call \nmultiple multivariate linear regression.\nlinear regression model :\n\\[\\hat{y}_i = f(\\mathbf{x}_i) = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i = \\beta_0 + \\sum_{j = 1}^p{\\left(\\beta_jx_{ij}\\right) + \\epsilon_i},\\]\\(\\beta_0\\) intercept, \\(\\boldsymbol{\\beta}\\) \\(p+1\\)-dimensional parameter\nvector learned data, \\(\\epsilon\\) error term\n(called residuals). often also written vector notation \n\\[\\hat{\\mathbf{y}} = \\mathbf{X} \\boldsymbol{\\beta} + \\epsilon,\\]\n\\(\\hat{\\mathbf{y}}\\)\n\\(\\boldsymbol{\\beta}\\) vectors \\(\\mathbf{X}\\) matrix\ncovariates (called design matrix).error often minimized regression problems squared error defined\n:\\[SE= \\sum_i (y_i - f(\\mathbf{x}_i))^2\\]parameter vector found minimizing squared error training data.Linear regression makes several assumptions checked:Linearity: linear relationship dependent\nindependent variables.Homoscedasticity: variance error (\\(\\epsilon\\))\nchange (increase) predicted value.Independence errors: Errors observations uncorrelated.multicollinearity predictors: Predictors \nperfectly correlated parameter vector identified. Note\nhighly correlated predictors lead unstable results \navoided using, e.g., variable selection.","code":""},{"path":"appendix_regression.html","id":"a-first-linear-regression-model","chapter":"B Regression","heading":"B.2 A First Linear Regression Model","text":"use Iris dataset try predict\nPetal.Width using variables. first\nload shuffle data since flowers dataset order species.Iris data clean, make data little messy adding random error\nvariable introduce useless, completely random feature.split data training test data. Since data shuffled, \neffectively perform holdout sampling. often done \nstatistical applications, use machine learning approach .Linear regression done R using lm() (linear model) function \npart R core package stats.\nLike modeling functions R, lm() uses formula interface. create formula\npredict Petal.Width variables Species.result model fitted \\(\\beta\\) coefficients. information can\ndisplayed using summary function.summary shows:coefficients significantly different 0. Petal.Length\nsignificant coefficient useless close 0.\nLook scatter plot matrix see case.R-squared (coefficient determination): Proportion (range \\([0,1]\\)) variability \ndependent variable explained model. better look \nadjusted R-square (adjusted number dependent variables). Typically,\nR-squared greater \\(0.7\\) considered good, just rule thumb \nrather use test set evaluation.Plotting model produces diagnostic plots (see plot.lm()). example,\ncheck error term mean 0 homoscedastic,\nresidual vs. predicted value scatter plot\nred line stays clode 0 \nlook like funnel increasing fitted values increase.\ncheck residuals approximately normally distributed,\nQ-Q plot close straight diagonal line.case, two plots look fine.","code":"\ndata(iris)\nset.seed(2000) # make the sampling reproducible\n\nx <- iris[sample(1:nrow(iris)),]\nplot(x, col=x$Species)\nx[,1] <- x[,1] + rnorm(nrow(x))\nx[,2] <- x[,2] + rnorm(nrow(x))\nx[,3] <- x[,3] + rnorm(nrow(x))\nx <- cbind(x[,-5], \n           useless = mean(x[,1]) + rnorm(nrow(x)), \n           Species = x[,5])\n\nplot(x, col=x$Species)\nsummary(x)\n##   Sepal.Length   Sepal.Width     Petal.Length   \n##  Min.   :2.68   Min.   :0.611   Min.   :-0.713  \n##  1st Qu.:5.05   1st Qu.:2.306   1st Qu.: 1.876  \n##  Median :5.92   Median :3.171   Median : 4.102  \n##  Mean   :5.85   Mean   :3.128   Mean   : 3.781  \n##  3rd Qu.:6.70   3rd Qu.:3.945   3rd Qu.: 5.546  \n##  Max.   :8.81   Max.   :5.975   Max.   : 7.708  \n##   Petal.Width     useless           Species  \n##  Min.   :0.1   Min.   :2.92   setosa    :50  \n##  1st Qu.:0.3   1st Qu.:5.23   versicolor:50  \n##  Median :1.3   Median :5.91   virginica :50  \n##  Mean   :1.2   Mean   :5.88                  \n##  3rd Qu.:1.8   3rd Qu.:6.56                  \n##  Max.   :2.5   Max.   :8.11\nhead(x)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 85         2.980       1.464        5.227         1.5\n## 104        5.096       3.044        5.187         1.8\n## 30         4.361       2.832        1.861         0.2\n## 53         8.125       2.406        5.526         1.5\n## 143        6.372       1.565        6.147         1.9\n## 142        6.526       3.697        5.708         2.3\n##     useless    Species\n## 85    5.712 versicolor\n## 104   6.569  virginica\n## 30    4.299     setosa\n## 53    6.124 versicolor\n## 143   6.553  virginica\n## 142   5.222  virginica\ntrain <- x[1:100,]\ntest <- x[101:150,]\nmodel1 <- lm(Petal.Width ~ Sepal.Length\n            + Sepal.Width + Petal.Length + useless,\n            data = train)\nmodel1\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + \n##     useless, data = train)\n## \n## Coefficients:\n##  (Intercept)  Sepal.Length   Sepal.Width  Petal.Length  \n##     -0.20589       0.01957       0.03406       0.30814  \n##      useless  \n##      0.00392\nsummary(model1)\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + \n##     useless, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0495 -0.2033  0.0074  0.2038  0.8564 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -0.20589    0.28723   -0.72     0.48    \n## Sepal.Length  0.01957    0.03265    0.60     0.55    \n## Sepal.Width   0.03406    0.03549    0.96     0.34    \n## Petal.Length  0.30814    0.01819   16.94   <2e-16 ***\n## useless       0.00392    0.03558    0.11     0.91    \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.366 on 95 degrees of freedom\n## Multiple R-squared:  0.778,  Adjusted R-squared:  0.769 \n## F-statistic: 83.4 on 4 and 95 DF,  p-value: <2e-16\nplot(model1, which = 1:2)"},{"path":"appendix_regression.html","id":"comparing-nested-models","chapter":"B Regression","heading":"B.3 Comparing Nested Models","text":"perform model selection compare several linear models.\nNested means models use subset set features.\ncreate simpler model removing feature useless model .can remove intercept adding -1 formula.simple model.need statistical test compare nested models. appropriate test called\nANOVA (analysis variance)\ngeneralization t-test check treatments\n(.e., models) effect.\nImportant note: works nested models. Models nested one model contains predictors model.Models 1 significantly better model 2. Model 2 significantly better model 3. Model 3 significantly better model 4! Use model 4, simplest model.\nSee anova.lm() manual page ANOVA linear models.","code":"\nmodel2 <- lm(Petal.Width ~ Sepal.Length + \n               Sepal.Width + Petal.Length,\n             data = train)\nsummary(model2)\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length, \n##     data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0440 -0.2024  0.0099  0.1998  0.8513 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -0.1842     0.2076   -0.89     0.38    \n## Sepal.Length   0.0199     0.0323    0.62     0.54    \n## Sepal.Width    0.0339     0.0353    0.96     0.34    \n## Petal.Length   0.3080     0.0180   17.07   <2e-16 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.365 on 96 degrees of freedom\n## Multiple R-squared:  0.778,  Adjusted R-squared:  0.771 \n## F-statistic:  112 on 3 and 96 DF,  p-value: <2e-16\nmodel3 <- lm(Petal.Width ~ Sepal.Length + \n               Sepal.Width + Petal.Length - 1,\n             data = train)\nsummary(model3)\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length - \n##     1, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0310 -0.1961 -0.0051  0.2264  0.8246 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## Sepal.Length -0.00168    0.02122   -0.08     0.94    \n## Sepal.Width   0.01859    0.03073    0.61     0.55    \n## Petal.Length  0.30666    0.01796   17.07   <2e-16 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.364 on 97 degrees of freedom\n## Multiple R-squared:  0.938,  Adjusted R-squared:  0.936 \n## F-statistic:  486 on 3 and 97 DF,  p-value: <2e-16\nmodel4 <- lm(Petal.Width ~ Petal.Length -1,\n             data = train)\nsummary(model4)\n## \n## Call:\n## lm(formula = Petal.Width ~ Petal.Length - 1, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.9774 -0.1957  0.0078  0.2536  0.8455 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## Petal.Length  0.31586    0.00822    38.4   <2e-16 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.362 on 99 degrees of freedom\n## Multiple R-squared:  0.937,  Adjusted R-squared:  0.936 \n## F-statistic: 1.47e+03 on 1 and 99 DF,  p-value: <2e-16\nanova(model1, model2, model3, model4)\n## Analysis of Variance Table\n## \n## Model 1: Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + useless\n## Model 2: Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length\n## Model 3: Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length - 1\n## Model 4: Petal.Width ~ Petal.Length - 1\n##   Res.Df  RSS Df Sum of Sq    F Pr(>F)\n## 1     95 12.8                         \n## 2     96 12.8 -1   -0.0016 0.01   0.91\n## 3     97 12.9 -1   -0.1046 0.78   0.38\n## 4     99 13.0 -2   -0.1010 0.38   0.69"},{"path":"appendix_regression.html","id":"stepwise-variable-selection","chapter":"B Regression","heading":"B.4 Stepwise Variable Selection","text":"Stepwise variable section performs backward (forward) model selection linear models\nuses smallest AIC (Akaike information criterion) decide\nvariable remove stop.table represents one step shows AIC variable removed \none smallest AIC (first table useless) removed. stops\nsmall model uses Petal.Length.","code":"\ns1 <- step(lm(Petal.Width ~ . -Species, data = train))\n## Start:  AIC=-195.9\n## Petal.Width ~ (Sepal.Length + Sepal.Width + Petal.Length + useless + \n##     Species) - Species\n## \n##                Df Sum of Sq  RSS    AIC\n## - useless       1       0.0 12.8 -197.9\n## - Sepal.Length  1       0.0 12.8 -197.5\n## - Sepal.Width   1       0.1 12.9 -196.9\n## <none>                      12.8 -195.9\n## - Petal.Length  1      38.6 51.3  -58.7\n## \n## Step:  AIC=-197.9\n## Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length\n## \n##                Df Sum of Sq  RSS    AIC\n## - Sepal.Length  1       0.1 12.8 -199.5\n## - Sepal.Width   1       0.1 12.9 -198.9\n## <none>                      12.8 -197.9\n## - Petal.Length  1      38.7 51.5  -60.4\n## \n## Step:  AIC=-199.5\n## Petal.Width ~ Sepal.Width + Petal.Length\n## \n##                Df Sum of Sq  RSS    AIC\n## - Sepal.Width   1       0.1 12.9 -200.4\n## <none>                      12.8 -199.5\n## - Petal.Length  1      44.7 57.5  -51.3\n## \n## Step:  AIC=-200.4\n## Petal.Width ~ Petal.Length\n## \n##                Df Sum of Sq  RSS    AIC\n## <none>                      12.9 -200.4\n## - Petal.Length  1      44.6 57.6  -53.2\nsummary(s1)\n## \n## Call:\n## lm(formula = Petal.Width ~ Petal.Length, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.9848 -0.1873  0.0048  0.2466  0.8343 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    0.0280     0.0743    0.38     0.71    \n## Petal.Length   0.3103     0.0169   18.38   <2e-16 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.364 on 98 degrees of freedom\n## Multiple R-squared:  0.775,  Adjusted R-squared:  0.773 \n## F-statistic:  338 on 1 and 98 DF,  p-value: <2e-16"},{"path":"appendix_regression.html","id":"modeling-with-interaction-terms","chapter":"B Regression","heading":"B.5 Modeling with Interaction Terms","text":"Linear regression models effect predictor separately\nusing \\(\\beta\\) coefficient.\ntwo variables important together?\ncalled interaction predictors modeled using\ninteraction terms.\nR’s formula() can use : *\nspecify interactions. linear regression, interaction\nmeans new predictor created multiplying two original\npredictors.can create model interaction terms Sepal.Length, Sepal.Width \nPetal.Length.interaction terms significant new model, ANOVA shows \nmodel 5 significantly better model 4","code":"\nmodel5 <- step(lm(Petal.Width ~ Sepal.Length * \n                    Sepal.Width * Petal.Length,\n             data = train))\n## Start:  AIC=-196.4\n## Petal.Width ~ Sepal.Length * Sepal.Width * Petal.Length\n## \n##                                         Df Sum of Sq  RSS\n## <none>                                               11.9\n## - Sepal.Length:Sepal.Width:Petal.Length  1     0.265 12.2\n##                                          AIC\n## <none>                                  -196\n## - Sepal.Length:Sepal.Width:Petal.Length -196\nsummary(model5)\n## \n## Call:\n## lm(formula = Petal.Width ~ Sepal.Length * Sepal.Width * Petal.Length, \n##     data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1064 -0.1882  0.0238  0.1767  0.8577 \n## \n## Coefficients:\n##                                       Estimate Std. Error\n## (Intercept)                            -2.4207     1.3357\n## Sepal.Length                            0.4484     0.2313\n## Sepal.Width                             0.5983     0.3845\n## Petal.Length                            0.7275     0.2863\n## Sepal.Length:Sepal.Width               -0.1115     0.0670\n## Sepal.Length:Petal.Length              -0.0833     0.0477\n## Sepal.Width:Petal.Length               -0.0941     0.0850\n## Sepal.Length:Sepal.Width:Petal.Length   0.0201     0.0141\n##                                       t value Pr(>|t|)  \n## (Intercept)                             -1.81    0.073 .\n## Sepal.Length                             1.94    0.056 .\n## Sepal.Width                              1.56    0.123  \n## Petal.Length                             2.54    0.013 *\n## Sepal.Length:Sepal.Width                -1.66    0.100 .\n## Sepal.Length:Petal.Length               -1.75    0.084 .\n## Sepal.Width:Petal.Length                -1.11    0.272  \n## Sepal.Length:Sepal.Width:Petal.Length    1.43    0.157  \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.36 on 92 degrees of freedom\n## Multiple R-squared:  0.792,  Adjusted R-squared:  0.777 \n## F-statistic: 50.2 on 7 and 92 DF,  p-value: <2e-16\nanova(model5, model4)\n## Analysis of Variance Table\n## \n## Model 1: Petal.Width ~ Sepal.Length * Sepal.Width * Petal.Length\n## Model 2: Petal.Width ~ Petal.Length - 1\n##   Res.Df  RSS Df Sum of Sq    F Pr(>F)\n## 1     92 11.9                         \n## 2     99 13.0 -7     -1.01 1.11   0.36"},{"path":"appendix_regression.html","id":"prediction","chapter":"B Regression","heading":"B.6 Prediction","text":"preform prediction held test set.used error measure regression \nRMSE\nroot-mean-square error.can also visualize quality prediction using simple scatter plot \npredicted vs. actual values.Perfect predictions red line, farther away,\nlarger error.","code":"\ntest[1:5,]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 128        8.017       1.541        3.515         1.8\n## 92         5.268       4.064        6.064         1.4\n## 50         5.461       4.161        1.117         0.2\n## 134        6.055       2.951        4.599         1.5\n## 8          4.900       5.096        1.086         0.2\n##     useless    Species\n## 128   6.110  virginica\n## 92    4.938 versicolor\n## 50    6.373     setosa\n## 134   5.595  virginica\n## 8     7.270     setosa\ntest[1:5,]$Petal.Width\n## [1] 1.8 1.4 0.2 1.5 0.2\npredict(model4, test[1:5,])\n##    128     92     50    134      8 \n## 1.1104 1.9155 0.3529 1.4526 0.3429\nRMSE <- function(predicted, true) mean((predicted-true)^2)^.5\n\nRMSE(predict(model4, test), test$Petal.Width)\n## [1] 0.3874\nplot(test[,\"Petal.Width\"], predict(model4, test),\n  xlim=c(0,3), ylim=c(0,3), \n  xlab = \"actual\", ylab = \"predicted\",\n  main = \"Petal.Width\")\nabline(0,1, col=\"red\")\ncor(test[,\"Petal.Width\"], predict(model4, test))\n## [1] 0.8636"},{"path":"appendix_regression.html","id":"using-nominal-variables","chapter":"B Regression","heading":"B.7 Using Nominal Variables","text":"Dummy variables also\ncalled one-hot encoding machine learning used factors\n(.e., levels translated individual 0-1 variable).\nfirst level factors automatically used reference \nlevels presented 0-1 dummy variables called contrasts.model.matrix() used internally create dummy variables design matrix\nregression created..Note dummy variable species Setosa, \nused reference (two dummy variables 0).\noften useful set reference level.\nsimple way use \nfunction relevel() change factor listed first.Let us perform model selection using AIC training data evaluate\nfinal model held test set estimate generalization error.see Species variable provides information improve regression model.","code":"\nlevels(train$Species)\n## [1] \"setosa\"     \"versicolor\" \"virginica\"\nhead(model.matrix(Petal.Width ~ ., data=train))\n##     (Intercept) Sepal.Length Sepal.Width Petal.Length\n## 85            1        2.980       1.464        5.227\n## 104           1        5.096       3.044        5.187\n## 30            1        4.361       2.832        1.861\n## 53            1        8.125       2.406        5.526\n## 143           1        6.372       1.565        6.147\n## 142           1        6.526       3.697        5.708\n##     useless Speciesversicolor Speciesvirginica\n## 85    5.712                 1                0\n## 104   6.569                 0                1\n## 30    4.299                 0                0\n## 53    6.124                 1                0\n## 143   6.553                 0                1\n## 142   5.222                 0                1\nmodel6 <- step(lm(Petal.Width ~ ., data=train))\n## Start:  AIC=-308.4\n## Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + useless + \n##     Species\n## \n##                Df Sum of Sq   RSS  AIC\n## - Sepal.Length  1      0.01  3.99 -310\n## - Sepal.Width   1      0.01  3.99 -310\n## - useless       1      0.02  4.00 -310\n## <none>                       3.98 -308\n## - Petal.Length  1      0.47  4.45 -299\n## - Species       2      8.78 12.76 -196\n## \n## Step:  AIC=-310.2\n## Petal.Width ~ Sepal.Width + Petal.Length + useless + Species\n## \n##                Df Sum of Sq   RSS  AIC\n## - Sepal.Width   1      0.01  4.00 -312\n## - useless       1      0.02  4.00 -312\n## <none>                       3.99 -310\n## - Petal.Length  1      0.49  4.48 -300\n## - Species       2      8.82 12.81 -198\n## \n## Step:  AIC=-311.9\n## Petal.Width ~ Petal.Length + useless + Species\n## \n##                Df Sum of Sq   RSS  AIC\n## - useless       1      0.02  4.02 -313\n## <none>                       4.00 -312\n## - Petal.Length  1      0.48  4.48 -302\n## - Species       2      8.95 12.95 -198\n## \n## Step:  AIC=-313.4\n## Petal.Width ~ Petal.Length + Species\n## \n##                Df Sum of Sq   RSS  AIC\n## <none>                       4.02 -313\n## - Petal.Length  1      0.50  4.52 -304\n## - Species       2      8.93 12.95 -200\nmodel6\n## \n## Call:\n## lm(formula = Petal.Width ~ Petal.Length + Species, data = train)\n## \n## Coefficients:\n##       (Intercept)       Petal.Length  Speciesversicolor  \n##            0.1597             0.0664             0.8938  \n##  Speciesvirginica  \n##            1.4903\nsummary(model6)\n## \n## Call:\n## lm(formula = Petal.Width ~ Petal.Length + Species, data = train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.7208 -0.1437  0.0005  0.1254  0.5460 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         0.1597     0.0441    3.62  0.00047 ***\n## Petal.Length        0.0664     0.0192    3.45  0.00084 ***\n## Speciesversicolor   0.8938     0.0746   11.98  < 2e-16 ***\n## Speciesvirginica    1.4903     0.1020   14.61  < 2e-16 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.205 on 96 degrees of freedom\n## Multiple R-squared:  0.93,   Adjusted R-squared:  0.928 \n## F-statistic:  427 on 3 and 96 DF,  p-value: <2e-16\nRMSE(predict(model6, test), test$Petal.Width)\n## [1] 0.1885\nplot(test[,\"Petal.Width\"], predict(model6, test),\n  xlim=c(0,3), ylim=c(0,3), \n  xlab = \"actual\", ylab = \"predicted\",\n  main = \"Petal.Width\")\nabline(0,1, col=\"red\")\ncor(test[,\"Petal.Width\"], predict(model6, test))\n## [1] 0.9696"},{"path":"appendix_regression.html","id":"alternative-regression-models","chapter":"B Regression","heading":"B.8 Alternative Regression Models","text":"","code":""},{"path":"appendix_regression.html","id":"regression-trees","chapter":"B Regression","heading":"B.8.1 Regression Trees","text":"Many models used classification can also perform regression.\nexample CART implemented rpart performs regression\nestimating value leaf note.\nRegression always performed rpart() response variable\nfactor().Let’s evaluate regression tree calculating RMSE.visualize quality.plot correlation coefficient indicate model good.\nplot see important property method predicts\nexactly value data falls leaf node.","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\n\nmodel7 <- rpart(Petal.Width ~ ., data = train,\n  control = rpart.control(cp = 0.01))\nmodel7\n## n= 100 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n## 1) root 100 57.5700 1.2190  \n##   2) Species=setosa 32  0.3797 0.2469 *\n##   3) Species=versicolor,virginica 68 12.7200 1.6760  \n##     6) Species=versicolor 35  1.5350 1.3310 *\n##     7) Species=virginica 33  2.6010 2.0420 *\nrpart.plot(model7)\npred <- predict(model7, test)\nRMSE(pred, test$Petal.Width)\n## [1] 0.182\nplot(test[,\"Petal.Width\"], pred,\n  xlim = c(0,3), ylim = c(0,3), \n  xlab = \"actual\", ylab = \"predicted\",\n  main = \"Petal.Width\")\nabline(0,1, col = \"red\")\ncor(test[,\"Petal.Width\"], pred)\n## [1] 0.9717"},{"path":"appendix_regression.html","id":"regularized-regression","chapter":"B Regression","heading":"B.8.2 Regularized Regression","text":"LASSO (least absolute shrinkage selection operator)\nuses L1 regularization reduce perform automatic variable selection.\nregularization adds penalty L1 norm parameter vector \\(\\boldsymbol{\\beta}\\) (.e., summ \nparameter values)\noptimization. Increasing weight penalty term (weight called \\(\\lambda\\))\nresults weights pushed 0 effectively reduces number \nparameters used regression.\nimplementation called\nelastic net\navailable function lars() package lars.create design matrix (dummy variables interaction terms).\nlm() automatically us, lars() implementation\nmanually.fitted model’s plot shows variables added (left right model).\ntext output shoes Petal.Length important variable added model step 1.\nSpeciesvirginica added .\ncreates sequence nested models one variable added time.\nselect best model Mallows’s Cp statistic\ncan used.variables selected \\(\\beta\\) coefficient 0.make predictions model, first convert test data\ndesign matrix dummy variables interaction terms.Now can compute predictions.prediction fit element. can calculate RMSE.visualize prediction.model shows good predictive power test set.","code":"\nlibrary(lars)\n## Loaded lars 1.3\nx <- model.matrix(~ . + Sepal.Length*Sepal.Width*Petal.Length ,\n  data = train[, -4])\nhead(x)\n##     (Intercept) Sepal.Length Sepal.Width Petal.Length\n## 85            1        2.980       1.464        5.227\n## 104           1        5.096       3.044        5.187\n## 30            1        4.361       2.832        1.861\n## 53            1        8.125       2.406        5.526\n## 143           1        6.372       1.565        6.147\n## 142           1        6.526       3.697        5.708\n##     useless Speciesversicolor Speciesvirginica\n## 85    5.712                 1                0\n## 104   6.569                 0                1\n## 30    4.299                 0                0\n## 53    6.124                 1                0\n## 143   6.553                 0                1\n## 142   5.222                 0                1\n##     Sepal.Length:Sepal.Width Sepal.Length:Petal.Length\n## 85                     4.362                    15.578\n## 104                   15.511                    26.431\n## 30                    12.349                     8.114\n## 53                    19.546                    44.900\n## 143                    9.972                    39.168\n## 142                   24.126                    37.253\n##     Sepal.Width:Petal.Length\n## 85                     7.650\n## 104                   15.787\n## 30                     5.269\n## 53                    13.294\n## 143                    9.620\n## 142                   21.103\n##     Sepal.Length:Sepal.Width:Petal.Length\n## 85                                  22.80\n## 104                                 80.45\n## 30                                  22.98\n## 53                                 108.01\n## 143                                 61.30\n## 142                                137.72\ny <- train[, 4]\nmodel_lars <- lars(x, y)\n\nplot(model_lars)\nmodel_lars\n## \n## Call:\n## lars(x = x, y = y)\n## R-squared: 0.933 \n## Sequence of LASSO moves:\n##      Petal.Length Speciesvirginica Speciesversicolor\n## Var             4                7                 6\n## Step            1                2                 3\n##      Sepal.Width:Petal.Length\n## Var                        10\n## Step                        4\n##      Sepal.Length:Sepal.Width:Petal.Length useless\n## Var                                     11       5\n## Step                                     5       6\n##      Sepal.Width Sepal.Length:Petal.Length Sepal.Length\n## Var            3                         9            2\n## Step           7                         8            9\n##      Sepal.Width:Petal.Length Sepal.Width:Petal.Length\n## Var                       -10                       10\n## Step                       10                       11\n##      Sepal.Length:Sepal.Width Sepal.Width Sepal.Width\n## Var                         8          -3           3\n## Step                       12          13          14\nplot(model_lars, plottype = \"Cp\")\nbest <- which.min(model_lars$Cp)\n\ncoef(model_lars, s = best)\n##                           (Intercept) \n##                             0.0000000 \n##                          Sepal.Length \n##                             0.0000000 \n##                           Sepal.Width \n##                             0.0000000 \n##                          Petal.Length \n##                             0.0603837 \n##                               useless \n##                            -0.0086551 \n##                     Speciesversicolor \n##                             0.8463786 \n##                      Speciesvirginica \n##                             1.4181850 \n##              Sepal.Length:Sepal.Width \n##                             0.0000000 \n##             Sepal.Length:Petal.Length \n##                             0.0000000 \n##              Sepal.Width:Petal.Length \n##                             0.0029688 \n## Sepal.Length:Sepal.Width:Petal.Length \n##                             0.0003151\nx_test <- model.matrix(~ . + Sepal.Length*Sepal.Width*Petal.Length,\n  data = test[, -4])\nhead(x_test)\n##     (Intercept) Sepal.Length Sepal.Width Petal.Length\n## 128           1        8.017       1.541        3.515\n## 92            1        5.268       4.064        6.064\n## 50            1        5.461       4.161        1.117\n## 134           1        6.055       2.951        4.599\n## 8             1        4.900       5.096        1.086\n## 58            1        5.413       4.728        5.990\n##     useless Speciesversicolor Speciesvirginica\n## 128   6.110                 0                1\n## 92    4.938                 1                0\n## 50    6.373                 0                0\n## 134   5.595                 0                1\n## 8     7.270                 0                0\n## 58    7.092                 1                0\n##     Sepal.Length:Sepal.Width Sepal.Length:Petal.Length\n## 128                    12.35                    28.182\n## 92                     21.41                    31.944\n## 50                     22.72                     6.102\n## 134                    17.87                    27.847\n## 8                      24.97                     5.319\n## 58                     25.59                    32.424\n##     Sepal.Width:Petal.Length\n## 128                    5.417\n## 92                    24.647\n## 50                     4.649\n## 134                   13.572\n## 8                      5.531\n## 58                    28.323\n##     Sepal.Length:Sepal.Width:Petal.Length\n## 128                                 43.43\n## 92                                 129.83\n## 50                                  25.39\n## 134                                 82.19\n## 8                                   27.10\n## 58                                 153.31\npredict(model_lars, x_test[1:5,], s = best)\n## $s\n## 6 \n## 7 \n## \n## $fraction\n##      6 \n## 0.4286 \n## \n## $mode\n## [1] \"step\"\n## \n## $fit\n##    128     92     50    134      8 \n## 1.8237 1.5003 0.2505 1.9300 0.2439\npred <- predict(model_lars, x_test, s = best)$fit\nRMSE(pred, test$Petal.Width)\n## [1] 0.1907\nplot(test[,\"Petal.Width\"], \n     pred,\n     xlim=c(0,3), ylim=c(0,3), \n     xlab = \"actual\", ylab = \"predicted\",\n     main = \"Petal.Width\")\nabline(0,1, col = \"red\")\ncor(test[,\"Petal.Width\"],\n    pred)\n## [1] 0.9686"},{"path":"appendix_regression.html","id":"anns","chapter":"B Regression","heading":"B.8.3 ANNs","text":"Regression can performed using artificial neural networks\ntypically linear final layer.\ncreate network using single hidden layer\n3 neurons (manually tuned hyper parameter) linear output layer (set via linout).visualize quality.Note: often necessary scale inputs ANN can learn effectively.\npopular scale inputs\nranges \\([0,1]\\) \\([-1,1]\\). ranges Iris dataset fine, \nneed scaling.","code":"\nlibrary(nnet)\n\nmodel_nnet <- nnet(Petal.Width ~ ., data = train, size = 3, linout = TRUE)\n## # weights:  25\n## initial  value 82.633473 \n## iter  10 value 14.094963\n## iter  20 value 3.767394\n## iter  30 value 3.372093\n## iter  40 value 3.264074\n## iter  50 value 3.224384\n## iter  60 value 3.201191\n## iter  70 value 3.182708\n## iter  80 value 3.117583\n## iter  90 value 3.087831\n## iter 100 value 3.009898\n## final  value 3.009898 \n## stopped after 100 iterations\nmodel_nnet\n## a 6-3-1 network with 25 weights\n## inputs: Sepal.Length Sepal.Width Petal.Length useless Speciesversicolor Speciesvirginica \n## output(s): Petal.Width \n## options were - linear output units\npred <- predict(model_nnet, test)\nRMSE(pred, test$Petal.Width)\n## [1] 0.2322\nplot(test[,\"Petal.Width\"], pred,\n  xlim = c(0,3), ylim = c(0,3), \n  xlab = \"actual\", ylab = \"predicted\",\n  main = \"Petal.Width\")\nabline(0,1, col = \"red\")\ncor(test[,\"Petal.Width\"], pred)\n##        [,1]\n## [1,] 0.9546"},{"path":"appendix_regression.html","id":"other-types-of-regression","chapter":"B Regression","heading":"B.8.4 Other Types of Regression","text":"Robust regression: robust violation assumptions like heteroscedasticity outliers\n(robustbase::roblm() robustbase::robglm)Generalized linear models (glm()). example logistic regression discussed next chapter.Nonlinear least squares (nlm()).","code":""},{"path":"appendix_regression.html","id":"exercises-6","chapter":"B Regression","heading":"B.9 Exercises","text":"use Palmer penguin data exercises.Create R markdown document performs following:Create linear regression model predict weight\npenguin (body_mass_g).Create linear regression model predict weight\npenguin (body_mass_g).high R-squared. mean.high R-squared. mean.variables significant, ?variables significant, ?Use stepwise variable selection remove unnecessary variables.Use stepwise variable selection remove unnecessary variables.Predict weight following new penguin:\n\nnew_penguin <- tibble(\n  species = factor(\"Adelie\", \n    levels = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")),\n  island = factor(\"Dream\", \n    levels = c(\"Biscoe\", \"Dream\", \"Torgersen\")),\n bill_length_mm = 39.8, \n bill_depth_mm = 19.1, \n flipper_length_mm = 184, \n body_mass_g = NA, \n sex = factor(\"male\", levels = c(\"female\", \"male\")), \n year = 2007\n) \nnew_penguin\n## # tibble: 1 × 8\n##   species island bill_length_mm bill_depth_mm\n##   <fct>   <fct>           <dbl>         <dbl>\n## 1 Adelie  Dream            39.8          19.1\n## # ℹ 4 variables: flipper_length_mm <dbl>,\n## #   body_mass_g <lgl>, sex <fct>, year <dbl>Predict weight following new penguin:Create regression tree. Look tree explain .\nuse regression tree predict weight penguin.Create regression tree. Look tree explain .\nuse regression tree predict weight penguin.","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm\n##   <chr>   <chr>              <dbl>         <dbl>\n## 1 Adelie  Torgersen           39.1          18.7\n## 2 Adelie  Torgersen           39.5          17.4\n## 3 Adelie  Torgersen           40.3          18  \n## 4 Adelie  Torgersen           NA            NA  \n## 5 Adelie  Torgersen           36.7          19.3\n## 6 Adelie  Torgersen           39.3          20.6\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <dbl>, sex <chr>, year <dbl>\nnew_penguin <- tibble(\n  species = factor(\"Adelie\", \n    levels = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")),\n  island = factor(\"Dream\", \n    levels = c(\"Biscoe\", \"Dream\", \"Torgersen\")),\n bill_length_mm = 39.8, \n bill_depth_mm = 19.1, \n flipper_length_mm = 184, \n body_mass_g = NA, \n sex = factor(\"male\", levels = c(\"female\", \"male\")), \n year = 2007\n) \nnew_penguin\n## # A tibble: 1 × 8\n##   species island bill_length_mm bill_depth_mm\n##   <fct>   <fct>           <dbl>         <dbl>\n## 1 Adelie  Dream            39.8          19.1\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <lgl>, sex <fct>, year <dbl>"},{"path":"appendix_logistic_regression.html","id":"appendix_logistic_regression","chapter":"C Logistic Regression","heading":"C Logistic Regression","text":"chapter introduces popular classification method\nlogistic regression detail. Logistic regression introduced\nalternative classification method Chapter 4 Introduction Data Mining.packages used chapter :caret (Kuhn 2024)glmnet (Friedman et al. 2025)","code":"\npkgs <- c(\"glmnet\", \"caret\")\n\npkgs_install <- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\nif(length(pkgs_install)) install.packages(pkgs_install)"},{"path":"appendix_logistic_regression.html","id":"introduction-2","chapter":"C Logistic Regression","heading":"C.1 Introduction","text":"Logistic regression contains word regression, actually \nstatistical classification\nmodel predict probability \\(p\\) binary outcome given set features.\npowerful classification model can fit quickly. one \nfirst classification models try new data.Logistic regression generalized linear model\nlogit link function binomial error distribution.\ncan\nthought linear regression \nlog odds ratio (logit)\nbinary outcome dependent variable:\\[logit(p) = ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\]logit function links probability p linear regression converting\nnumber probability range \\([0,1]\\) range \\([-\\infty,+\\infty]\\).figure shows actually inverse logit function.\ninverse logit function called logistic (sigmoid) function (\\(\\sigma(\\cdot)\\)) \noften used ML, especially artificial neural networks,\nsquash set real numbers \\([0,1]\\) interval.\nUsing inverse function, see probability outcome \\(p\\)\nmodeled logistic function linear regression:\\[ p = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...}}{1 +  e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...}} = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...)}} = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...)\\]\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,...)\\) parameter vector fitted using training data \nminimizing log loss (.e., cross-entropy loss),\nequation can used predict\nprobability \\(p\\) given new data point \\(\\mathbf{x} = (x_1, x_2, ...)\\). predicted \\(p > .5\\)\npredict event happens, otherwise predict happen.outcome binary therefore Bernoulli distribution. Since \nmultiple examples data draw several times distribution resulting\nBinomial distribution number successful events drawn. Logistic\nregression therefore uses logit link function link probability event\nlinear regression distribution family Binomial.","code":"\nlogit  <- function(p) log(p/(1-p))\np <- seq(0, 1, length.out = 100)\nplot(logit(p), p, type = \"l\")\nabline(h = 0.5, lty = 2)\nabline(v = 0, lty = 2)"},{"path":"appendix_logistic_regression.html","id":"data-preparation-1","chapter":"C Logistic Regression","heading":"C.2 Data Preparation","text":"load shuffle data. also add useless variable see logistic regression removes .create binary classification problem \nasking flower species Virginica .\ncreate new logical variable called virginica remove \nSpecies column.can visualize data using scatter plot matrix use color red \nvirginica == TRUE black flowers.","code":"\ndata(iris)\nset.seed(100) # for reproducability\n\nx <- iris[sample(1:nrow(iris)),]\nx <- cbind(x, useless = rnorm(nrow(x)))\nx$virginica <- x$Species == \"virginica\"\nx$Species <- NULL\nplot(x, col=x$virginica + 1)"},{"path":"appendix_logistic_regression.html","id":"a-first-logistic-regression-model","chapter":"C Logistic Regression","heading":"C.3 A first Logistic Regression Model","text":"Logistic regression generalized linear model (GLM) logit \nlink function binomial distribution. glm() function provided \nR core package stats installed R automatically loads\nR started.warning: glm.fit: fitted probabilities numerically 0 1 occurred means data possibly linearly separable.Check features significant?AIC (Akaike information criterion)\nmeasure good model . Smaller better. can used model selection.parameter estimates coefficients table log odds. * .\nindicate effect parameter significantly different 0.\nPositive numbers\nmean increasing variable increases predicted probability\nnegative numbers mean probability decreases. example,\nobserving larger Petal.Length increases predicted probability flower \nclass Virginica. effect significant can\nverify scatter plot . Petal.Length, red dots \nlarger values \nblack dots.","code":"\nmodel <- glm(virginica ~ .,\n  family = binomial(logit), data = x)\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\nmodel\n## \n## Call:  glm(formula = virginica ~ ., family = binomial(logit), data = x)\n## \n## Coefficients:\n##  (Intercept)  Sepal.Length   Sepal.Width  Petal.Length  \n##      -41.649        -2.531        -6.448         9.376  \n##  Petal.Width       useless  \n##       17.696         0.098  \n## \n## Degrees of Freedom: 149 Total (i.e. Null);  144 Residual\n## Null Deviance:       191 \n## Residual Deviance: 11.9  AIC: 23.9\nsummary(model)\n## \n## Call:\n## glm(formula = virginica ~ ., family = binomial(logit), data = x)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)  \n## (Intercept)   -41.649     26.556   -1.57    0.117  \n## Sepal.Length   -2.531      2.458   -1.03    0.303  \n## Sepal.Width    -6.448      4.794   -1.34    0.179  \n## Petal.Length    9.376      4.763    1.97    0.049 *\n## Petal.Width    17.696     10.632    1.66    0.096 .\n## useless         0.098      0.807    0.12    0.903  \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 190.954  on 149  degrees of freedom\n## Residual deviance:  11.884  on 144  degrees of freedom\n## AIC: 23.88\n## \n## Number of Fisher Scoring iterations: 12"},{"path":"appendix_logistic_regression.html","id":"stepwise-variable-selection-1","chapter":"C Logistic Regression","heading":"C.4 Stepwise Variable Selection","text":"two variables flagged significant. can remove insignificant\nvariables trying remove one variable time\nlong model significantly deteriorate (according AIC).\nvariable selection process done automatically step() function.estimates (\\(\\beta_0, \\beta_1,...\\) ) \nlog-odds can converted odds using \\(exp(\\beta)\\).\nnegative log-odds ratio means odds go increase \nvalue predictor. predictor \npositive log-odds ratio increases odds. case, odds \nlooking Virginica iris goes Sepal.Width increases \ntwo predictors.","code":"\nmodel2 <- step(model, data = x)\n## Start:  AIC=23.88\n## virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + \n##     useless\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n##                Df Deviance  AIC\n## - useless       1     11.9 21.9\n## - Sepal.Length  1     13.2 23.2\n## <none>                11.9 23.9\n## - Sepal.Width   1     14.8 24.8\n## - Petal.Width   1     22.4 32.4\n## - Petal.Length  1     25.9 35.9\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## \n## Step:  AIC=21.9\n## virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n##                Df Deviance  AIC\n## - Sepal.Length  1     13.3 21.3\n## <none>                11.9 21.9\n## - Sepal.Width   1     15.5 23.5\n## - Petal.Width   1     23.8 31.8\n## - Petal.Length  1     25.9 33.9\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## \n## Step:  AIC=21.27\n## virginica ~ Sepal.Width + Petal.Length + Petal.Width\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n## Warning: glm.fit: fitted probabilities numerically 0 or 1\n## occurred\n##                Df Deviance  AIC\n## <none>                13.3 21.3\n## - Sepal.Width   1     20.6 26.6\n## - Petal.Length  1     27.4 33.4\n## - Petal.Width   1     31.5 37.5\nsummary(model2)\n## \n## Call:\n## glm(formula = virginica ~ Sepal.Width + Petal.Length + Petal.Width, \n##     family = binomial(logit), data = x)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)  \n## (Intercept)    -50.53      23.99   -2.11    0.035 *\n## Sepal.Width     -8.38       4.76   -1.76    0.079 .\n## Petal.Length     7.87       3.84    2.05    0.040 *\n## Petal.Width     21.43      10.71    2.00    0.045 *\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 190.954  on 149  degrees of freedom\n## Residual deviance:  13.266  on 146  degrees of freedom\n## AIC: 21.27\n## \n## Number of Fisher Scoring iterations: 12"},{"path":"appendix_logistic_regression.html","id":"calculate-the-response","chapter":"C Logistic Regression","heading":"C.5 Calculate the Response","text":"Note: -sample testing data learned data\n. get generalization error estimate use test set \ncross-validation!response predicted probability flower species\nVirginica. probabilities first 10 flowers shown. \nhistogram predicted probabilities. color used show \nexamples true class Virginica.","code":"\npr <- predict(model2, x, type = \"response\")\nround(pr[1:10], 2)\n##  102  112    4   55   70   98  135    7   43  140 \n## 1.00 1.00 0.00 0.00 0.00 0.00 0.86 0.00 0.00 1.00\nhist(pr, breaks = 20, main = \"Predicted Probability vs. True Class\")\nhist(pr[x$virginica == TRUE], col = \"red\", breaks = 20, add = TRUE)"},{"path":"appendix_logistic_regression.html","id":"check-classification-performance","chapter":"C Logistic Regression","heading":"C.6 Check Classification Performance","text":"perform -sample evaluation training set. get estimate \ngeneralization error, calculate performance held test set.predicted class calculated checking predicted probability\nlarger .5.Now can create confusion table calculate accuracy.can also use caret’s advanced function caret::confusionMatrix(). code\nuses logical vectors. caret, need make sure ,\nreference predictions coded factor.see model performs well high accuracy kappa value.","code":"\npred <- pr > .5\ntbl <- table(predicted = pred, actual = x$virginica)\ntbl\n##          actual\n## predicted FALSE TRUE\n##     FALSE    98    1\n##     TRUE      2   49\nsum(diag(tbl))/sum(tbl)\n## [1] 0.98\ncaret::confusionMatrix(\n  reference = factor(x$virginica, labels = c(\"Yes\", \"No\"), levels = c(TRUE, FALSE)), \n  data = factor(pred, labels = c(\"Yes\", \"No\"), levels = c(TRUE, FALSE)))\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction Yes No\n##        Yes  49  2\n##        No    1 98\n##                                         \n##                Accuracy : 0.98          \n##                  95% CI : (0.943, 0.996)\n##     No Information Rate : 0.667         \n##     P-Value [Acc > NIR] : <2e-16        \n##                                         \n##                   Kappa : 0.955         \n##                                         \n##  Mcnemar's Test P-Value : 1             \n##                                         \n##             Sensitivity : 0.980         \n##             Specificity : 0.980         \n##          Pos Pred Value : 0.961         \n##          Neg Pred Value : 0.990         \n##              Prevalence : 0.333         \n##          Detection Rate : 0.327         \n##    Detection Prevalence : 0.340         \n##       Balanced Accuracy : 0.980         \n##                                         \n##        'Positive' Class : Yes           \n## "},{"path":"appendix_logistic_regression.html","id":"regularized-logistic-regression","chapter":"C Logistic Regression","heading":"C.7 Regularized Logistic Regression","text":"glmnet::glmnet() fits generalized linear models (including logistic regression)\nusing regularization via penalized maximum likelihood.\nregularization parameter \\(\\lambda\\) hyperparameter \nglmnet can use cross-validation find appropriate\nvalue. glmnet function interface, \nsupply matrix X vector responses y.several selection rules lambda, look \ncoefficients logistic regression using \nlambda gives regularized model cross-validated error within one standard error minimum cross-validated error.dot means 0. see predictors Sepal.Length \nuseless used prediction giving models similar \nstepwise variable selection .predict function provided. need specify\nregularization use want predict class\nlabel.Glmnet provides supports many types \ngeneralized linear models. Examples can found \narticle Introduction glmnet.","code":"\nlibrary(glmnet)\n## Loaded glmnet 4.1-10\nX <- as.matrix(x[, 1:5])\ny <- x$virginica\n\nfit <- cv.glmnet(X, y, family = \"binomial\")\nfit\n## \n## Call:  cv.glmnet(x = X, y = y, family = \"binomial\") \n## \n## Measure: Binomial Deviance \n## \n##      Lambda Index Measure     SE Nonzero\n## min 0.00164    59   0.126 0.0456       5\n## 1se 0.00664    44   0.167 0.0422       3\ncoef(fit, s = fit$lambda.1se)\n## 6 x 1 sparse Matrix of class \"dgCMatrix\"\n##              s=0.00664\n## (Intercept)    -16.961\n## Sepal.Length     .    \n## Sepal.Width     -1.766\n## Petal.Length     2.197\n## Petal.Width      6.820\n## useless          .\npredict(fit, newx = X[1:5,], s = fit$lambda.1se, type = \"class\")\n##     s=0.00664\n## 102 \"TRUE\"   \n## 112 \"TRUE\"   \n## 4   \"FALSE\"  \n## 55  \"FALSE\"  \n## 70  \"FALSE\""},{"path":"appendix_logistic_regression.html","id":"multinomial-logistic-regression-2","chapter":"C Logistic Regression","heading":"C.8 Multinomial Logistic Regression","text":"Regular logistic regression predicts one outcome binary event represented\ntwo classes. Extending model data two classes\ncalled multinomial logistic regression,\n(log-linear model).\npopular implementation uses simple artificial neural networks.\nRegular logistic regression equivalent single neuron \nsigmoid (.e., logistic) activation function optimized cross-entropy loss.\nmultinomial logistic regression, one neuron used class \nprobability distribution calculated softmax activation.\nextension implemented nnet::multinom().get \\(\\boldsymbol{\\beta}\\) vector weights two three classes.\nthird class used default class weights set 0. can\ninterpreted comparing log odds two classes \ndefault class. positive number means increasing variable makes \nclass likely negative number means opposite.Predict class first 5 flowers training data.package glmnet implements also multinomial logistic regression using\nglmnet(..., family = \"multinomial\").","code":"\nset.seed(100)\nx <- iris[sample(1:nrow(iris)), ]\n\nmodel <- nnet::multinom(Species ~., data = x)\n## # weights:  18 (10 variable)\n## initial  value 164.791843 \n## iter  10 value 16.177348\n## iter  20 value 7.111438\n## iter  30 value 6.182999\n## iter  40 value 5.984028\n## iter  50 value 5.961278\n## iter  60 value 5.954900\n## iter  70 value 5.951851\n## iter  80 value 5.950343\n## iter  90 value 5.949904\n## iter 100 value 5.949867\n## final  value 5.949867 \n## stopped after 100 iterations\nmodel\n## Call:\n## nnet::multinom(formula = Species ~ ., data = x)\n## \n## Coefficients:\n##            (Intercept) Sepal.Length Sepal.Width\n## versicolor       18.69       -5.458      -8.707\n## virginica       -23.84       -7.924     -15.371\n##            Petal.Length Petal.Width\n## versicolor        14.24      -3.098\n## virginica         23.66      15.135\n## \n## Residual Deviance: 11.9 \n## AIC: 31.9\nx[1:5, ]\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 102          5.8         2.7          5.1         1.9\n## 112          6.4         2.7          5.3         1.9\n## 4            4.6         3.1          1.5         0.2\n## 55           6.5         2.8          4.6         1.5\n## 70           5.6         2.5          3.9         1.1\n##        Species\n## 102  virginica\n## 112  virginica\n## 4       setosa\n## 55  versicolor\n## 70  versicolor\npredict(model, x[1:5,])\n## [1] virginica  virginica  setosa     versicolor versicolor\n## Levels: setosa versicolor virginica"},{"path":"appendix_logistic_regression.html","id":"exercises-7","chapter":"C Logistic Regression","heading":"C.9 Exercises","text":"use Palmer penguin data exercises.Create R markdown document performs following:Create test training data set (see section Holdout Method Chapter 3).Create logistic regression using training set predict variable sex.Use stepwise variable selection. variables selected?parameters selected features tell ?Predict sex penguins test set. Create \nconfusion table calculate accuracy discuss well model works.","code":"\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm\n##   <chr>   <chr>              <dbl>         <dbl>\n## 1 Adelie  Torgersen           39.1          18.7\n## 2 Adelie  Torgersen           39.5          17.4\n## 3 Adelie  Torgersen           40.3          18  \n## 4 Adelie  Torgersen           NA            NA  \n## 5 Adelie  Torgersen           36.7          19.3\n## 6 Adelie  Torgersen           39.3          20.6\n## # ℹ 4 more variables: flipper_length_mm <dbl>,\n## #   body_mass_g <dbl>, sex <chr>, year <dbl>"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
