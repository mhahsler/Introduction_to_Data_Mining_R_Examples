<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="3.6 Feature Selection and Feature Preparation | R Code Companion for the Textbook Introduction to Data Mining" />
<meta property="og:type" content="book" />


<meta property="og:description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (2006 or 2017 edition)." />


<meta name="author" content="Michael Hahsler" />

<meta name="date" content="2021-07-09" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (2006 or 2017 edition).">

<title>3.6 Feature Selection and Feature Preparation | R Code Companion for the Textbook Introduction to Data Mining</title>

<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap/css/united.min.css" rel="stylesheet" />
<script src="libs/bootstrap/js/bootstrap.min.js"></script>
<script src="libs/bootstrap/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation/tabsets.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/plotly-binding/plotly.js"></script>
<script src="libs/typedarray/typedarray.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main/plotly-latest.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider/jquery.nouislider.min.js"></script>
<link href="libs/selectize/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize/selectize.min.js"></script>
<link href="libs/vis/vis.css" rel="stylesheet" />
<script src="libs/vis/vis.min.js"></script>
<script src="libs/visNetwork-binding/visNetwork.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="1-introduction.html#introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="1-1-data-manipulation-with-tidyverse.html#data-manipulation-with-tidyverse"><span class="toc-section-number">1.1</span> Data Manipulation with Tidyverse</a></li>
<li><a href="1-2-visualization-with-ggplot2.html#visualization-with-ggplot2"><span class="toc-section-number">1.2</span> Visualization with ggplot2</a></li>
</ul></li>
<li><a href="2-data.html#data"><span class="toc-section-number">2</span> Data</a>
<ul>
<li><a href="2-1-the-iris-dataset.html#the-iris-dataset"><span class="toc-section-number">2.1</span> The Iris Dataset</a></li>
<li><a href="2-2-data-quality.html#data-quality"><span class="toc-section-number">2.2</span> Data Quality</a></li>
<li><a href="2-3-aggregation.html#aggregation"><span class="toc-section-number">2.3</span> Aggregation</a></li>
<li><a href="2-4-sampling.html#sampling"><span class="toc-section-number">2.4</span> Sampling</a>
<ul>
<li><a href="2-4-sampling.html#random-sampling"><span class="toc-section-number">2.4.1</span> Random Sampling</a></li>
<li><a href="2-4-sampling.html#stratified-sampling"><span class="toc-section-number">2.4.2</span> Stratified Sampling</a></li>
</ul></li>
<li><a href="2-5-features.html#features"><span class="toc-section-number">2.5</span> Features</a>
<ul>
<li><a href="2-5-features.html#dimensionality-reduction"><span class="toc-section-number">2.5.1</span> Dimensionality Reduction</a></li>
<li><a href="2-5-features.html#feature-selection"><span class="toc-section-number">2.5.2</span> Feature Selection</a></li>
<li><a href="2-5-features.html#discretize-features"><span class="toc-section-number">2.5.3</span> Discretize Features</a></li>
<li><a href="2-5-features.html#standardize-data-z-scores"><span class="toc-section-number">2.5.4</span> Standardize Data (Z-Scores)</a></li>
</ul></li>
<li><a href="2-6-proximities-similarities-and-distances.html#proximities-similarities-and-distances"><span class="toc-section-number">2.6</span> Proximities: Similarities and Distances</a>
<ul>
<li><a href="2-6-proximities-similarities-and-distances.html#minkowsky-distances"><span class="toc-section-number">2.6.1</span> Minkowsky Distances</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#distances-for-binary-data"><span class="toc-section-number">2.6.2</span> Distances for Binary Data</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#distances-for-mixed-data"><span class="toc-section-number">2.6.3</span> Distances for Mixed Data</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#additional-proximity-measures-available-in-package-proxy"><span class="toc-section-number">2.6.4</span> Additional proximity Measures Available in Package proxy</a></li>
</ul></li>
<li><a href="2-7-relationships-between-features.html#relationships-between-features"><span class="toc-section-number">2.7</span> Relationships Between Features</a>
<ul>
<li><a href="2-7-relationships-between-features.html#correlation"><span class="toc-section-number">2.7.1</span> Correlation</a></li>
<li><a href="2-7-relationships-between-features.html#rank-correlation"><span class="toc-section-number">2.7.2</span> Rank Correlation</a></li>
<li><a href="2-7-relationships-between-features.html#relationship-between-nominal-and-ordinal-features"><span class="toc-section-number">2.7.3</span> Relationship Between Nominal and Ordinal Features</a></li>
</ul></li>
<li><a href="2-8-density-estimation.html#density-estimation"><span class="toc-section-number">2.8</span> Density Estimation</a></li>
<li><a href="2-9-exploring-data.html#exploring-data"><span class="toc-section-number">2.9</span> Exploring Data</a>
<ul>
<li><a href="2-9-exploring-data.html#basic-statistics"><span class="toc-section-number">2.9.1</span> Basic statistics</a></li>
<li><a href="2-9-exploring-data.html#tabulate-data"><span class="toc-section-number">2.9.2</span> Tabulate data</a></li>
<li><a href="2-9-exploring-data.html#percentiles-quantiles"><span class="toc-section-number">2.9.3</span> Percentiles (Quantiles)</a></li>
</ul></li>
<li><a href="2-10-visualization.html#visualization"><span class="toc-section-number">2.10</span> Visualization</a>
<ul>
<li><a href="2-10-visualization.html#histogram"><span class="toc-section-number">2.10.1</span> Histogram</a></li>
<li><a href="2-10-visualization.html#boxplot"><span class="toc-section-number">2.10.2</span> Boxplot</a></li>
<li><a href="2-10-visualization.html#scatter-plot"><span class="toc-section-number">2.10.3</span> Scatter plot</a></li>
<li><a href="2-10-visualization.html#scatter-plot-matrix"><span class="toc-section-number">2.10.4</span> Scatter Plot Matrix</a></li>
<li><a href="2-10-visualization.html#data-matrix-visualization"><span class="toc-section-number">2.10.5</span> Data Matrix Visualization</a></li>
<li><a href="2-10-visualization.html#correlation-matrix"><span class="toc-section-number">2.10.6</span> Correlation Matrix</a></li>
<li><a href="2-10-visualization.html#parallel-coordinates-plot"><span class="toc-section-number">2.10.7</span> Parallel Coordinates Plot</a></li>
</ul></li>
</ul></li>
<li><a href="3-classification-basic-concepts-and-techniques.html#classification-basic-concepts-and-techniques"><span class="toc-section-number">3</span> Classification: Basic Concepts and Techniques</a>
<ul>
<li><a href="3-1-the-zoo-dataset.html#the-zoo-dataset"><span class="toc-section-number">3.1</span> The Zoo Dataset</a></li>
<li><a href="3-2-decision-trees.html#decision-trees"><span class="toc-section-number">3.2</span> Decision Trees</a>
<ul>
<li><a href="3-2-decision-trees.html#create-tree-with-default-settings-uses-pre-pruning"><span class="toc-section-number">3.2.1</span> Create Tree With Default Settings (uses pre-pruning)</a></li>
<li><a href="3-2-decision-trees.html#create-a-full-tree"><span class="toc-section-number">3.2.2</span> Create a Full Tree</a></li>
<li><a href="3-2-decision-trees.html#make-predictions-for-new-data"><span class="toc-section-number">3.2.3</span> Make Predictions for New Data</a></li>
</ul></li>
<li><a href="3-3-model-evaluation-with-caret.html#model-evaluation-with-caret"><span class="toc-section-number">3.3</span> Model Evaluation with Caret</a>
<ul>
<li><a href="3-3-model-evaluation-with-caret.html#hold-out-test-data"><span class="toc-section-number">3.3.1</span> Hold out Test Data</a></li>
<li><a href="3-3-model-evaluation-with-caret.html#learn-a-model-and-tune-hyperparameters-on-the-training-data"><span class="toc-section-number">3.3.2</span> Learn a Model and Tune Hyperparameters on the Training Data</a></li>
</ul></li>
<li><a href="3-4-testing-confusion-matrix-and-confidence-interval-for-accuracy.html#testing-confusion-matrix-and-confidence-interval-for-accuracy"><span class="toc-section-number">3.4</span> Testing: Confusion Matrix and Confidence Interval for Accuracy</a></li>
<li><a href="3-5-model-comparison.html#model-comparison"><span class="toc-section-number">3.5</span> Model Comparison</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#feature-selection-and-feature-preparation"><span class="toc-section-number">3.6</span> Feature Selection and Feature Preparation</a>
<ul>
<li><a href="3-6-feature-selection-and-feature-preparation.html#univariate-feature-importance-score"><span class="toc-section-number">3.6.1</span> Univariate Feature Importance Score</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#feature-subset-selection"><span class="toc-section-number">3.6.2</span> Feature Subset Selection</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#using-dummy-variables-for-factors"><span class="toc-section-number">3.6.3</span> Using Dummy Variables for Factors</a></li>
</ul></li>
<li><a href="3-7-class-imbalance.html#class-imbalance"><span class="toc-section-number">3.7</span> Class Imbalance</a>
<ul>
<li><a href="3-7-class-imbalance.html#option-1-use-the-data-as-is-and-hope-for-the-best"><span class="toc-section-number">3.7.1</span> Option 1: Use the Data As Is and Hope For The Best</a></li>
<li><a href="3-7-class-imbalance.html#option-2-balance-data-with-resampling"><span class="toc-section-number">3.7.2</span> Option 2: Balance Data With Resampling</a></li>
<li><a href="3-7-class-imbalance.html#option-3-build-a-larger-tree-and-use-predicted-probabilities"><span class="toc-section-number">3.7.3</span> Option 3: Build A Larger Tree and use Predicted Probabilities</a></li>
<li><a href="3-7-class-imbalance.html#option-4-use-a-cost-sensitive-classifier"><span class="toc-section-number">3.7.4</span> Option 4: Use a Cost-Sensitive Classifier</a></li>
</ul></li>
</ul></li>
<li><a href="4-classification-alternative-techniques.html#classification-alternative-techniques"><span class="toc-section-number">4</span> Classification: Alternative Techniques</a>
<ul>
<li><a href="4-1-training-and-test-data.html#training-and-test-data"><span class="toc-section-number">4.1</span> Training and Test Data</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#fitting-different-classification-models-to-the-training-data"><span class="toc-section-number">4.2</span> Fitting Different Classification Models to the Training Data</a>
<ul>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#conditional-inference-tree-decision-tree"><span class="toc-section-number">4.2.1</span> Conditional Inference Tree (Decision Tree)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#c-4.5-decision-tree"><span class="toc-section-number">4.2.2</span> C 4.5 Decision Tree</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#k-nearest-neighbors"><span class="toc-section-number">4.2.3</span> K-Nearest Neighbors</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#part-rule-based-classifier"><span class="toc-section-number">4.2.4</span> PART (Rule-based classifier)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#linear-support-vector-machines"><span class="toc-section-number">4.2.5</span> Linear Support Vector Machines</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#random-forest"><span class="toc-section-number">4.2.6</span> Random Forest</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#gradient-boosted-decision-trees-xgboost"><span class="toc-section-number">4.2.7</span> Gradient Boosted Decision Trees (xgboost)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#artificial-neural-network"><span class="toc-section-number">4.2.8</span> Artificial Neural Network</a></li>
</ul></li>
<li><a href="4-3-comparing-models.html#comparing-models"><span class="toc-section-number">4.3</span> Comparing Models</a></li>
<li><a href="4-4-applying-the-chosen-model-to-the-test-data.html#applying-the-chosen-model-to-the-test-data"><span class="toc-section-number">4.4</span> Applying the Chosen Model to the Test Data</a></li>
<li><a href="4-5-decision-boundaries.html#decision-boundaries"><span class="toc-section-number">4.5</span> Decision Boundaries</a>
<ul>
<li><a href="4-5-decision-boundaries.html#iris-dataset"><span class="toc-section-number">4.5.1</span> Iris Dataset</a></li>
<li><a href="4-5-decision-boundaries.html#circle-dataset"><span class="toc-section-number">4.5.2</span> Circle Dataset</a></li>
</ul></li>
<li><a href="4-6-more-information.html#more-information"><span class="toc-section-number">4.6</span> More Information</a></li>
</ul></li>
<li><a href="5-association-analysis-basic-concepts-and-algorithms.html#association-analysis-basic-concepts-and-algorithms"><span class="toc-section-number">5</span> Association Analysis: Basic Concepts and Algorithms</a>
<ul>
<li><a href="5-1-the-arules-package.html#the-arules-package"><span class="toc-section-number">5.1</span> The arules Package</a></li>
<li><a href="5-2-transactions.html#transactions"><span class="toc-section-number">5.2</span> Transactions</a>
<ul>
<li><a href="5-2-transactions.html#create-transactions"><span class="toc-section-number">5.2.1</span> Create Transactions</a></li>
<li><a href="5-2-transactions.html#inspect-transactions"><span class="toc-section-number">5.2.2</span> Inspect Transactions</a></li>
<li><a href="5-2-transactions.html#vertical-layout-transaction-id-lists"><span class="toc-section-number">5.2.3</span> Vertical Layout (Transaction ID Lists)</a></li>
</ul></li>
<li><a href="5-3-frequent-itemsets.html#frequent-itemsets"><span class="toc-section-number">5.3</span> Frequent Itemsets</a>
<ul>
<li><a href="5-3-frequent-itemsets.html#mine-frequent-itemsets"><span class="toc-section-number">5.3.1</span> Mine Frequent Itemsets</a></li>
<li><a href="5-3-frequent-itemsets.html#concise-representation-of-itemsets"><span class="toc-section-number">5.3.2</span> Concise Representation of Itemsets</a></li>
</ul></li>
<li><a href="5-4-association-rules.html#association-rules"><span class="toc-section-number">5.4</span> Association Rules</a>
<ul>
<li><a href="5-4-association-rules.html#mine-association-rules"><span class="toc-section-number">5.4.1</span> Mine Association Rules</a></li>
<li><a href="5-4-association-rules.html#calculate-additional-interest-measures"><span class="toc-section-number">5.4.2</span> Calculate Additional Interest Measures</a></li>
<li><a href="5-4-association-rules.html#mine-using-templates"><span class="toc-section-number">5.4.3</span> Mine Using Templates</a></li>
</ul></li>
<li><a href="5-5-association-rule-visualization.html#association-rule-visualization"><span class="toc-section-number">5.5</span> Association Rule Visualization</a></li>
<li><a href="5-6-interactive-visualizations.html#interactive-visualizations"><span class="toc-section-number">5.6</span> Interactive Visualizations</a>
<ul>
<li><a href="5-6-interactive-visualizations.html#interactive-inspect-with-sorting-filtering-and-paging"><span class="toc-section-number">5.6.1</span> Interactive Inspect With Sorting, Filtering and Paging</a></li>
<li><a href="5-6-interactive-visualizations.html#scatter-plot-1"><span class="toc-section-number">5.6.2</span> Scatter Plot</a></li>
<li><a href="5-6-interactive-visualizations.html#matrix-visualization"><span class="toc-section-number">5.6.3</span> Matrix Visualization</a></li>
<li><a href="5-6-interactive-visualizations.html#visualization-as-graph"><span class="toc-section-number">5.6.4</span> Visualization as Graph</a></li>
<li><a href="5-6-interactive-visualizations.html#interactive-rule-explorer"><span class="toc-section-number">5.6.5</span> Interactive Rule Explorer</a></li>
</ul></li>
</ul></li>
<li><a href="6-association-analysis-advanced-concepts.html#association-analysis-advanced-concepts"><span class="toc-section-number">6</span> Association Analysis: Advanced Concepts</a></li>
<li><a href="7-clustering-analysis.html#clustering-analysis"><span class="toc-section-number">7</span> Clustering Analysis</a>
<ul>
<li><a href="7-1-data-preparation.html#data-preparation"><span class="toc-section-number">7.1</span> Data Preparation</a>
<ul>
<li><a href="7-1-data-preparation.html#data-cleaning"><span class="toc-section-number">7.1.1</span> Data cleaning</a></li>
<li><a href="7-1-data-preparation.html#scale-data"><span class="toc-section-number">7.1.2</span> Scale data</a></li>
</ul></li>
<li><a href="7-2-clustering-methods.html#clustering-methods"><span class="toc-section-number">7.2</span> Clustering methods</a>
<ul>
<li><a href="7-2-clustering-methods.html#k-means-clustering"><span class="toc-section-number">7.2.1</span> k-means Clustering</a></li>
<li><a href="7-2-clustering-methods.html#hierarchical-clustering"><span class="toc-section-number">7.2.2</span> Hierarchical Clustering</a></li>
<li><a href="7-2-clustering-methods.html#density-based-clustering-with-dbscan"><span class="toc-section-number">7.2.3</span> Density-based clustering with DBSCAN</a></li>
<li><a href="7-2-clustering-methods.html#partitioning-around-medoids-pam"><span class="toc-section-number">7.2.4</span> Partitioning Around Medoids (PAM)</a></li>
<li><a href="7-2-clustering-methods.html#gaussian-mixture-models"><span class="toc-section-number">7.2.5</span> Gaussian Mixture Models</a></li>
<li><a href="7-2-clustering-methods.html#spectral-clustering"><span class="toc-section-number">7.2.6</span> Spectral clustering</a></li>
<li><a href="7-2-clustering-methods.html#fuzzy-c-means-clustering"><span class="toc-section-number">7.2.7</span> Fuzzy C-Means Clustering</a></li>
</ul></li>
<li><a href="7-3-internal-cluster-validation.html#internal-cluster-validation"><span class="toc-section-number">7.3</span> Internal Cluster Validation</a>
<ul>
<li><a href="7-3-internal-cluster-validation.html#compare-the-clustering-quality"><span class="toc-section-number">7.3.1</span> Compare the Clustering Quality</a></li>
<li><a href="7-3-internal-cluster-validation.html#silhouette-plot"><span class="toc-section-number">7.3.2</span> Silhouette plot</a></li>
<li><a href="7-3-internal-cluster-validation.html#find-optimal-number-of-clusters-for-k-means"><span class="toc-section-number">7.3.3</span> Find Optimal Number of Clusters for k-means</a></li>
<li><a href="7-3-internal-cluster-validation.html#visualizing-the-distance-matrix"><span class="toc-section-number">7.3.4</span> Visualizing the Distance Matrix</a></li>
</ul></li>
<li><a href="7-4-external-cluster-validation.html#external-cluster-validation"><span class="toc-section-number">7.4</span> External Cluster Validation</a></li>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#advanced-data-preparation-for-clustering"><span class="toc-section-number">7.5</span> Advanced Data Preparation for Clustering</a>
<ul>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#outlier-removal"><span class="toc-section-number">7.5.1</span> Outlier Removal</a></li>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#clustering-tendency"><span class="toc-section-number">7.5.2</span> Clustering Tendency</a></li>
</ul></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="feature-selection-and-feature-preparation" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Feature Selection and Feature Preparation</h2>
<p>Decision trees implicitly select features for splitting, but we can also
select features manually.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb300-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(FSelector)</span></code></pre></div>
<p>see: <a href="http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection#The_Feature_Ranking_Approach" class="uri">http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection#The_Feature_Ranking_Approach</a></p>
<div id="univariate-feature-importance-score" class="section level3" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Univariate Feature Importance Score</h3>
<p>These scores measure how related
each feature is to the class variable.
For discrete features (as in our case), the chi-square statistic can be used
to derive a score.</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb301-1" aria-hidden="true" tabindex="-1"></a>weights <span class="ot">&lt;-</span> Zoo_train <span class="sc">%&gt;%</span> <span class="fu">chi.squared</span>(type <span class="sc">~</span> ., <span class="at">data =</span> .) <span class="sc">%&gt;%</span></span>
<span id="cb301-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb301-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>(<span class="at">rownames =</span> <span class="st">&quot;feature&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb301-3"><a href="3-6-feature-selection-and-feature-preparation.html#cb301-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(attr_importance))</span>
<span id="cb301-4"><a href="3-6-feature-selection-and-feature-preparation.html#cb301-4" aria-hidden="true" tabindex="-1"></a>weights</span></code></pre></div>
<pre><code>## # A tibble: 16 x 2
##    feature  attr_importance
##    &lt;chr&gt;              &lt;dbl&gt;
##  1 feathers           1    
##  2 milk               1    
##  3 backbone           1    
##  4 toothed            0.975
##  5 eggs               0.933
##  6 hair               0.907
##  7 breathes           0.898
##  8 airborne           0.848
##  9 fins               0.845
## 10 legs               0.828
## 11 tail               0.779
## 12 catsize            0.664
## 13 aquatic            0.655
## 14 venomous           0.475
## 15 predator           0.385
## 16 domestic           0.231</code></pre>
<p>plot importance in descending order (using <code>reorder</code> to order factor levels used by <code>ggplot</code>).</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb303-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(weights,</span>
<span id="cb303-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb303-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> attr_importance, <span class="at">y =</span> <span class="fu">reorder</span>(feature, attr_importance))) <span class="sc">+</span></span>
<span id="cb303-3"><a href="3-6-feature-selection-and-feature-preparation.html#cb303-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb303-4"><a href="3-6-feature-selection-and-feature-preparation.html#cb303-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Importance score&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Feature&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-115-1.png" width="672" /></p>
<p>Get the 5 best features</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb304-1" aria-hidden="true" tabindex="-1"></a>subset <span class="ot">&lt;-</span> <span class="fu">cutoff.k</span>(weights <span class="sc">%&gt;%</span> <span class="fu">column_to_rownames</span>(<span class="st">&quot;feature&quot;</span>), <span class="dv">5</span>)</span>
<span id="cb304-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb304-2" aria-hidden="true" tabindex="-1"></a>subset</span></code></pre></div>
<pre><code>## [1] &quot;feathers&quot; &quot;milk&quot;     &quot;backbone&quot; &quot;toothed&quot; 
## [5] &quot;eggs&quot;</code></pre>
<p>Use only the best 5 features to build a model (<code>Fselector</code> provides <code>as.simple.formula</code>)</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb306-1" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="fu">as.simple.formula</span>(subset, <span class="st">&quot;type&quot;</span>)</span>
<span id="cb306-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb306-2" aria-hidden="true" tabindex="-1"></a>f</span></code></pre></div>
<pre><code>## type ~ feathers + milk + backbone + toothed + eggs
## &lt;environment: 0x56503bd328f0&gt;</code></pre>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb308-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> Zoo_train <span class="sc">%&gt;%</span> <span class="fu">rpart</span>(f, <span class="at">data =</span> .)</span>
<span id="cb308-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb308-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(m, <span class="at">extra =</span> <span class="dv">2</span>, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-117-1.png" width="672" /></p>
<p>There are many alternative ways to calculate univariate importance
scores (see package FSelector). Some of them (also) work for continuous
features. One example is the information gain ratio based on entropy as used in decision tree induction.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb309-1" aria-hidden="true" tabindex="-1"></a>Zoo_train <span class="sc">%&gt;%</span> <span class="fu">gain.ratio</span>(type <span class="sc">~</span> ., <span class="at">data =</span> .) <span class="sc">%&gt;%</span></span>
<span id="cb309-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb309-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>(<span class="at">rownames =</span> <span class="st">&quot;feature&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb309-3"><a href="3-6-feature-selection-and-feature-preparation.html#cb309-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(attr_importance))</span></code></pre></div>
<pre><code>## # A tibble: 16 x 2
##    feature  attr_importance
##    &lt;chr&gt;              &lt;dbl&gt;
##  1 milk              1     
##  2 backbone          1     
##  3 feathers          1     
##  4 toothed           0.919 
##  5 eggs              0.827 
##  6 breathes          0.821 
##  7 hair              0.782 
##  8 fins              0.689 
##  9 legs              0.682 
## 10 airborne          0.671 
## 11 tail              0.573 
## 12 aquatic           0.391 
## 13 catsize           0.383 
## 14 venomous          0.351 
## 15 predator          0.125 
## 16 domestic          0.0975</code></pre>
</div>
<div id="feature-subset-selection" class="section level3" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Feature Subset Selection</h3>
<p>Often features are related and calculating importance for each feature
independently is not optimal. We can use greedy search heuristics. For
example <code>cfs</code> uses correlation/entropy with best first search.</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb311-1" aria-hidden="true" tabindex="-1"></a>Zoo_train <span class="sc">%&gt;%</span> <span class="fu">cfs</span>(type <span class="sc">~</span> ., <span class="at">data =</span> .)</span></code></pre></div>
<pre><code>##  [1] &quot;hair&quot;     &quot;feathers&quot; &quot;eggs&quot;     &quot;milk&quot;    
##  [5] &quot;toothed&quot;  &quot;backbone&quot; &quot;breathes&quot; &quot;fins&quot;    
##  [9] &quot;legs&quot;     &quot;tail&quot;</code></pre>
<p>Black-box feature selection uses an evaluator function (the black box)
to calculate a score to be maximized.
First, we define an evaluation function that builds a model given a subset
of features and calculates a quality score. We use here the
average for 5 bootstrap samples (<code>method = "cv"</code> can also be used instead), no tuning (to be faster), and the
average accuracy as the score.</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-1" aria-hidden="true" tabindex="-1"></a>evaluator <span class="ot">&lt;-</span> <span class="cf">function</span>(subset) {</span>
<span id="cb313-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-2" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> Zoo_train <span class="sc">%&gt;%</span> <span class="fu">train</span>(<span class="fu">as.simple.formula</span>(subset, <span class="st">&quot;type&quot;</span>),</span>
<span id="cb313-3"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> .,</span>
<span id="cb313-4"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb313-5"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;boot&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>),</span>
<span id="cb313-6"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">tuneLength =</span> <span class="dv">0</span>)</span>
<span id="cb313-7"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-7" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">&lt;-</span> model<span class="sc">$</span>resample<span class="sc">$</span>Accuracy</span>
<span id="cb313-8"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Trying features:&quot;</span>, <span class="fu">paste</span>(subset, <span class="at">collapse =</span> <span class="st">&quot; + &quot;</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb313-9"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-9" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="fu">mean</span>(results)</span>
<span id="cb313-10"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Accuracy:&quot;</span>, <span class="fu">round</span>(m, <span class="dv">2</span>), <span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>)</span>
<span id="cb313-11"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-11" aria-hidden="true" tabindex="-1"></a>  m</span>
<span id="cb313-12"><a href="3-6-feature-selection-and-feature-preparation.html#cb313-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Start with all features (but not the class variable <code>type</code>)</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb314-1" aria-hidden="true" tabindex="-1"></a>features <span class="ot">&lt;-</span> Zoo_train <span class="sc">%&gt;%</span> <span class="fu">colnames</span>() <span class="sc">%&gt;%</span> <span class="fu">setdiff</span>(<span class="st">&quot;type&quot;</span>)</span></code></pre></div>
<p>There are several (greedy) search strategies available. These run
for a while!</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb315-1" aria-hidden="true" tabindex="-1"></a><span class="do">##subset &lt;- backward.search(features, evaluator)</span></span>
<span id="cb315-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb315-2" aria-hidden="true" tabindex="-1"></a><span class="do">##subset &lt;- forward.search(features, evaluator)</span></span>
<span id="cb315-3"><a href="3-6-feature-selection-and-feature-preparation.html#cb315-3" aria-hidden="true" tabindex="-1"></a><span class="do">##subset &lt;- best.first.search(features, evaluator)</span></span>
<span id="cb315-4"><a href="3-6-feature-selection-and-feature-preparation.html#cb315-4" aria-hidden="true" tabindex="-1"></a><span class="do">##subset &lt;- hill.climbing.search(features, evaluator)</span></span>
<span id="cb315-5"><a href="3-6-feature-selection-and-feature-preparation.html#cb315-5" aria-hidden="true" tabindex="-1"></a><span class="do">##subset</span></span></code></pre></div>
</div>
<div id="using-dummy-variables-for-factors" class="section level3" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Using Dummy Variables for Factors</h3>
<p>Nominal features (factors) are often encoded as a series of 0-1 dummy variables.
For example, let us try to predict if an animal is a predator given the type.
First we use the original encoding of type as a factor with several values.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb316-1" aria-hidden="true" tabindex="-1"></a>tree_predator <span class="ot">&lt;-</span> Zoo_train <span class="sc">%&gt;%</span> <span class="fu">rpart</span>(predator <span class="sc">~</span> type, <span class="at">data =</span> .)</span>
<span id="cb316-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb316-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_predator, <span class="at">extra =</span> <span class="dv">2</span>, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-123-1.png" width="672" /></p>
<p><strong>Note:</strong> Some splits use multiple values. Building the tree will become
extremely slow if a factor has many levels (different values) since the tree has to check all possible splits into two subsets. This situation should be avoided.</p>
<p>Recode type as a set of 0-1 dummy variables using <code>class2ind</code>. See also
<code>? dummyVars</code> in package <code>caret</code>.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb317-1" aria-hidden="true" tabindex="-1"></a>Zoo_train_dummy <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(<span class="fu">class2ind</span>(Zoo_train<span class="sc">$</span>type)) <span class="sc">%&gt;%</span> <span class="fu">mutate_all</span>(as.factor) <span class="sc">%&gt;%</span></span>
<span id="cb317-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb317-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">predator =</span> Zoo_train<span class="sc">$</span>predator)</span>
<span id="cb317-3"><a href="3-6-feature-selection-and-feature-preparation.html#cb317-3" aria-hidden="true" tabindex="-1"></a>Zoo_train_dummy</span></code></pre></div>
<pre><code>## # A tibble: 83 x 8
##    mammal bird  reptile fish  amphibian insect
##    &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt; 
##  1 1      0     0       0     0         0     
##  2 1      0     0       0     0         0     
##  3 0      0     0       1     0         0     
##  4 1      0     0       0     0         0     
##  5 1      0     0       0     0         0     
##  6 1      0     0       0     0         0     
##  7 0      0     0       1     0         0     
##  8 0      0     0       1     0         0     
##  9 1      0     0       0     0         0     
## 10 0      1     0       0     0         0     
## # â€¦ with 73 more rows, and 2 more variables:
## #   mollusc.et.al &lt;fct&gt;, predator &lt;fct&gt;</code></pre>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb319-1" aria-hidden="true" tabindex="-1"></a>tree_predator <span class="ot">&lt;-</span> Zoo_train_dummy <span class="sc">%&gt;%</span> <span class="fu">rpart</span>(predator <span class="sc">~</span> ., <span class="at">data =</span> .,</span>
<span id="cb319-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb319-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">2</span>, <span class="at">cp =</span> <span class="fl">0.01</span>))</span>
<span id="cb319-3"><a href="3-6-feature-selection-and-feature-preparation.html#cb319-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_predator, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-124-1.png" width="672" /></p>
<p>Using <code>caret</code> on the original factor encoding automatically translates factors
(here type) into 0-1 dummy variables (e.g., <code>typeinsect = 0</code>).
The reason is that some models cannot
directly use factors and <code>caret</code> tries to consistently work with
all of them.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb320-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> Zoo_train <span class="sc">%&gt;%</span> <span class="fu">train</span>(predator <span class="sc">~</span> type, <span class="at">data =</span> ., <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb320-2"><a href="3-6-feature-selection-and-feature-preparation.html#cb320-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">2</span>),</span>
<span id="cb320-3"><a href="3-6-feature-selection-and-feature-preparation.html#cb320-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">cp =</span> <span class="fl">0.01</span>))</span>
<span id="cb320-4"><a href="3-6-feature-selection-and-feature-preparation.html#cb320-4" aria-hidden="true" tabindex="-1"></a>fit</span></code></pre></div>
<pre><code>## CART 
## 
## 83 samples
##  1 predictor
##  2 classes: &#39;TRUE&#39;, &#39;FALSE&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... 
## Resampling results:
## 
##   Accuracy  Kappa 
##   0.606     0.2034
## 
## Tuning parameter &#39;cp&#39; was held constant at a value
##  of 0.01</code></pre>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="3-6-feature-selection-and-feature-preparation.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(fit<span class="sc">$</span>finalModel, <span class="at">extra =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-125-1.png" width="672" /></p>
<p><em>Note:</em> To use a fixed value for the tuning parameter <code>cp</code>, we have to
create a tuning grid that only icontains that value.</p>
</div>
</div>
<p style="text-align: center;">
<a href="3-5-model-comparison.html"><button class="btn btn-default">Previous</button></a>
<a href="3-7-class-imbalance.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
