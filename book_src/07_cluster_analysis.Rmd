---
editor_options: 
  markdown: 
    wrap: 72
---

# Cluster Analysis

This chapter introduces cluster analysis using K-means, hierarchical clustering
and DBSCAN. We will discuss how to choose the number of clusters and how 
to evaluate the quality clusterings. In addition, we will introduce more 
clustering algorithms and how clustering is influenced by outliers.

The corresponding chapter of 
the data mining textbook is available online: 
[Chapter 7: Cluster Analysis: Basic Concepts and Algorithms.](https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf)

### Packages Used in this Chapter {.unnumbered}

```{r setup_07}
pkgs <- c("cluster", "dbscan", "e1071", "factoextra", "fpc", 
          "GGally", "kernlab", "mclust", "mlbench", "scatterpie", 
          "seriation", "tidyverse")
  
pkgs_install <- pkgs[!(pkgs %in% installed.packages()[,"Package"])]
if(length(pkgs_install)) install.packages(pkgs_install)
```

```{r setup_07-2, include=FALSE}
source("_common.R")
source("format_pkgs.R")
all_pkgs <- union(all_pkgs, pkgs)
```

The packages used for this chapter are: 

`r format_pkgs(pkgs)`

## Overview

[Cluster analysis or clustering](https://en.wikipedia.org/wiki/Cluster_analysis) 
is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).

Clustering is also called unsupervised learning, because it tries to directly learns the structure of the data
and does not rely on the availability of a correct answer or class label as supervised learning does.
Clustering is often used for exploratory analysis or to preprocess data by grouping.   

You can read the free sample chapter from the textbook [@Tan2005]:
[Chapter 7. Cluster Analysis: Basic Concepts and
Algorithms](https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf)

### Data Preparation

```{r }
library(tidyverse)
```

We will use here a small and very clean toy dataset called Ruspini which is
included in the R package **cluster**.

```{r }
data(ruspini, package = "cluster")
```

The Ruspini data set, consisting of 75 points in four groups that is
popular for illustrating clustering techniques. It is a very simple data
set with well separated clusters. The original dataset has the points
ordered by group. We can shuffle the data (rows) using `sample_frac`
which samples by default 100%.

```{r }
ruspini <- as_tibble(ruspini) |> 
  sample_frac()
ruspini
```

### Data cleaning

```{r }
ggplot(ruspini, aes(x = x, y = y)) + geom_point()

summary(ruspini)
```

For most clustering algorithms it is necessary to handle missing values
and outliers (e.g., remove the observations). For details see Section
"Outlier removal" below. This data set has not missing values or strong
outlier and looks like it has some very clear groups.

### Scale data

Clustering algorithms use distances and the variables with the largest
number range will dominate distance calculation. The summary above shows
that this is not an issue for the Ruspini dataset with both, x and y,
being roughly between 0 and 150. Most data analysts will still scale
each column in the data to zero mean and unit standard deviation
([z-scores](https://en.wikipedia.org/wiki/Standard_score)). 

*Note:* The standard `scale()` function scales a whole data
matrix so we implement a function for a single vector and apply it to
all numeric columns.

```{r }
## I use this till tidyverse implements a scale function
scale_numeric <- function(x) {
  x |> mutate(across(where(is.numeric), 
                     function(y) (y - mean(y, na.rm = TRUE)) / sd(y, na.rm = TRUE)))
}
```

```{r }
ruspini_scaled <- ruspini |> 
  scale_numeric()
summary(ruspini_scaled)
```

After scaling, most z-scores will fall in the range $[-3,3]$ (z-scores
are measured in standard deviations from the mean), where $0$ means
average.


## K-means

[k-means](https://en.wikipedia.org/wiki/K-means_clustering) implicitly
assumes Euclidean distances. We use $k = 4$ clusters and run the
algorithm 10 times with random initialized centroids. The best result is
returned.

```{r }
km <- kmeans(ruspini_scaled, centers = 4, nstart = 10)
km
```

`km` is an R object implemented as a list. 

```{r}
str(km)
```

The clustering vector is just a list element
containing the cluster assignment for each data row and can be accessed
using `km$cluster`. I add the cluster assignment as a column to the
original dataset (I make it a factor since it represents a nominal label).

```{r }
ruspini_clustered <- ruspini |> 
  add_column(cluster = factor(km$cluster))
ruspini_clustered

ggplot(ruspini_clustered, aes(x = x, y = y)) + 
  geom_point(aes(color = cluster))
```

Add the centroids to the plot. The centroids are scaled, so we need to unscale them 
to plot them over the original data. The second `geom_points` uses 
not the original data but specifies the centroids as its dataset.

```{r }
unscale <- function(x, original_data) {
  if (ncol(x) != ncol(original_data))
    stop("Function needs matching columns!")
  x * matrix(apply(original_data, MARGIN = 2, sd, na.rm = TRUE), 
             byrow = TRUE, nrow = nrow(x), ncol = ncol(x)) + 
    matrix(apply(original_data, MARGIN = 2, mean, na.rm = TRUE), 
           byrow = TRUE, nrow = nrow(x), ncol = ncol(x))
}

centroids <- km$centers %>% unscale(original_data = ruspini) %>% as_tibble(rownames = "cluster")
centroids

ggplot(ruspini_clustered, aes(x = x, y = y)) + 
  geom_point(aes(color = cluster)) +
  geom_point(data = centroids, aes(x = x, y = y, color = cluster), 
             shape = 3, size = 10)
```

The `factoextra` package provides also a good visualization with object labels
and ellipses for clusters. 

```{r }
library(factoextra)
fviz_cluster(km, data = ruspini_scaled, centroids = TRUE, 
             repel = TRUE, ellipse.type = "norm")
```

### Inspect Clusters

We inspect the clusters created by the 4-cluster k-means solution. The
following code can be adapted to be used for other clustering methods.

Inspect the centroids with horizontal bar charts organized by cluster.
To group the plots by cluster, we have to change the data format to the
"long"-format using a pivot operation. I use colors to match the
clusters in the scatter plots.

```{r, fig.dim = c(7, 2)}
ggplot(pivot_longer(centroids, 
                    cols = c(x, y), 
                    names_to = "feature"),
  #aes(x = feature, y = value, fill = cluster)) +
  aes(x = value, y = feature, fill = cluster)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(cluster))
```

### Extract a Single Cluster

You need is to filter the rows corresponding to the cluster index. The
next example calculates summary statistics and then plots all data
points of cluster 1.

```{r }
cluster1 <- ruspini_clustered |> 
  filter(cluster == 1)
cluster1
summary(cluster1)

ggplot(cluster1, aes(x = x, y = y)) + geom_point() +
  coord_cartesian(xlim = c(0, 120), ylim = c(0, 160))
```

What happens if we try to cluster with 8 centers?

```{r }
fviz_cluster(kmeans(ruspini_scaled, centers = 8), data = ruspini_scaled,
  centroids = TRUE,  geom = "point", ellipse.type = "norm")
```

## Agglomerative Hierarchical Clustering

Hierarchical clustering starts with a distance matrix. `dist()` defaults
to `method = "euclidean"`. 

**Notes:** 

* Distance matrices become very large
quickly (the space complexity is $O(n^2)$ where $n$ is the number if
data points). It is only possible to calculate and store the matrix for
small to medium data sets (maybe a few hundred thousand data points) in main
memory. If your data is too large then you can use sampling to reduce the 
number of points to cluster.
* The data needs to be scaled since we compute distances.


### Creating a Dendrogram

```{r }
d <- dist(ruspini_scaled)
```

`hclust()` implements [agglomerative hierarchical
clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering). 
The linkage function defines how the distance between two clusters (sets of points) 
is calculated. We specify the complete-link method where the distance between two groups 
is measured by the distance between the two farthest apart points in the two groups.

```{r }
hc <- hclust(d, method = "complete")
```

Hierarchical clustering does not return cluster assignments but a
dendrogram object which shows how the objects on the x-axis are successively joined together
when going up along the y-axis. The y-axis represents the distance at which groups of points
are joined together. The standard plot function displays the dendrogram.

```{r }
plot(hc)
```

The package `factoextra` provides a ggplot function to plot dendrograms. 

```{r warning=FALSE}
fviz_dend(hc)
```

More plotting options for dendrograms, including plotting parts of large
dendrograms can be found [here.](https://rpubs.com/gaston/dendrograms)

### Extracting a Partitional Clustering

Partitional clusterings with cluster assignments can be extracted from a dendrogram
by cutting the dendrogram horizontally using `cutree()`. Here we cut it into four parts
and add the cluster id to the data.

```{r }
clusters <- cutree(hc, k = 4)
cluster_complete <- ruspini_scaled |>
  add_column(cluster = factor(clusters))
cluster_complete
```

The 4 different clusters can be shown in the dendrogram using different colors.  

```{r warning=FALSE}
fviz_dend(hc, k = 4)
```

We can also color in the scatterplot.

```{r}
ggplot(cluster_complete, aes(x, y, color = cluster)) +
  geom_point()
```

The package `factoextra` provides an alternative visualization. Since it needs
a partition object as the first argument, we create one as a simple list.

```{r }
fviz_cluster(list(data = ruspini_scaled, 
                  cluster = cutree(hc, k = 4)), 
             geom = "point")
```

Here are the results if we use the single-link method to join clusters. 
Single-link measures the distance between two groups by the 
by the distance between the two closest points from the two groups.

```{r }
hc_single <- hclust(d, method = "single")
fviz_dend(hc_single, k = 4)

fviz_cluster(list(data = ruspini_scaled, 
                  cluster = cutree(hc_single, k = 4)), 
             geom = "point")
```

The most important difference between complete-link and single-link is that 
complete-link prefers globular clusters and single-link shows chaining 
(see the staircase pattern in the dendrogram).

## DBSCAN


[DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) stands for "Density-Based
Spatial Clustering of Applications with Noise." It groups together
points that are closely packed together and treats points in low-density
regions as outliers. DBSCAN is implemented in package `dbscan`.

```{r }
library(dbscan)
```

### DBSCAN Parameters 

DBSCAN has two parameters
that interact with each other. Changing one typically 
means that the other one also has to be adjusted. 

`minPts` defines how many points are needed in the epsilon
neighborhood to make a point a core point for a cluster. It is often chosen
as a smoothing parameter, where larger values smooth the density estimation more 
but also ignore smaller structures with less then minPts. 

`eps` is the radius of the epsilon neighborhood around a point in which the
number of points is counted.

Users typically first select minPts. We use here `minPts = 4`.
To decide on epsilon, the knee in the kNN distance plot is often used.
All points are sorted by their kNN distance. The idea is that points with
a low kNN distance are located in a dense area which should become a cluster. 
Points with a large kNN distance are in a low density area and most 
likely represent outliers and noise.  

```{r }
kNNdistplot(ruspini_scaled, minPts = 4)
abline(h = .32, col = "red")
```

Note that minPts contains the point itself, while the k-nearest neighbor
distance calculation does not. 
The plot uses therefor `k = minPts - 1`.

The knee is visible around `eps = .32` (shown by the manually added red line). 
The points to the left of the intersection of the
k-nearest neighbor distance line and the red line are the points that will be 
core points during clustering at the specified parameters.

### Cluster using DBSCAN

Clustering with `dbscan()` returns a dbscan object.

```{r }
db <- dbscan(ruspini_scaled, eps = .32, minPts = 4)
db

str(db)
```

The `cluster` element is the cluster assignment with cluster labels. The 
special cluster label `0` means that the point is noise and not assigned to a 
cluster.

```{r }
ggplot(ruspini_scaled |> add_column(cluster = factor(db$cluster)),
  aes(x, y, color = cluster)) + geom_point()
```

DBSCAN found `r sum(db$cluster == 0)` noise points. `fviz_cluster()` can be 
used to create a ggplot visualization.



```{r }
fviz_cluster(db, ruspini_scaled, geom = "point")
```

Different values for `minPts` and `eps` can lead to vastly different clusterings.
Often, a misspecification leads to all points being noise points or a single cluster.
Also, if the clusters are not well separated, then DBSCAN has a hard 
time splitting them.

## Cluster Evaluation

### Unsupervised Cluster Evaluation

This is also often called internal cluster evaluation since it does not use
extra external labels.

We will evaluate the k-means clustering from above.

```{r }
ggplot(ruspini_scaled, 
       aes(x, y, color = factor(km$cluster))) + 
  geom_point()

d <- dist(ruspini_scaled)
```

The two most popular quality metrics are the within-cluster sum of
squares (WCSS) used as the optimization objective by
[$k$-means](https://en.wikipedia.org/wiki/K-means_clustering) and the
[average silhouette
width](https://en.wikipedia.org/wiki/Silhouette_(clustering)). Look at
`within.cluster.ss` and `avg.silwidth` below.

Some notes about the code: 

* I do not load the package `fpc` using `library()` since it would mask the `dbscan()` function in package `dbscan`. Instead I use the namespace operator `::`. 
* The clustering (second argument below) has to be supplied as a vector
of integers (cluster IDs) and cannot be a factor (to make sure, we can use `as.integer()`).

```{r }
# library(fpc)
fpc::cluster.stats(d, as.integer(km$cluster))
```

The correlation between the distances and a 0-1-vector cluster incidence matrix is
called `pearsongamma`. 
Some numbers are `NULL`. These are measures only available for supervised evaluation.
Read the man page for `cluster.stats()` for an explanation of all the available indices.


We can compare different clusterings.
```{r }
sapply(
  list(
    km_4 = km$cluster,
    hc_compl_4 = cutree(hc, k = 4),
    hc_compl_5 = cutree(hc, k = 5),
    hc_single_5 = cutree(hc_single, k = 5)
  ),
  FUN = function(x)
    fpc::cluster.stats(d, x))[c("within.cluster.ss", "avg.silwidth", "pearsongamma", "dunn"), ]
```

With 4 clusters, k-means and hierarchical clustering produce 
exactly the same clustering. While the two different hierarchical methods with 
5 clusters produce a smaller WCSS, they are actually worse given all three other 
measures.


Next, we look at the silhouette using a 
silhouette plot.

```{r }
library(cluster)
plot(silhouette(km$cluster, d))
```

**Note:** The silhouette plot does not show correctly in R Studio if you
have too many objects (bars are missing). I will work when you open a
new plotting device with `windows()`, `x11()` or `quartz()`.

ggplot visualization using `factoextra`

```{r }
fviz_silhouette(silhouette(km$cluster, d))
```

Unsupervised cluster evaluation can be done by inspecting and visualizing the proximity matrix.
We can inspect the distance matrix between the first 5 objects.

```{r }
as.matrix(d)[1:5, 1:5]
```

A false-color image visualizes each value in the matrix as a pixel with
the color representing the value.

```{r }
library(seriation)
ggpimage(d)
```

Rows and columns are the objects as they are ordered in the data set.
The diagonal represents the distance between an object and itself and
has by definition a distance of 0 (dark line). Visualizing the unordered
distance matrix does not show much structure, but we can reorder the
matrix (rows and columns) using the k-means cluster labels from cluster
1 to 4. A clear block structure representing the clusters becomes
visible.

```{r }
ggpimage(d, order=order(km$cluster))
```

Plot function `dissplot` in package **seriation** rearranges the matrix
and adds lines and cluster labels. In the lower half of the plot, it
shows average dissimilarities between clusters. The function organizes
the objects by cluster and then reorders clusters and objects within
clusters so that more similar objects are closer together.

```{r }
ggdissplot(d, labels = km$cluster, 
         options = list(main = "k-means with k=4"))
```

The reordering by `dissplot` makes the misspecification of k visible as
blocks.

```{r }
ggdissplot(d, 
         labels = kmeans(ruspini_scaled, centers = 3)$cluster)

ggdissplot(d, 
         labels = kmeans(ruspini_scaled, centers = 9)$cluster)
```

Using `factoextra`

```{r }
fviz_dist(d)
```



### Determining the Correct Number of Clusters

The user needs to specify the number of clusters for most clustering algorithms.
[Determining the number of clusters in a data set](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)
is therefor an important task before we can cluster data. We will apply different methods
to the scaled Ruspini data set.


```{r }
ggplot(ruspini_scaled, aes(x, y)) + geom_point()

## We will use different methods and try 1-10 clusters.
set.seed(1234)
ks <- 2:10
```

#### Elbow Method: Within-Cluster Sum of Squares

A method often used for k-means is to calculate the within-cluster sum of squares for different numbers of
clusters and look for the [knee or
elbow](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) in the
plot. We use `nstart = 5` to restart k-means 5 times and return the best
solution.

```{r }
WCSS <- sapply(ks, FUN = function(k) {
  kmeans(ruspini_scaled, centers = k, nstart = 5)$tot.withinss
  })

ggplot(tibble(ks, WCSS), aes(ks, WCSS)) + 
  geom_line() +
  geom_vline(xintercept = 4, color = "red", linetype = 2)
```

#### Average Silhouette Width

Another popular method (often preferred for clustering methods starting with a distance matrix) 
is to plot the average silhouette width for different numbers of clusters and
look for the maximum in the plot.

```{r }
ASW <- sapply(ks, FUN=function(k) {
  fpc::cluster.stats(d, 
                     kmeans(ruspini_scaled, 
                            centers = k, 
                            nstart = 5)$cluster)$avg.silwidth
  })

best_k <- ks[which.max(ASW)]
best_k

ggplot(tibble(ks, ASW), aes(ks, ASW)) + 
  geom_line() +
  geom_vline(xintercept = best_k, color = "red", linetype = 2)
```

#### Dunn Index

The [Dunn index](https://en.wikipedia.org/wiki/Dunn_index) is another
internal measure given by the smallest separation between clusters scaled by 
the largest cluster diameter.

```{r }
DI <- sapply(ks, FUN = function(k) {
  fpc::cluster.stats(d, 
                     kmeans(ruspini_scaled, centers = k, 
                            nstart = 5)$cluster)$dunn
})

best_k <- ks[which.max(DI)]
ggplot(tibble(ks, DI), aes(ks, DI)) + 
  geom_line() +
  geom_vline(xintercept = best_k, color = "red", linetype = 2)
```

#### Gap Statistic

The [Gap statistic](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_gap_statistics)Compares the change in within-cluster dispersion with that expected from
a null model (see `clusGap()`). The default method is to choose the
smallest k such that its value Gap(k) is not more than 1 standard error
away from the first local maximum.

```{r }
library(cluster)
k <- clusGap(ruspini_scaled, 
             FUN = kmeans,  
             nstart = 10, 
             K.max = 10)
k
plot(k)
```


There have been many other methods and indices proposed to determine the
number of clusters. See, e.g., package
[NbClust](https://cran.r-project.org/package=NbClust).

### Clustering Tendency

Most clustering algorithms will always produce a clustering, even if the
data does not contain a cluster structure. It is typically good to check
cluster tendency before attempting to cluster the data.

We use again the smiley data.

```{r }
library(mlbench)
shapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)$x
colnames(shapes) <- c("x", "y")
shapes <- as_tibble(shapes)
```

#### Scatter plots

The first step is visual inspection using scatter plots.

```{r }
ggplot(shapes, aes(x = x, y = y)) + geom_point()
```

Cluster tendency is typically indicated by several separated point
clouds. Often an appropriate number of clusters can also be visually
obtained by counting the number of point clouds. We see four clusters,
but the mouth is not convex/spherical and thus will pose a problems to
algorithms like k-means.

If the data has more than two features then you can use a pairs plot
(scatterplot matrix) or look at a scatterplot of the first two principal
components using PCA. 

#### Visual Analysis for Cluster Tendency Assessment (VAT)

VAT reorders the objects to show potential clustering tendency as a
block structure (dark blocks along the main diagonal). We scale the data
before using Euclidean distance.

```{r }
library(seriation)

d_shapes <- dist(scale(shapes))
ggVAT(d_shapes)
```

iVAT uses the largest distances for all possible paths between two
objects instead of the direct distances to make the block structure
better visible.

```{r }
ggiVAT(d_shapes)
```

#### Hopkins statistic

`factoextra` can also create a VAT plot and calculate the [Hopkins
statistic](https://en.wikipedia.org/wiki/Hopkins_statistic) to assess
clustering tendency. For the Hopkins statistic, a sample of size $n$ is
drawn from the data and then compares the nearest neighbor distribution
with a simulated dataset drawn from a random uniform distribution (see
[detailed
explanation](https://www.datanovia.com/en/lessons/assessing-clustering-tendency/#statistical-methods)).
A values \>.5 indicates usually a clustering tendency.

```{r }
get_clust_tendency(shapes, n = 10)
```

Both plots show a strong cluster structure with 4 clusters.

#### Data Without Clustering Tendency

```{r }
data_random <- tibble(x = runif(500), y = runif(500))
ggplot(data_random, aes(x, y)) + geom_point()
```

No point clouds are visible, just noise.

```{r }
d_random <- dist(data_random)
ggVAT(d_random)
ggiVAT(d_random)
get_clust_tendency(data_random, n = 10, graph = FALSE)
```

There is very little clustering structure visible indicating low
clustering tendency and clustering should not be performed on this data.
However, k-means can be used to partition the data into $k$ regions of
roughly equivalent size. This can be used as a data-driven
discretization of the space.

#### k-means on Data Without Clustering Tendency

What happens if we perform k-means on data that has no inherent
clustering structure?

```{r }
km <- kmeans(data_random, centers = 4)

random_clustered<- data_random |> 
  add_column(cluster = factor(km$cluster))
ggplot(random_clustered, aes(x = x, y = y, color = cluster)) + 
  geom_point()
```

k-means discretizes the space into similarly sized regions.


### Supervised Cluster Evaluation

Also called external cluster validation since it uses ground truth information.
That is, the
user has an idea how the data should be grouped. This could be a known
class label not provided to the clustering algorithm.

We use an artificial data set with known groups.

```{r }
library(mlbench)
set.seed(1234)
shapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)
plot(shapes)
```

First, we prepare the data and hide the known class label which we will used 
for evaluation after clustering.

```{r }
truth <- as.integer(shapes$class)
shapes <- shapes$x
colnames(shapes) <- c("x", "y")

shapes <- shapes |> scale() |> as_tibble()

ggplot(shapes, aes(x, y)) + geom_point()
```

Find optimal number of Clusters for k-means

```{r }
ks <- 2:20
```

Use within sum of squares (look for the knee)

```{r }
WCSS <- sapply(ks, FUN = function(k) {
  kmeans(shapes, centers = k, nstart = 10)$tot.withinss
})

ggplot(tibble(ks, WCSS), aes(ks, WCSS)) + geom_line()
```

Looks like it could be 7 clusters?

```{r }
km <- kmeans(shapes, centers = 7, nstart = 10)

ggplot(shapes |> add_column(cluster = factor(km$cluster)), 
       aes(x, y, color = cluster)) +
  geom_point()
```

The mouth is an issue for k-means. We could use hierarchical clustering with single-linkage because of the mouth is
non-convex and chaining may help.

```{r }
d <- dist(shapes)
hc <- hclust(d, method = "single")
```

Find optimal number of clusters

```{r }
ASW <- sapply(ks, FUN = function(k) {
  fpc::cluster.stats(d, cutree(hc, k))$avg.silwidth
})

ggplot(tibble(ks, ASW), aes(ks, ASW)) + geom_line()
```

The maximum is clearly at 4 clusters.

```{r }
hc_4 <- cutree(hc, 4)

ggplot(shapes |> add_column(cluster = factor(hc_4)), 
       aes(x, y, color = cluster)) +
  geom_point()
```

Compare with ground truth with the [adjusted Rand index
(ARI, also cpp!C00L
corrected Rand index)](https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index),
the [variation of information (VI)
index](https://en.wikipedia.org/wiki/Variation_of_information),
[mutual information (MI)](https://en.wikipedia.org/wiki/Mutual_information),
[entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))
and
[purity](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation).

`cluster_stats()` computes ARI and VI as comparative measures. I define
functions for the purity and the entropy of a clustering given the ground truth here:

```{r }
purity <- function(cluster, truth, show_table = FALSE) {
  if (length(cluster) != length(truth))
    stop("Cluster vector and ground truth vectors are not of the same length!")
  
  # tabulate
  tbl <- table(cluster, truth)
  if(show_table)
    print(tbl)
  
  # find majority class
  majority <- apply(tbl, 1, max)
  sum(majority) / length(cluster)
}

entropy <- function(cluster, truth, show_table = FALSE) {
  if (length(cluster) != length(truth))
    stop("Cluster vector and ground truth vectors are not of the same length!")
  
  # calculate membership probability of cluster to class
  tbl <- table(cluster, truth)
  p <- sweep(tbl, 2, colSums(tbl), "/")
  
  if(show_table)
    print(p)

  # calculate cluster entropy
  e <- -p * log(p, 2)
  e <- rowSums(e, na.rm = TRUE)
  
  # weighted sum over clusters
  w <- table(cluster) / length(cluster)
  sum(w * e)
}
```

We calculate the measures and add for comparison two random "clusterings" with
4 and 6 clusters.

```{r }
random_4 <- sample(1:4, nrow(shapes), replace = TRUE)
random_6 <- sample(1:6, nrow(shapes), replace = TRUE)

r <- rbind(
  truth = c(
    unlist(fpc::cluster.stats(d, truth, 
                              truth, compareonly = TRUE)),
    purity = purity(truth, truth),
    entropy = entropy(truth, truth)
  ),
  
  kmeans_7 = c(
    unlist(fpc::cluster.stats(d, km$cluster, 
                              truth, compareonly = TRUE)),
    purity = purity(km$cluster, truth),
    entropy = entropy(km$cluster, truth)
    ),
  hc_4 = c(
    unlist(fpc::cluster.stats(d, hc_4, 
                              truth, compareonly = TRUE)),
    purity = purity(hc_4, truth),
    entropy = entropy(hc_4, truth)
    ),
  random_4 = c(
    unlist(fpc::cluster.stats(d, random_4, 
                              truth, compareonly = TRUE)),
    purity = purity(random_4, truth),
    entropy = entropy(random_4, truth)
    ),
  random_6 = c(
    unlist(fpc::cluster.stats(d, random_6, 
                              truth, compareonly = TRUE)),
    purity = purity(random_6, truth),
    entropy = entropy(random_6, truth)
    )
  )
r
```

Notes:

-   Comparing the ground truth to itself produces perfect scores.
-   Hierarchical clustering found the perfect clustering.
-   Entropy and purity are heavily impacted by the number of clusters
    (more clusters improve the metric) and the fact that the mouth has 41.8% 
    of all the data points becoming automatically the majority class for purity.
-   The adjusted rand index shows clearly that the random clusterings
    have no relationship with the ground truth (very close to 0). This
    is a very helpful property and explains why the ARI is the most popular 
    measure.

Read the manual page for `fpc::cluster.stats()` for an explanation of all the available indices.


## More Clustering Algorithms*

**Note:** Some of these methods are covered in Chapter 8 of the textbook.

### Partitioning Around Medoids (PAM)

[PAM](https://en.wikipedia.org/wiki/K-medoids) tries to solve the
$k$-medoids problem. The problem is similar to $k$-means, but uses
medoids instead of centroids to represent clusters. Like hierarchical
clustering, it typically works with a precomputed distance matrix. An
advantage is that you can use any distance metric not just Euclidean
distances. **Note:** The medoid is the most central data point in the
middle of the cluster. PAM is a lot more computationally expensive compared to 
k-means.

```{r }
library(cluster)

d <- dist(ruspini_scaled)
str(d)

p <- pam(d, k = 4)
p
```

Extract the clustering and medoids for visualization.

```{r }
ruspini_clustered <- ruspini_scaled |> 
  add_column(cluster = factor(p$cluster))

medoids <- as_tibble(ruspini_scaled[p$medoids, ], 
                     rownames = "cluster")
medoids

ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + 
  geom_point() +
  geom_point(data = medoids, aes(x = x, y = y, color = cluster), 
             shape = 3, size = 10)
```


Alternative visualization using `fviz_cluster()`.
```{r }
## __Note:__ `fviz_cluster` needs the original data.
fviz_cluster(c(p, list(data = ruspini_scaled)), 
             geom = "point", 
             ellipse.type = "norm")
```

### Gaussian Mixture Models

```{r }
library(mclust)
```

[Gaussian mixture
models](https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model)
assume that the data set is the result of drawing data from a set of
Gaussian distributions where each distribution represents a cluster.
Estimation algorithms try to identify the location parameters of the
distributions and thus can be used to find clusters. `Mclust()` uses
Bayesian Information Criterion (BIC) to find the number of clusters
(model selection). BIC uses the likelihood and a penalty term to guard
against overfitting.

```{r }
m <- Mclust(ruspini_scaled)
summary(m)
plot(m, what = "classification")
```

Rerun with a fixed number of 4 clusters

```{r }
m <- Mclust(ruspini_scaled, G=4)
summary(m)
plot(m, what = "classification")
```

### Spectral clustering

[Spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering)
works by embedding the data points of the partitioning problem into the
subspace of the k largest eigenvectors of a normalized affinity/kernel
matrix. Then uses a simple clustering method like k-means.

```{r }
library("kernlab")

cluster_spec <- specc(as.matrix(ruspini_scaled), centers = 4)
cluster_spec

ggplot(ruspini_scaled |> 
         add_column(cluster = factor(cluster_spec)),
       aes(x, y, color = cluster)) + 
  geom_point()
```

### Fuzzy C-Means Clustering

The [fuzzy clustering](https://en.wikipedia.org/wiki/Fuzzy_clustering)
version of the k-means clustering problem. Each data point has a degree
of membership to for each cluster.

```{r }
library("e1071")

cluster_cmeans <- cmeans(as.matrix(ruspini_scaled), centers = 4)
cluster_cmeans
```

Plot membership (shown as small pie charts)

```{r }
library("scatterpie")
ggplot()  +
  geom_scatterpie(
    data = cbind(ruspini_scaled, cluster_cmeans$membership),
    aes(x = x, y = y), 
    cols = colnames(cluster_cmeans$membership), 
    legend_name = "Membership") + 
  coord_equal()
```


## Outliers in Clustering*

Most clustering algorithms perform complete assignment (i.e., all data
points need to be assigned to a cluster). Outliers will affect the
clustering. It is useful to identify outliers and remove strong outliers
prior to clustering. A density based method to identify outlier is
[LOF](https://en.wikipedia.org/wiki/Local_outlier_factor) (Local Outlier
Factor). It is related to dbscan and compares the density around a point
with the densities around its neighbors (you have to specify the
neighborhood size $k$). The LOF value for a regular data point is 1. The
larger the LOF value gets, the more likely the point is an outlier.

```{r }
library(dbscan)
```

Add a clear outlier to the scaled Ruspini dataset that is 10 standard
deviations above the average for the x axis.

```{r }
ruspini_scaled_outlier <- ruspini_scaled |> add_case(x=10,y=0)
```

### Visual inspection of the data

Outliers can be identified using summary statistics, histograms,
scatterplots (pairs plots), and boxplots, etc. We use here a pairs plot
(the diagonal contains smoothed histograms). The outlier is visible as
the single separate point in the scatter plot and as the long tail of
the smoothed histogram for `x`. Remember, in scaled data, we expect most observations to
fall in the range $[-3,3]$.

```{r }
library("GGally")
ggpairs(ruspini_scaled_outlier, progress = FALSE)
```

The outlier messes up the scaling which presents a problem for k-means.

```{r }
km <- kmeans(ruspini_scaled_outlier, centers = 4, nstart = 10)
ruspini_scaled_outlier_km <- ruspini_scaled_outlier|>
  add_column(cluster = factor(km$cluster))
centroids <- as_tibble(km$centers, rownames = "cluster")

ggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + 
  geom_point() +
  geom_point(data = centroids, 
             aes(x = x, y = y, color = cluster), 
             shape = 3, size = 10)
```

This problem can be fixed by increasing the number of clusters and
removing small clusters in a post-processing step or by identifying and
removing outliers before clustering.

### Local Outlier Factor (LOF)

The [Local Outlier
Factor](https://en.wikipedia.org/wiki/Local_outlier_factor) is related
to concepts of DBSCAN can help to identify potential outliers. To calculate
the LOF, a local neighborhood size (MinPts in DBSCAN) for density estimation 
needs to be chosen. I use 10 here since I expect my clusters to have each more 
than 10 points.

```{r }
lof <- lof(ruspini_scaled_outlier, minPts= 10)
lof

ggplot(ruspini_scaled_outlier |> add_column(lof = lof), 
       aes(x, y, color = lof)) +
  geom_point() + 
  scale_color_gradient(low = "gray", high = "red")
```

Plot the points sorted by increasing LOF and look for a knee.

```{r }
ggplot(tibble(index = seq_len(length(lof)), lof = sort(lof)), 
       aes(index, lof)) +
  geom_line() +
  geom_hline(yintercept = 1, color = "red", linetype = 2)
```

We choose here a threshold above 2.

```{r }
ggplot(ruspini_scaled_outlier |> add_column(outlier = lof >= 2), 
       aes(x, y, color = outlier)) +
  geom_point()
```

You should analyze the found outliers. They are often interesting and important 
data points. Perform clustering on the regular data without outliers.

```{r}
ruspini_scaled_clean <- ruspini_scaled_outlier  |> filter(lof < 2)

km <- kmeans(ruspini_scaled_clean, centers = 4, nstart = 10)
ruspini_scaled_clean_km <- ruspini_scaled_clean|>
  add_column(cluster = factor(km$cluster))
centroids <- as_tibble(km$centers, rownames = "cluster")

ggplot(ruspini_scaled_clean_km, aes(x = x, y = y, color = cluster)) + 
  geom_point() +
  geom_point(data = centroids, 
             aes(x = x, y = y, color = cluster), 
             shape = 3, size = 10)
```

There are many other outlier removal strategies available. See, e.g.,
package [outliers](https://cran.r-project.org/package=outliers).

## Exercises*

We will again use the Palmer penguin data for the exercises.

```{r }
library(palmerpenguins)
head(penguins)
```

Create a R markdown file with the code and discussion for the following below.

1. What features do you use for clustering? What about missing values?
  Discuss your answers. Do you need to scale the data before clustering? Why?
2. What distance measure do you use to reflect similarities between penguins? 
  See [Measures of Similarity and Dissimilarity] in Chapter 2.
3. Apply k-means clustering.  Use an appropriate method to determine the number 
  of clusters. Compare the clustering using unscaled data and 
  scaled data. What is the difference? Visualize and describe the results. 
4. Apply hierarchical clustering.
  Create a dendrogram and discuss what it means.
5. Apply DBSCAN. How do you choose the parameters? How well does it work? 



