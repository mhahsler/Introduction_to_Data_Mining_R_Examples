---
editor_options:
  markdown:
    wrap: 72
---

# Data

This chapter provides examples for cleaning and preparing data for data
mining. In addition, data exploration is covered in this chapter.

Install the packages used in this chapter:

```{r setup_02}
pkgs <- c("arules", "caret", "factoextra", "GGally", 
          "ggcorrplot", "hexbin", "palmerpenguins", "plotly", 
          "proxy", "sampling", "seriation", "tidyverse")

pkgs_install <- pkgs[!(pkgs %in% installed.packages()[,"Package"])]
if(length(pkgs_install)) install.packages(pkgs_install)
```

The packages used for this chapter are: `r format_pkgs(pkgs)`

```{r setup_02-2, include=FALSE}
source("_common.R")
source("format_pkgs.R")
all_pkgs <- union(all_pkgs, pkgs)
```


```{r include=FALSE}
options(digits = 3)
```

Data for data mining is typically organized in tabular form, with rows containing
the objects of interest and columns representing attributes describing the objects.
We will discuss topics like data quality, sampling, feature selection,
and how to measure similarities between objects and features. 
The second part of this chapter deals with data exploration and visualization. 

## Types of Data

### Attributes and Measurement

The values of features can be measured on [several scales](https://en.wikipedia.org/wiki/Level_of_measurement) 
ranging from 
simple labels all the way to numbers. The scales come in four levels.

| Scale Name | Description | Operations | Statistics | R |
| ---------- | ------------| --------- | -------| ------- |
| Nominal | just a label (e.g., red, green) | $==, !=$ | counts | `factor` |
| Ordinal | label with order (e.g., small, med., large) | $<, >$ | median | `ordered` | 
| Interval | difference between two values is meaningful (regular number) | $+, -$ | mean, sd | `numeric` |
| Ratio | has a natural zero (e.g., count, distance) | $/, *$ | percent | `numeric` |

The scales build on each other meaning that an ordinal variable also has the characteristics of 
a nominal variable with the added order information.
We often do not differentiate between interval and ratio scale because we rarely 
not need to calculate percentages or other statistics that require a meaningful zero value.
The following code gives example vectors with nominal, ordinal (`levels` defines the order) 
and interval scale.  

```{r}
factor(c("red", "green", "green", "blue"))

factor("S", "L", "M", "S", levels = c("S", "M", "L"), ordered = TRUE)

c(1, 2, 3, 4, 3)
```



### The Iris Dataset

We will use a toy dataset that comes with R. [Fisher's iris
dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) gives the
measurements in centimeters of the variables sepal length, sepal width
petal length, and petal width representing the features for 150 flowers (the objects). 
The dataset contains 50
flowers from each of 3 species of iris. The species are Iris Setosa,
Iris Versicolor, and Iris Virginica. For more details see `? iris`.

We load the iris data set. Datasets that come with R or R packages can
be loaded with `data()`. The standard format for data in R is a
data.frame. We convert the data.frame into a tidyverse tibble.

```{r }
library(tidyverse)
data(iris)
iris <- as_tibble(iris)
iris
```

We see that the data contains 150 rows (flowers) and 5 features. tibbles
only show the first few rows and do not show all features, if they do
not fit the screen width. We can call `print` and define how many rows
to show using parameter `n` and force print to show all features by
changing the `width` to infinity.

```{r }
print(iris, n = 3, width = Inf)
```

## Data Quality

Assessing the quality of the available data is crucial before we start
using the data. Start with summary statistics for each column to
identify outliers and missing values. The easiest way is to use the base R
function `summary()`.

```{r }
summary(iris)
```

You can also summarize 
individual columns using tidyverse's dplyr functions.

```{r }
iris |> 
  summarize(mean = mean(Sepal.Length))
```

Using `across()`, multiple columns can be summarized. Un the following,
we calculate all numeric columns using the `mean` function.

```{r }
iris |> 
  summarize(across(where(is.numeric), mean))
```

To find outliers or data problems, you need to look for very small
values (often a suspicious large number of zeros) using min and for
extremely large values using max. Comparing median and mean tells us if
the distribution is symmetric.

A visual method to inspect the data is to use a scatterplot matrix (we
use here `ggpairs()` from package `GGally`). In this plot, we can
visually identify noise data points and outliers (points that are far
from the majority of other points).

```{r }
library(GGally)
ggpairs(iris, aes(color = Species), progress = FALSE)
```

This useful visualization combines many visualizations used to understand the
data and check for quality issues. Rows and columns are the features in the data.
We have also specified the aesthetic that we want to group each species using a different color.

* The visualizations in the diagonal panels show the
smoothed histograms with the distribution for each feature. 
The plot tries to pick a good number of bins for the histogram 
(see messages above). The distribution can be checked if it is close to normal,
unimodal or highly skewed. Also, we can see if the different groups are overlapping 
or separable for each feature. For example, the three distributions for `Sepal.Width`
are almost identical meaning that it is hard to distinguish between 
the different species using this feature alone. `Petal.Lenght` and `Petal.Width`
are much better.

* The lower-left triangle panels contain scatterplots for all pairs features. These 
are useful to see if there if features are correlated (the pearson correlation 
coefficient if printed in the upper-right triangle). For example,
`Petal.Length` and `Petal.Width` are highly correlated overall 
This makes sense since larger plants will have both longer and wider petals.
Inside the Setosa group this correlation it is a lot weaker.
We can also see if groups are 
well separated using projections on two variables. Almost all panels show that
Setosa forms a point cloud well separated from the other two classes while 
Versicolor and Virginica overlap. 
We can also see outliers that are far from the other data points in its group.
See if you can spot the one red dot that is far away from all others.

* The last row/column represents in this data set the class label Species. 
It is a nominal variable so the plots are different. The bottom row panels 
show (regular) histograms. The last column shows boxplots to represent
the distribution of the different features by group. Dots represent 
outliers. Finally, the bottom-right panel contains the counts for the different 
groups as a barplot. In this data set, each group has the same number of observations.

Many data mining methods require complete data, that is the data cannot
contain missing values (`NA`). To remove missing values and duplicates
(identical data points which might be a mistake in the data), we often
do this:

```{r }
clean.data <- iris |> 
  drop_na() |> 
  unique()

summary(clean.data)
```

Note that one non-unique case is gone leaving only 149 flowers. The data
did not contain missing values, but if it did, they would also have been
dropped. Typically, you should spend a lot more time on data cleaning.

## Data Preprocessing

### Aggregation

Data often contains groups and we want to compare these groups. We group
the iris dataset by species and then calculate a summary statistic for
each group.

```{r }
iris |> 
  group_by(Species) |> 
  summarize(across(everything(), mean))

iris |> 
  group_by(Species) |> 
  summarize(across(everything(), median))
```

Using this information, we can compare how features differ between
groups.

### Sampling

[Sampling](https://en.wikipedia.org/wiki/Sampling_(statistics)) is often
used in data mining to reduce the dataset size before modeling or
visualization.

#### Random Sampling

The built-in sample function can sample from a vector. Here we sample
with replacement.

```{r }
sample(c("A", "B", "C"), size = 10, replace = TRUE)
```

We often want to sample rows from a dataset. This can be done by
sampling without replacement from a vector with row indices (using the
functions `seq()` and `nrow()`). The sample vector is then used to
subset the rows of the dataset.

```{r }
take <- sample(seq(nrow(iris)), size = 15)
take

iris[take, ]
```

`dplyr` from tidyverse lets us sample rows from tibbles directly using
`slice_sample()`. I set the random number generator seed to make the
results reproducible.

```{r }
set.seed(1000)
s <- iris |> 
  slice_sample(n = 15)

library(GGally)
ggpairs(s, aes(color = Species), progress = FALSE)
```

#### Stratified Sampling

[Stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling)
is a method of sampling from a population which can be partitioned into
subpopulations, while controlling the proportions of the subpopulation
in the resulting sample.

In the following, the subpopulations are the different types of species
and we want to make sure to sample the same number (5) flowers from
each. The library `sampling` provides a function for stratified
sampling. The column `ID_unit` in the resulting data.frame contains the
row numbers of the sampled rows. We can use `slice()` from `dplyr` to
select the sampled rows.

```{r }
library(sampling)
id2 <- strata(iris, stratanames = "Species", 
              size = c(5,5,5), method = "srswor")
id2

s2 <- iris |> 
  slice(id2$ID_unit)

ggpairs(s2, aes(color = Species), progress = FALSE)
```

### Dimensionality Reduction

The number of features is often called the dimensional of the data following the 
idea that each feature (at least the numeric features) can be seen as an axis of the data.

Common feature preprocessing includes 
[dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)
which tries to represent high-dimensional data in a low-dimensional space so that the 
low-dimensional representation retains some meaningful properties (e.g., information about
similarity or distances) of the original data. A special case is feature selection.

Other important feature preprocessing includes discretization and feature scaling.


#### Principal Components Analysis (PCA)

[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)
calculates principal components (a set of new orthonormal basis vectors
in the data space) from data points such that the first principal
component explains the most variability in the data, the second the next
most and so on. In data analysis, PCA is used to project
high-dimensional data points onto the first few (typically two)
principal components for visualization as a scatter plot and as
preprocessing for modeling (e.g., before k-means clustering). Points
that are closer together in the high-dimensional original space, tend
also be closer together when projected into the lower-dimensional space,

We can use an interactive 3-d plot (from package `plotly`) to look at
three of the four dimensions of the iris dataset. Note that it is hard
to visualize more than 3 dimensions.

```{r }
# library(plotly) # I don't load the package because it's namespace clashes with select in dplyr.
plotly::plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, z = ~Sepal.Width, 
      color = ~Species, size = 1) |> 
  plotly::add_markers()
```

The principal components can be calculated from a matrix using the
function `prcomp()`. We select all numeric columns (by unselecting the
species column) and convert the tibble into a matrix before the
calculation.

```{r }
pc <- iris |> 
  select(-Species) |> 
  as.matrix() |> 
  prcomp()
summary(pc)
```

How important is each principal component can also be seen using a
[scree plot](https://en.wikipedia.org/wiki/Scree_plot). The plot
function for the result of the `prcomp` function visualizes how much
variability in the data is explained by each additional principal
component.

```{r }
plot(pc, type = "line")
```

Note that the first principal component (PC1) explains most of the
variability in the iris dataset.

To find out what information is stored in the object `pc`, we can
inspect the raw object (display *str*ucture).

```{r }
str(pc)
```

The object `pc` (like most objects in R) is a list with a class
attribute. The list element `x` contains the data points projected on
the principal components. We can convert the matrix into a tibble and
add the species column from the original dataset back (since the rows
are in the same order), and then display the data projected on the first
two principal components.

```{r }
iris_projected <- as_tibble(pc$x) |> 
  add_column(Species = iris$Species)

ggplot(iris_projected, aes(x = PC1, y = PC2, color = Species)) + 
  geom_point()
```

Flowers that are displayed close together in this projection are also
close together in the original 4-dimensional space. Since the first
principal component represents most of the variability, we can also show
the data projected only on PC1.

```{r }
ggplot(iris_projected, 
  aes(x = PC1, y = 0, color = Species)) + 
  geom_point() +
  scale_y_continuous(expand=c(0,0)) +
  theme(axis.text.y = element_blank(),
      axis.title.y = element_blank()
  )
```

We see that we can perfectly separate the species Setosa using just the
first principal component. The other two species are harder to separate.

A plot of the projected data with the original axes added as arrows is
called a [biplot](https://en.wikipedia.org/wiki/Biplot). If the arrows
(original axes) align roughly with the axes of the projection, then they
are correlated (linearly dependent).

```{r }
library(factoextra)
fviz_pca(pc)
```

We can also display only the old and new axes.

```{r }
fviz_pca_var(pc)
```

We see Petal.Width and Petal.Length point in the same direction which
indicates that they are highly correlated. They are also roughly aligned
with PC1 (called Dim1 in the plot) which means that PC1 represents most
of the variability of these two variables. Sepal.Width is almost aligned
with the y-axis and therefore it is represented by PC2 (Dim2).
Petal.Width/Petal.Length and Sepal.Width are almost at 90 degrees,
indicating that they are close to uncorrelated. Sepal.Length is
correlated to all other variables and represented by both, PC1 and PC2
in the projection.

There exist other methods to embed data from higher dimensions into a
lower-dimensional space. A popular method to project data into lower
dimensions for visualization is **t-distributed stochastic neighbor
embedding (t-SNE)** available in package `Rtsne`.

#### Multi-Dimensional Scaling (MDS)

[MDS](https://en.wikipedia.org/wiki/Multidimensional_scaling) is similar
to PCA. Instead of data points, it starts with pairwise distances (i.e.,
a distance matrix) and produces a space where points are placed to
represent these distances as well as possible. The axes in this space
are called components and are similar to the principal components in
PCA.

First, we calculate a distance matrix (Euclidean distances) from the 4-d
space of the iris dataset.

```{r }
d <- iris |> 
  select(-Species) |> 
  dist()
```

Metric (classic) MDS tries to construct a space where points with lower
distances are placed closer together. We project the data represented by
a distance matrix on `k = 2` dimensions.

```{r }
fit <- cmdscale(d, k = 2)
colnames(fit) <- c("comp1", "comp2")
fit <- as_tibble(fit) |> 
  add_column(Species = iris$Species)

ggplot(fit, aes(x = comp1, y = comp2, color = Species)) + geom_point()
```

The resulting projection is similar (except for rotation and reflection)
to the result of the projection using PCA.

#### Non-Parametric Multidimensional Scaling

Non-parametric multidimensional scaling performs MDS while relaxing the
need of linear relationships. Methods are available in package `MASS` as
functions `isoMDS()` and `sammon()`.

### Feature Subset Selection

Feature selection is the process of identifying the features that are
used to create a model. We will talk about feature selection when we
discuss classification models in Chapter 3 in [Feature Selection*].

### Discretization

Some data mining methods require discrete data. Discretization converts
continuous features into discrete features. As an example, we will
discretize the continuous feature Petal.Width. Before we perform
discretization, we should look at the distribution and see if it gives
us an idea how we should group the continuous values into a set of
discrete values. A histogram visualizes the distribution of a single
continuous feature.

```{r }
ggplot(iris, aes(x = Petal.Width)) + geom_histogram(binwidth = .2)
```

The bins in the histogram represent a discretization using a fixed bin
width. The R function `cut()` performs equal interval width
discretization.

```{r }
iris |> 
  pull(Sepal.Width) |> 
  cut(breaks = 3)
```

Other discretization methods include equal frequency discretization or
using k-means clustering. These methods are implemented by several R
packages. We use here the implementation in package `arules` and
visualize the results as histograms with blue lines to separate
intervals assigned to each discrete value.

```{r }
library(arules)
iris |> pull(Petal.Width) |> 
  discretize(method = "interval", breaks = 3)

iris |> pull(Petal.Width) |> 
  discretize(method = "frequency", breaks = 3)

iris |> pull(Petal.Width) |> 
  discretize(method = "cluster", breaks = 3)

ggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +
  geom_vline(xintercept = iris |> pull(Petal.Width) |> 
        discretize(method = "interval", breaks = 3, onlycuts = TRUE),
    color = "blue") +
  labs(title = "Discretization: interval", 
       subtitle = "Blue lines are boundaries")

ggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +
  geom_vline(xintercept = iris |> pull(Petal.Width) |> 
        discretize(method = "frequency", breaks = 3, onlycuts = TRUE),
    color = "blue") +
  labs(title = "Discretization: frequency", 
       subtitle = "Blue lines are boundaries")

ggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +
  geom_vline(xintercept = iris |> pull(Petal.Width) |> 
               discretize(method = "cluster", breaks = 3, onlycuts = TRUE),
    color = "blue") +
  labs(title = "Discretization: cluster", 
       subtitle = "Blue lines are boundaries")
```

The user needs to decide on the number of intervals and the used method.

### Variable Transformation: Standardization

Standardizing (scaling, normalizing) the range of features values is
important to make them comparable. The most popular method is to convert
the values of each feature to
[z-scores](https://en.wikipedia.org/wiki/Standard_score). by subtracting
the mean (centering) and dividing by the standard deviation (scaling).
The standardized feature will have a mean of zero and are measured in
standard deviations from the mean. Positive values indicate how many
standard deviation the original feature value was above the average.
Negative standardized values indicate below-average values.

Tidyverse currently does not have a simple scale function, so we
make one that provides a wrapper for the standard scale function in R.
It mutates all numeric columns using an anonymous function
that uses the base R scale function and converts the result into a vector.

```{r }
scale_numeric <- function(x) 
  x |> 
  mutate(across(where(is.numeric), function(y) as.vector(scale(y))))
```

```{r }
iris.scaled <- iris |> 
  scale_numeric()
iris.scaled
summary(iris.scaled)
```

The standardized feature has a mean of zero and most "normal" values
will fall in the range $[-3,3]$ and is measured in standard deviations from the average.
Negative values mean smaller than the average and positive values mean larger than the average.

## Measures of Similarity and Dissimilarity

Proximities help with quantifying how similar two objects are. 
[Similariy](https://en.wikipedia.org/wiki/Similarity_(geometry)) is a concept from geometry.
The best-known 
way to define similarity is Euclidean distance, but proximities can be measured in
different ways depending on the information we have about the objects.

R stores proximity as dissimilarities/distances matrices. Similarities
are first converted to dissimilarities. Distances are symmetric, i.e.,
the distance from A to B is the same as the distance from B to A. R
therefore stores only a triangle (typically the lower triangle) of the
distance matrix.

### Minkowsky Distances

The [Minkowsky
distance](https://en.wikipedia.org/wiki/Minkowski_distance) is a family
of metric distances including Euclidean and Manhattan distance. To avoid
one feature to dominate the distance calculation, scaled data is
typically used. We select the first 5 flowers for this example.

```{r }
iris_sample <- iris.scaled |> 
  select(-Species) |> 
  slice(1:5)
iris_sample
```

Different types of Minkowsky distance matrices between the first 5
flowers can be calculated using `dist()`.

```{r }
dist(iris_sample, method = "euclidean")
dist(iris_sample, method = "manhattan")
dist(iris_sample, method = "maximum")
```

We see that only the lower triangle of the distance matrices are stored
(note that rows start with row 2).

### Distances for Binary Data

Binary data can be encodes as `0` and `1` (numeric) or `TRUE` and
`FALSE` (logical).

```{r }
b <- rbind(
  c(0,0,0,1,1,1,1,0,0,1),
  c(0,0,1,1,1,0,0,1,0,0)
  )
b

b_logical <- apply(b, MARGIN = 2, as.logical)
b_logical
```

#### Hamming Distance

The [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)
is the number of mismatches between two binary vectors. For 0-1 data
this is equivalent to the Manhattan distance and also the squared
Euclidean distance.

```{r }
dist(b, method = "manhattan")
dist(b, method = "euclidean")^2
```

#### Jaccard Index

The [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) is a
similarity measure that focuses on matching 1s. R converts the
similarity into a dissimilarity using $d_{J} = 1 - s_{J}$.

```{r }
dist(b, method = "binary")
```

### Distances for Mixed Data

Most distance measures work only on numeric data. Often, we have a
mixture of numbers and nominal or ordinal features like this data:

```{r }
people <- tibble(
  height = c(      160,    185,    170),
  weight = c(       52,     90,     75),
  sex    = c( "female", "male", "male")
)
people
```

It is important that nominal features are stored as factors and not
character (`<chr>`).

```{r }
people <- people |> 
  mutate(across(where(is.character), factor))
people
```

#### Gower's Coefficient

The Gower's coefficient of similarity works with mixed data by
calculating the appropriate similarity for each feature and then
aggregating them into a single measure. The package `proxy` implements
Gower's coefficient converted into a distance.

```{r }
library(proxy)
d_Gower <- dist(people, method = "Gower")
d_Gower
```

Gower's coefficient calculation implicitly scales the data because it
calculates distances on each feature individually, so there is no need
to scale the data first.

#### Using Euclidean Distance with Mixed Data

Sometimes methods (e.g., k-means) can only use Euclidean distance. In
this case, nominal features can be converted into 0-1 dummy variables.
After scaling, Euclidean distance will result in a usable distance
measure.

We use package `caret` to create dummy variables.

```{r }
library(caret)
data_dummy <- dummyVars(~., people) |> 
  predict(people)
data_dummy
```

Note that feature sex has now two columns. If we want that height,
weight and sex have the same influence on the distance measure, then we
need to weight the sex columns by 1/2 after scaling.

```{r }
weight_matrix <- matrix(c(1, 1, 1/2, 1/2), ncol = 4, nrow = nrow(data_dummy), byrow = TRUE)
data_dummy_scaled <- scale(data_dummy) * weight_matrix

d_dummy <- dist(data_dummy_scaled)
d_dummy
```

The distance using dummy variables is consistent with Gower's distance.
However, note that Gower's distance is scaled between 0 and 1 while the
Euclidean distance is not.

```{r }
ggplot(tibble(d_dummy, d_Gower), aes(x = d_dummy, y = d_Gower)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

### Additional proximity Measures Available in Package proxy

The package `proxy` implements a wide array of distances.

```{r }
library(proxy)
pr_DB$get_entry_names()
```

Note that loading the package `proxy` replaces the `dist` function in R.
You can specify which dist function to use by specifying the package in
the call. For example `stats::dist()` calls the default function in R
(the package `stats` is part of R) while `proxy::dist()` calls the
version in the package `proxy`.

## Data Exploration*

The following code covers the important part of 
data exploration. For space reasons, this chapter was moved from the
printed textbook to this 
[Data Exploration Web Chapter.](https://www-users.cse.umn.edu/~kumar001/dmbook/data_exploration_1st_edition.pdf)

We will use again the iris dataset.

```{r }
library(tidyverse)
data(iris)
iris <- as_tibble(iris)
iris
```

### Basic statistics

Get summary statistics (using base R)

```{r }
summary(iris)
```

Get mean and standard deviation for sepal length

```{r }
iris |> 
  summarize(avg_Sepal.Length = mean(Sepal.Length), sd_Sepal.Length = sd(Sepal.Length))
```

Data with missing values will result in statistics of `NA`. Adding the
parameter `na.rm = TRUE` can be used in most statistics functions to
ignore missing values.

```{r }
mean(c(1, 2, NA, 3, 4, 5))
mean(c(1, 2, NA, 3, 4, 5),  na.rm = TRUE)
```

Outliers are typically the smallest or the largest values of a feature.
To make the mean more robust to outliers, we can trim 10% of
observations from each end of the distribution.

```{r }
iris |>
  summarize(avg_Sepal.Length = mean(Sepal.Length),
            trimmed_avg_Sepal.Length = mean(Sepal.Length, trim = .1))
```

Sepal length does not have outliers, so the trimmed mean is almost
identical.

To calculate a summary for a set of features (e.g., all numeric
features), tidyverse provides `across(where(is.numeric), fun)`.

```{r }
iris |> summarize(across(where(is.numeric), mean))
iris |> summarize(across(where(is.numeric), sd))

iris |> summarize(across(where(is.numeric), 
            list(min = min, median = median, max = max)))
```

The median absolute deviation (MAD) is another measure of dispersion.

```{r }
iris |> summarize(across(where(is.numeric), mad))
```

### Grouped Operations and Calculations

We can use the nominal feature to form groups and then calculate
group-wise statistics for the continuous features. We often use
group-wise averages to see if they differ between groups.

```{r }
iris |> 
  group_by(Species) |> 
  summarize(across(Sepal.Length, mean))
iris |> 
  group_by(Species) |> 
  summarize(across(where(is.numeric), mean))
```

We see that the species Virginica has the highest average for all, but
Sepal.Width.

The statistical difference between the groups can be tested using [ANOVA
(analysis of
variance)](http://www.sthda.com/english/wiki/one-way-anova-test-in-r).

```{r }
res.aov <- aov(Sepal.Length ~ Species, data = iris)
summary(res.aov)
TukeyHSD(res.aov)
```

The summary shows that there is a significant difference for
Sepal.Length between the groups. `TukeyHDS` evaluates differences
between pairs of groups. In this case, all are significantly different.
If the data only contains two groups, the `t.test` can be used.

### Tabulate data

We can count the number of flowers for each species.

```{r }
iris |> 
  group_by(Species) |> 
  summarize(n())
```

In base R, this can be also done using `count(iris$Species)`.

For the following examples, we discretize the data using cut.

```{r }
iris_ord <- iris |> mutate(across(where(is.numeric),  
  function(x) cut(x, 3, labels = c("short", "medium", "long"), ordered = TRUE)))

iris_ord
summary(iris_ord)
```

Cross tabulation is used to find out if two discrete features are
related.

```{r }
tbl <- iris_ord |> 
  select(Sepal.Length, Species) |> 
  table()
tbl
```

The table contains the number of rows that contain the combination of
values (e.g., the number of flowers with a short Sepal.Length and
species Setosa is `r tbl[1, 1]`). If a few cells have very large counts
and most others have very low counts, then there might be a
relationship. For the iris data, we see that species Setosa has mostly a
short Sepal.Length, while Versicolor and Virginica have longer sepals.

Creating a cross table with tidyverse is a little more involved and uses
pivot operations and grouping.

```{r }
iris_ord |>
  select(Species, Sepal.Length) |>
### Relationship Between Nominal and Ordinal Features
  pivot_longer(cols = Sepal.Length) |>
  group_by(Species, value) |> 
  count() |> 
  ungroup() |>
  pivot_wider(names_from = Species, values_from = n)
```

We can use a statistical test to determine if there is a significant
relationship between the two features. [Pearson's chi-squared
test](https://en.wikipedia.org/wiki/Chi-squared_test) for independence
is performed with the null hypothesis that the joint distribution of the
cell counts in a 2-dimensional contingency table is the product of the
row and column marginals. The null hypothesis h0 is independence between
rows and columns.

```{r }
tbl |> 
  chisq.test()
```

The small p-value indicates that the null hypothesis of independence
needs to be rejected. For small counts (cells with counts \<5),
[Fisher's exact
test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test) is better.

```{r }
fisher.test(tbl)
```

### Percentiles (Quantiles)

[Quantiles](https://en.wikipedia.org/wiki/Quantile) are cutting points
dividing the range of a probability distribution into continuous
intervals with equal probability. For example, the median is the
empirical 50% quantile dividing the observations into 50% of the
observations being smaller than the median and the other 50% being
larger than the median.

By default quartiles are calculated. 25% is typically called Q1, 50% is
called Q2 or the median and 75% is called Q3.

```{r }
iris |> 
  pull(Petal.Length) |> 
  quantile()
```

The interquartile range is a measure for variability that is robust
against outliers. It is defined the length Q3 - Q2 which covers the 50%
of the data in the middle.

```{r }
iris |> 
  summarize(IQR = 
  quantile(Petal.Length, probs = 0.75) - quantile(Petal.Length, probs = 0.25))
```

### Correlation

#### Pearson Correlation

Correlation can be used for ratio/interval scaled features. We typically
think of the [Pearson correlation
coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)
between features (columns).

```{r }
cc <- iris |> 
  select(-Species) |> 
  cor()
cc
```

`cor` calculates a correlation matrix with pairwise correlations between
features. Correlation matrices are symmetric, but different to
distances, the whole matrix is stored.

The correlation between Petal.Length and Petal.Width can be visualized
using a scatter plot.

```{r }
ggplot(iris, aes(Petal.Length, Petal.Width)) + 
  geom_point() +
  geom_smooth(method = "lm")
```

`geom_smooth` adds a regression line by fitting a linear model (`lm`).
Most points are close to this line indicating strong linear dependence
(i.e., high correlation).

We can calculate individual correlations by specifying two vectors.

```{r }
with(iris, cor(Petal.Length, Petal.Width))
```

*Note:* `with` lets you use columns using just their names and
`with(iris, cor(Petal.Length, Petal.Width))` is the same as
`cor(iris$Petal.Length, iris$Petal.Width)`.

Finally, we can test if a correlation is significantly different from
zero.

```{r }
with(iris, cor.test(Petal.Length, Petal.Width))
```

A small p-value (less than 0.05) indicates that the observed correlation
is significantly different from zero. This can also be seen by the fact
that the 95% confidence interval does not span zero.

Sepal.Length and Sepal.Width show little correlation:

```{r } 
ggplot(iris, aes(Sepal.Length, Sepal.Width)) + 
  geom_point() +   
  geom_smooth(method = "lm") 

with(iris, cor(Sepal.Length, Sepal.Width)) 
with(iris, cor.test(Sepal.Length, Sepal.Width))
```

#### Rank Correlation

Rank correlation is used for ordinal features or if the correlation is
not linear. To show this, we first convert the continuous features in
the Iris dataset into ordered factors (ordinal) with three levels using
the function `cut`.

```{r }
iris_ord <- iris |> 
  mutate(across(where(is.numeric), 
  function(x) cut(x, 3, labels = c("short", "medium", "long"), ordered = TRUE)))

iris_ord
summary(iris_ord)
iris_ord |> 
  pull(Sepal.Length)
```

Two measures for rank correlation are Kendall's Tau and Spearman's Rho.

[Kendall's Tau Rank Correlation
Coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient)
measures the agreement between two rankings (i.e., ordinal features).

```{r }
iris_ord |> 
  select(-Species) |> 
  sapply(xtfrm) |> 
  cor(method = "kendall")
```

**Note:** We have to use `xtfrm` to transform the ordered factors into
ranks, i.e., numbers representing the order.

[Spearman's
Rho](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)
is equal to the Pearson correlation between the rank values of those two
features.

```{r }
iris_ord |> 
  select(-Species) |> 
  sapply(xtfrm) |> 
  cor(method = "spearman")
```

Spearman's Rho is much faster to compute on large datasets then
Kendall's Tau.

Comparing the rank correlation results with the Pearson correlation on
the original data shows that they are very similar. This indicates that
discretizing data does not result in the loss of too much information.

```{r }
iris |> 
  select(-Species) |> 
  cor()
```

### Density 

[Density estimation](https://en.wikipedia.org/wiki/Density_estimation)
estimate the probability density function
(distribution) of a continuous variable from observed data.

Just plotting the data using points is not very helpful for a single
feature.

```{r }
ggplot(iris, aes(x = Petal.Length, y = 0)) + geom_point()
```

#### Histograms

A [histograms](https://en.wikipedia.org/wiki/Histogram) shows more about
the distribution by counting how many values fall within a bin and
visualizing the counts as a bar chart. We use `geom_rug` to place marks
for the original data points at the bottom of the histogram.

```{r }
ggplot(iris, aes(x = Petal.Length)) +
  geom_histogram() +
  geom_rug(alpha = 1/2)
```

Two-dimensional distributions can be visualized using 2-d binning or
hexagonal bins.

```{r }
ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
  geom_bin2d(bins = 10) +
  geom_jitter(color = "red")

ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
  geom_hex(bins = 10) +
  geom_jitter(color = "red")
```

#### Kernel Density Estimate (KDE)

[Kernel density
estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) is
used to estimate the probability density function (distribution) of a
feature. It works by replacing each value with a kernel function (often
a Gaussian) and then adding them up. The result is an estimated
probability density function that looks like a smoothed version of the
histogram. The bandwidth (`bw`) of the kernel controls the amount of
smoothing.

```{r }
ggplot(iris, aes(Petal.Length)) +
  geom_density(bw = .2) +
  geom_rug(alpha = 1/2)
```

Kernel density estimates can also be done in two dimensions.

```{r }
ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
  geom_density_2d_filled() +
  geom_jitter()
```

## Visualization

We go through the basic visualizations used when working with data.
For space reasons, this chapter was moved from the
printed textbook to this 
[Data Exploration Web Chapter.](https://www-users.cse.umn.edu/~kumar001/dmbook/data_exploration_1st_edition.pdf)

### Histogram

Histograms show the distribution of a single continuous feature.

```{r }
ggplot(iris, aes(Petal.Width)) + geom_histogram(bins = 20)
```

If the data contains groups, then the group information can be easily 
added as an aesthetic to the histogram. We also add alpha and position 
to make the   

```{r }
ggplot(iris, aes(Petal.Width)) + 
         geom_histogram(bins = 20, aes(fill = Species))
```

### Boxplot

Boxplots are used to compare the distribution of a feature between
different groups. The horizontal line in the middle of the boxes are the
group-wise medians, the boxes span the interquartile range. The whiskers
(vertical lines) span typically 1.4 times the interquartile range.
Points that fall outside that range are typically outliers shown as
dots.

```{r }
ggplot(iris, aes(Species, Sepal.Length)) + 
  geom_boxplot()
```

The group-wise medians can also be calculated directly.

```{r }
iris |> group_by(Species) |> 
  summarize(across(where(is.numeric), median))
```

To compare the distribution of the four features using a ggplot boxplot,
we first have to transform the data into long format (i.e., all feature
values are combined into a single column).

```{r }
library(tidyr)
iris_long <- iris |> 
  mutate(id = row_number()) |> 
  pivot_longer(1:4)

ggplot(iris_long, aes(name, value)) + 
  geom_boxplot() +
  labs(y = "Original value")
```

This visualization is only useful if all features have roughly the same
range. The data can be scaled first to compare the distributions.

```{r }
library(tidyr)
iris_long_scaled <- iris |> 
  scale_numeric() |> 
  mutate(id = row_number()) |> pivot_longer(1:4)

ggplot(iris_long_scaled, aes(name, value)) + 
  geom_boxplot() +
  labs(y = "Scaled value")
```

### Scatter plot

Scatter plots show the relationship between two continuous features.

```{r }
ggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + 
  geom_point(aes(color = Species))
```

We can add a regression using `geom_smooth` with the 
linear model method to show that there is a 
linear relationship between the two variables. A confidence interval 
for the regression is also shown. 
This can be suppressed using `se = FALSE`.

```{r }
ggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + 
  geom_point(aes(color = Species)) +  
  geom_smooth(method = "lm")
```

We can also perform group-wise linear regression by adding the color
aesthetic also to `geom_smooth`.

```{r }
ggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) + 
  geom_point(aes(color = Species)) +  
  geom_smooth(method = "lm", aes(color = Species))
```

The same can be achieved by using the color aesthetic in the `qqplot` call,
then it applies to all geoms.

### Scatter Plot Matrix

A scatter plot matrix show the relationship between all pairs of features
by arranging panels in a matrix. First, lets look at a regular
R-base plot.

```{r }
pairs(iris, col = iris$Species)
```

The package `GGally` provides a way more sophisticated visualization.

```{r }
library("GGally")
ggpairs(iris,  aes(color = Species), progress = FALSE)
```

Additional plots
(histograms, density estimates and box plots) and correlation
coefficients are shown in different panels. See the [Data Quality]
section for a description of how to interpret the different panels.

### Matrix Visualization

Matrix visualization shows the values in the matrix using a color scale.

```{r }
iris_matrix <- iris |> select(-Species) |> as.matrix()
```

We need the long format for tidyverse.

```{r }
iris_long <- as_tibble(iris_matrix) |> 
  mutate(id = row_number()) |> 
  pivot_longer(1:4)

head(iris_long)

ggplot(iris_long, aes(x = name, y = id)) + 
  geom_tile(aes(fill = value))
```

Smaller values are darker. Package `seriation` provides a simpler
plotting function.

```{r }
library(seriation)
ggpimage(iris_matrix, prop = FALSE)
```

We can scale the features to z-scores to make them better comparable.

```{r }
iris_scaled <- scale(iris_matrix)
ggpimage(iris_scaled, prop = FALSE)
```

This reveals red and blue blocks. Each row is a flower and the flowers
in the Iris dataset are sorted by species. The blue blocks for the top
50 flowers show that these flowers are smaller than average for all but
Sepal.Width and the red blocks show that the bottom 50 flowers are
larger for most features.

Often, reordering data matrices help with visualization. A reordering
technique is called seriation. Ir reorders rows and columns to place
more similar points closer together.

```{r }
ggpimage(iris_scaled, order = seriate(iris_scaled), prop = FALSE)
```

We see that the rows (flowers) are organized from very blue to very red
and the features are reordered to move Sepal.Width all the way to the
right because it is very different from the other features.

### Correlation Matrix

A correlation matrix contains the correlation between features.

```{r }
cm1 <- iris |> select(-Species) |> as.matrix() |> cor()
cm1
```

Package `ggcorrplot` provides a visualization for correlation matrices.

```{r }
library(ggcorrplot)
ggcorrplot(cm1)
```

Package `seriation` provides a reordered version for this plot using a
heatmap.

```{r }
gghmap(cm1, prop = TRUE)
```

Correlations can also be calculates between objects by transposing the
data matrix.

```{r }
cm2 <- iris |> select(-Species) |> as.matrix() |> t() |> cor()

ggcorrplot(cm2)
```

Object-to-object correlations can be used as a measure of similarity.
The dark red blocks indicate different species.

### Parallel Coordinates Plot

Parallel coordinate plots can visualize several features in a single
plot. Lines connect the values for each object (flower).

```{r }
library(GGally)
ggparcoord(iris, columns = 1:4, groupColumn = 5)
```

The plot can be improved by reordering the variables to place correlated
features next to each other.

```{r }
o <- seriate(as.dist(1-cor(iris[,1:4])), method = "BBURCG")
get_order(o)
ggparcoord(iris, columns = as.integer(get_order(o)), groupColumn = 5)
```

### More Visualizations

A well organized collection of visualizations with code can be found at
[The R Graph Gallery](https://www.r-graph-gallery.com/).

## Exercises*

The R package **palmerpenguins** contains measurements for penguin of different
species from the Palmer Archipelago, Antarctica. Install the package.
It provides a CSV file which can be read in the following way:

```{r }
library("palmerpenguins")
penguins <- read_csv(path_to_file("penguins.csv"))
head(penguins)
```

Create in RStudio a new R Markdown document.
Apply the code in the sections of this chapter to the data set to answer the
following questions. 

1. What is the scale of measurement for each column?
2. Are there missing values in the data? How much data is missing?
3. Compute and discuss basic statistics.
4. Group the penguins by species, island or sex. What can you find out?
5. Can you identify correlations?
6. Apply histograms, boxplots, scatterplots and correlation matrix visualization.
  What do the visualizations show.

Make sure your markdown document contains now a well formatted report.
Use the `Knit` button to create a HTML document.



