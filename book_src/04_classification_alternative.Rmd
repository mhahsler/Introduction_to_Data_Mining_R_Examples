---
editor_options: 
  markdown: 
    wrap: 72
---

# Classification: Alternative Techniques

Install the packages used in this chapter:

```{r setup_04}
pkgs <- c("basemodels", "C50", "caret", "e1071", "keras", "klaR", 
          "lattice", "MASS", "mlbench", "nnet", "palmerpenguins", 
          "randomForest", "rpart", "RWeka", "scales", "tidyverse", 
          "xgboost")

pkgs_install <- pkgs[!(pkgs %in% installed.packages()[,"Package"])]
if(length(pkgs_install)) install.packages(pkgs_install)
```

```{r setup_04-2, include=FALSE}
source("_common.R")
source("format_pkgs.R")
all_pkgs <- union(all_pkgs, pkgs)
set.seed(1234)
```

The packages used for this chapter are: `r format_pkgs(pkgs)`


We will use tidyverse to prepare the data.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
```

Show fewer digits

```{r }
options(digits=3)
```


## Types of Classifiers

Many different [classification algorithms](https://en.wikipedia.org/wiki/Supervised_learning) 
have been proposed in the literature. 
In this chapter, we will apply some of the more popular methods.

### Set up the Training and Test Data

We will use the Zoo dataset which is included in the R package `mlbench`
(you may have to install it). The Zoo dataset containing 17 (mostly
logical) variables on different 101 animals as a data frame with 17
columns (hair, feathers, eggs, milk, airborne, aquatic, predator,
toothed, backbone, breathes, venomous, fins, legs, tail, domestic,
catsize, type). We convert the data frame into a tidyverse tibble
(optional).

```{r }
data(Zoo, package="mlbench")
Zoo <- as_tibble(Zoo)
Zoo
```

We will use the package [**caret**](https://topepo.github.io/caret/) to
make preparing training sets and building classification (and
regression) models easier. A great cheat sheet can be found
[here](https://ugoproto.github.io/ugo_r_doc/pdf/caret.pdf).

```{r message=FALSE, warning=FALSE}
library(caret)
```

Multi-core support can be used for cross-validation. **Note:** It is
commented out here because it does not work with rJava used by the RWeka-based
classifiers
below.

```{r }
##library(doMC, quietly = TRUE)
##registerDoMC(cores = 4)
##getDoParWorkers()
```

Test data is not used in the model building process and needs to be set
aside purely for testing the model after it is completely built. Here I
use 80% for training.

```{r }
inTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]
Zoo_train <- Zoo |> slice(inTrain)
Zoo_test <- Zoo |> slice(-inTrain)
```

`train()` is aware of the tunable hyperparameters of each method and 
automatically performs model selection using a validation set. 
We will use for all models the same validation sets by
creating a fixed sampling scheme (10-folds). This will help when we can compare the fitted models later.

```{r }
train_index <- createFolds(Zoo_train$type, k = 10, returnTrain = TRUE)
```

The fixed folds are used in `train()` with the argument
`trControl = trainControl(method = "cv", index = train_index))`. If
you don't need fixed folds, then remove `index = train_index` in the
code below.

For help with building models in caret see: `? train`

**Note:** Be careful if you have many `NA` values in your data.
`train()` and cross-validation many fail in some cases. If that is the
case then you can remove features (columns) which have many `NA`s, omit
`NA`s using `na.omit()` or use imputation to replace them with
reasonable values (e.g., by the feature mean or via kNN). Highly
imbalanced datasets are also problematic since there is a chance that a
fold does not contain examples of each class leading to a hard to
understand error message.

## Rule-based classifier: PART

```{r }
rulesFit <- Zoo_train |> train(type ~ .,
  method = "PART",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", index = train_index))
rulesFit
```
The model selection results are shown in the table. This is the selected model:

```{r }
rulesFit$finalModel
```

PART returns a decision list, i.e., an ordered rule set. For example, 
the first rule shows that an animal with no feathers but milk is a mammal.
I ordered rule sets, the decision of the first matching rule is used.

## Nearest Neighbor Classifier

K-Nearest neighbor classifiers classify a new data point by looking at the 
majority class labels of its k nearest neighbors in the training data set.
The used kNN implementation uses Euclidean distance to determine what data points 
are near by, so data needs be standardized
(scaled) first. Here legs are measured between 0 and 6 while all other
variables are between 0 and 1. Scaling to z-scores can be directly performed as
preprocessing in `train` using the parameter `preProcess = "scale"`.

The $k$ value is typically choose as an odd number so we get a clear majority.

```{r }
knnFit <- Zoo_train |> train(type ~ .,
  method = "knn",
  data = _,
  preProcess = "scale",
  tuneGrid = data.frame(k = c(1, 3, 5, 7, 9)),
	trControl = trainControl(method = "cv", index = train_index))
knnFit
```

```{r }
knnFit$finalModel
```

kNN classifiers are lazy models meaning that instead of learning, they just
keep the complete dataset. This is why the final model just gives us a summary statistic for the class labels in the training data.


## Naive Bayes Classifier

Caret's train formula interface translates logicals and factors into dummy 
variables which the classifier interprets as numbers so it would used a Gaussian naive Bayes estimation. To avoid this, I directly specify `x` and `y`.

```{r }
NBFit <- train(x = as.data.frame(Zoo_train[, -ncol(Zoo_train)]), 
               y = pull(Zoo_train, "type"),
               method = "nb",
               tuneGrid = data.frame(fL = c(.2, .5, 1, 5), 
                                     usekernel = TRUE, adjust = 1),
               trControl = trainControl(method = "cv", index = train_index))
NBFit
```

The final model contains the prior probabilities for each class.
```{r }
NBFit$finalModel$apriori
```

And the conditional probabilities as a table for each feature. For brevity, 
we only show the tables for the first three features. For example, 
the condition probability 
$P(\text{hair} = \text{TRUE} | \text{class} = \text{mammal})$ is 
`r NBFit$finalModel$tables$hair["mammal", "TRUE"]`.

```{r }
NBFit$finalModel$tables[1:3]
```

## Bayesian Network

Bayesian networks are not covered here. R has very good 
support for modeling with Bayesian Networks. An example is
the package [bnlearn](https://www.bnlearn.com/).

## Logistic regression

Logistic regression is a very powerful classification method 
and should always be tried as 
one of the first models. 
A detailed discussion with more code is available in Section 
[Logistic Regression*].

Regular logistic regression predicts only one outcome coded as a 
binary variable. Since we have data with several
classes, we use [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), 
also called a log-linear model
which is an extension of logistic regresses for multi-class problems.

```{r }
logRegFit <- Zoo_train |> train(type ~ .,
  method = "multinom",
  data = _,
  trace = FALSE, # suppress some output
	trControl = trainControl(method = "cv", index = train_index))
logRegFit
```

```{r }
logRegFit$finalModel
```
The coefficients are log-odds ratios. A negative log-odds ratio means that the odds go down with an increase in
the value of the predictor.  A predictor with a
positive log-odds ratio increases the odds. For example,
in the model above, `hair=TRUE` has a negative coefficient for 
bird but `feathers=TRUE` has a large positive coefficient.

## Artificial Neural Network (ANN)

Standard networks have an input layer, an output layer and in between
a single hidden layer.

```{r }
nnetFit <- Zoo_train |> train(type ~ .,
  method = "nnet",
  data = _,
	tuneLength = 5,
	trControl = trainControl(method = "cv", index = train_index),
  trace = FALSE # no progress output
  )
nnetFit
```

The input layer has a size of 16, one for each input feature and 
the output layer has a size of 7 representing the 7 classes.
Model selection chose a network architecture with a hidden
layer with `r nnetFit$bestTune$size` units
resulting in `r length(nnetFit$finalModel$wts)` learned weights.
Since the model is considered a black-box model only the network architecture
and the used variables are shown as a summary.

```{r }
nnetFit$finalModel
```


## Deep Learning with keras/tensorflow

Using deep learning requires some extra setup. It is not very helpful 
for the dataset we use, but it is included here to demonstrate how to set up
and train a simple model architecture.

The keras package needs the packages `reticulate` and `tensorflow`. To
install keras you need to

1.  have a working Python installation,
2.  install the keras R package `install.packages("keras")`, and
3.  install the tensorflow/keras Python modules with
    `library(keras); install_keras()`

```{r message=FALSE, warning=FALSE}
library(keras)
```

Prepare the data. All data needs to be in a matrix of all
numeric/integer values. The class variable needs to be one-hot encoded and the
the `keras` function `to_categorical()` expects a class index starting with 0 
instead of 1.

```{r warning=FALSE}
X <- Zoo_train |> 
  select(!type) |> 
  mutate(across(everything(), as.integer)) |> 
  as.matrix()
head(X)

y <- Zoo_train |> 
  pull("type") |> 
  { function(x)(as.integer(x) - 1L) }() |>   ## make index start with 0
  to_categorical()
head(y)

X_test <- Zoo_test |> 
  select(!type) |> 
  mutate(across(everything(), as.integer)) |> 
  as.matrix()
y_test <- Zoo_test |> 
  pull("type") |> 
  { function(x)(as.integer(x) - 1L) }() |>
  to_categorical()
```

Deep learning uses a large set of hyper-parameters. Choices are the
activation function, number of layers, number of units per layer, the
optimizer, and the type of regularization used. 

We define here a simple network with a single dense hidden layer.
Note that in keras, we have to specify the input size in this first dense layer. 
The output of the classifier is a categorical class value, therefore
we use for the output layer the softmax activation function, and for
training we specify 
categorical cross-entropy as the loss function, and accuracy
as an additional metric.
Also, L2 regularization is used for the weights of the hidden layer to
reduce overfitting. 

```{r message=FALSE, warning=FALSE}
model <- keras_model_sequential(input_shape = c(ncol(X)), 
              name = "single_hidden_layer_classifier") |>
  layer_dense(units = 10, activation = 'relu', 
              kernel_regularizer=regularizer_l2(l=0.01), name = "hidden1") |>
  layer_dense(units = ncol(y), activation = 'softmax', name = "output")

model
```


```{r message=FALSE, warning=FALSE}
model <- model |>  
  compile(loss = 'categorical_crossentropy', 
          optimizer = 'adam', 
          metrics = 'accuracy')
```

For model training, we need to specify the batch size and the number of
training epochs. The fitting process can also use a fraction of the
training data for validation to provide generalization loss/accuracy.

```{r include=FALSE}
# suppress the progress output for the book
options("keras.fit_verbose" = 0)
```

```{r }
history <- model |>
  fit(
    X, 
    y,
    batch_size = 10,
    epochs = 100,
    validation_split = .2
  )

plot(history)
```

To create predictions from the model, we have to convert the one-hot
encoding back to class labels.

```{r}
class_labels <- levels(Zoo_train |> pull(type))

pr <- predict(model, X_test) |> 
  apply(MARGIN = 1, FUN = which.max)
pr <- factor(pr, labels = class_labels, levels = seq_along(class_labels))

pr
```

## Support Vector Machines

```{r }
svmFit <- Zoo_train |> train(type ~.,
  method = "svmLinear",
  data = _,
	tuneLength = 5,
	trControl = trainControl(method = "cv", index = train_index))
svmFit
```

We use a linear support vector machine.
Support vector machines can use kernels to create non-linear decision boundaries.
`method` above can be changed to `"svmPoly"` or `"svmRadial"` to 
use kernels. The choice of kernel is typically make by experimentation.

The support vectors determining the decision boundary are stored in the model.
```{r }
svmFit$finalModel
```

## Ensemble Methods

Many ensemble methods are available in R. We only cover here code for two 
popular methods.

### Random Forest

```{r }
randomForestFit <- Zoo_train |> train(type ~ .,
  method = "rf",
  data = _,
	tuneLength = 5,
	trControl = trainControl(method = "cv", index = train_index))
randomForestFit
```

The default number of trees is 500 and 
`mtry` determines the number of variables randomly sampled as candidates 
at each split. This number is a tradeoff where a larger number allows each tree 
to pick better splits, but a smaller
number increases the independence between trees.

```{r }
randomForestFit$finalModel
```

The model is a set of 500 trees and the prediction is made by applying all trees and 
then using the majority vote.

Since random forests use bagging (bootstrap sampling to train trees),
the remaining data can be used like a test set. The resulting error 
is called out-of-bag (OOB) error and gives an estimate for the 
generalization error. The model above also shows the confusion matrix based on
the OOB error.


### Gradient Boosted Decision Trees (xgboost)

The idea of gradient boosting is to learn a base model and then to learn
successive models to predict and correct the error of all previous models.
Typically, tree models are used.

```{r }
xgboostFit <- Zoo_train |> train(type ~ .,
  method = "xgbTree",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", index = train_index),
  tuneGrid = expand.grid(
    nrounds = 20,
    max_depth = 3,
    colsample_bytree = .6,
    eta = 0.1,
    gamma=0,
    min_child_weight = 1,
    subsample = .5
  ))
xgboostFit
```

The final model is a complicated set of trees, so only some summary information
is shown.

```{r }
xgboostFit$finalModel
```

## Class Imbalance

Classifiers have a hard time to learn from data where we have much more
observations for one class (called the majority class). This is called
the class imbalance problem.

Here is a very good [article about the problem and
solutions.](http://www.kdnuggets.com/2016/08/learning-from-imbalanced-classes.html)

```{r }
library(rpart)
library(rpart.plot)
data(Zoo, package = "mlbench")
```

Class distribution

```{r }
ggplot(Zoo, aes(y = type)) + geom_bar()
```

To create an imbalanced problem, we want to decide if an animal is an
reptile. First, we change the class variable to make it into a binary
reptile/no reptile classification problem. **Note:** We use here the
training data for testing. You should use a separate testing data set!

```{r }
Zoo_reptile <- Zoo |> 
  mutate(type = factor(Zoo$type == "reptile", 
                       levels = c(FALSE, TRUE),
                       labels = c("nonreptile", "reptile")))
```

Do not forget to make the class variable a factor (a nominal variable)
or you will get a regression tree instead of a classification tree.

```{r }
summary(Zoo_reptile)
```

See if we have a class imbalance problem.

```{r }
ggplot(Zoo_reptile, aes(y = type)) + geom_bar()
```

Create test and training data. I use here a 50/50 split to make sure
that the test set has some samples of the rare reptile class.

```{r }
set.seed(1234)

inTrain <- createDataPartition(y = Zoo_reptile$type, p = .5)[[1]]
training_reptile <- Zoo_reptile |> slice(inTrain)
testing_reptile <- Zoo_reptile |> slice(-inTrain)
```

the new class variable is clearly not balanced. This is a problem for
building a tree!

### Option 1: Use the Data As Is and Hope For The Best

```{r }
fit <- training_reptile |> 
  train(type ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"))
```

**Warnings:** "There were missing values in resampled performance
measures." means that some test folds did not contain examples of both
classes. This is very likely with class imbalance and small datasets.

```{r }
fit
rpart.plot(fit$finalModel, extra = 2)
```

the tree predicts everything as non-reptile. Have a look at the error on
the test set.

```{r }
confusionMatrix(data = predict(fit, testing_reptile),
                ref = testing_reptile$type, positive = "reptile")
```

Accuracy is high, but it is exactly the same as the no-information rate
and kappa is zero. Sensitivity is also zero, meaning that we do not
identify any positive (reptile). If the cost of missing a positive is
much larger than the cost associated with misclassifying a negative,
then accuracy is not a good measure! By dealing with imbalance, we are
**not** concerned with accuracy, but we want to increase the
sensitivity, i.e., the chance to identify positive examples.

**Note:** The positive class value (the one that you want to detect) is
set manually to reptile using `positive = "reptile"`. Otherwise
sensitivity/specificity will not be correctly calculated.

### Option 2: Balance Data With Resampling

We use stratified sampling with replacement (to oversample the
minority/positive class). You could also use SMOTE (in package **DMwR**)
or other sampling strategies (e.g., from package **unbalanced**). We use
50+50 observations here (**Note:** many samples will be chosen several
times).

```{r }
library(sampling)
set.seed(1000) # for repeatability

id <- strata(training_reptile, stratanames = "type", 
             size = c(50, 50), method = "srswr")
training_reptile_balanced <- training_reptile |> 
  slice(id$ID_unit)
table(training_reptile_balanced$type)

fit <- training_reptile_balanced |> 
  train(type ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"),
        control = rpart.control(minsplit = 5))

fit
rpart.plot(fit$finalModel, extra = 2)
```

Check on the unbalanced testing data.

```{r }
confusionMatrix(data = predict(fit, testing_reptile),
                ref = testing_reptile$type, positive = "reptile")
```

**Note** that the accuracy is below the no information rate! However,
kappa (improvement of accuracy over randomness) and sensitivity (the
ability to identify reptiles) have increased.

There is a tradeoff between sensitivity and specificity (how many of the
identified animals are really reptiles) The tradeoff can be controlled
using the sample proportions. We can sample more reptiles to increase
sensitivity at the cost of lower specificity (this effect cannot be seen
in the data since the test set has only a few reptiles).

```{r }
id <- strata(training_reptile, stratanames = "type", 
             size = c(50, 100), method = "srswr")
training_reptile_balanced <- training_reptile |> 
  slice(id$ID_unit)
table(training_reptile_balanced$type)

fit <- training_reptile_balanced |> 
  train(type ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"),
        control = rpart.control(minsplit = 5))

confusionMatrix(data = predict(fit, testing_reptile),
                ref = testing_reptile$type, positive = "reptile")
```

### Option 3: Build A Larger Tree and use Predicted Probabilities

Increase complexity and require less data for splitting a node. Here I
also use AUC (area under the ROC) as the tuning metric. You need to
specify the two class summary function. Note that the tree still trying
to improve accuracy on the data and not AUC! I also enable class
probabilities since I want to predict probabilities later.

```{r }
fit <- training_reptile |> 
  train(type ~ .,
        data = _,
        method = "rpart",
        tuneLength = 10,
        trControl = trainControl(method = "cv",
        classProbs = TRUE,  ## necessary for predict with type="prob"
        summaryFunction=twoClassSummary),  ## necessary for ROC
        metric = "ROC",
        control = rpart.control(minsplit = 3))
fit

rpart.plot(fit$finalModel, extra = 2)

confusionMatrix(data = predict(fit, testing_reptile),
                ref = testing_reptile$type, positive = "reptile")
```

**Note:** Accuracy is high, but it is close or below to the
no-information rate!

#### Create A Biased Classifier

We can create a classifier which will detect more reptiles at the
expense of misclassifying non-reptiles. This is equivalent to increasing
the cost of misclassifying a reptile as a non-reptile. The usual rule is
to predict in each node the majority class from the test data in the
node. For a binary classification problem that means a probability of
\>50%. In the following, we reduce this threshold to 1% or more. This
means that if the new observation ends up in a leaf node with 1% or more
reptiles from training then the observation will be classified as a
reptile. The data set is small and this works better with more data.

```{r }
prob <- predict(fit, testing_reptile, type = "prob")
tail(prob)
pred <- ifelse(prob[,"reptile"]>=0.01, "reptile", "nonreptile") |> 
  as.factor()

confusionMatrix(data = pred,
                ref = testing_reptile$type, positive = "reptile")
```

**Note** that accuracy goes down and is below the no information rate.
However, both measures are based on the idea that all errors have the
same cost. What is important is that we are now able to find more
reptiles.

#### Plot the ROC Curve

Since we have a binary classification problem and a classifier that
predicts a probability for an observation to be a reptile, we can also
use a [receiver operating characteristic
(ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
curve. For the ROC curve all different cutoff thresholds for the
probability are used and then connected with a line. The area under the
curve represents a single number for how well the classifier works (the
closer to one, the better).

```{r }
library("pROC")
r <- roc(testing_reptile$type == "reptile", prob[,"reptile"])
r

ggroc(r) + geom_abline(intercept = 1, slope = 1, color = "darkgrey")
```

### Option 4: Use a Cost-Sensitive Classifier

The implementation of CART in `rpart` can use a cost matrix for making
splitting decisions (as parameter `loss`). The matrix has the form

TP FP FN TN

TP and TN have to be 0. We make FN very expensive (100).

```{r }
cost <- matrix(c(
  0,   1,
  100, 0
), byrow = TRUE, nrow = 2)
cost

fit <- training_reptile |> 
  train(type ~ .,
        data = _,
        method = "rpart",
        parms = list(loss = cost),
        trControl = trainControl(method = "cv"))
```

The warning "There were missing values in resampled performance
measures" means that some folds did not contain any reptiles (because of
the class imbalance) and thus the performance measures could not be
calculates.

```{r }
fit

rpart.plot(fit$finalModel, extra = 2)

confusionMatrix(data = predict(fit, testing_reptile),
                ref = testing_reptile$type, positive = "reptile")
```

The high cost for false negatives results in a classifier that does not
miss any reptile.

**Note:** Using a cost-sensitive classifier is often the best option.
Unfortunately, the most classification algorithms (or their
implementation) do not have the ability to consider misclassification
cost.


## Model Comparison

We first create a weak baseline model that always predicts the the majority 
class mammal.

```{r }
baselineFit <- Zoo_train |> train(type ~ .,
  method = basemodels::dummyClassifier,
  data = _,
  strategy = "constant",
  constant = "mammal",
  trControl = trainControl(method = "cv", index = train_index))
baselineFit
```

The kappa of 0 clearly indicates that the baseline model has no power.


We collect the performance metrics from the models trained on the same
data.

```{r }
resamps <- resamples(list(
  baseline = baselineFit,
  SVM = svmFit,
  KNN = knnFit,
  rules = rulesFit,
  randomForest = randomForestFit,
  xgboost = xgboostFit,
  NeuralNet = nnetFit
    ))
resamps
```

The summary statistics shows the performance. We see that all
methods do well on this easy data set, only the baseline model performs 
as expected poorly.

```{r }
summary(resamps)

library(lattice)
bwplot(resamps, layout = c(3, 1))
```

Perform inference about differences between models. For each metric, all
pair-wise differences are computed and tested to assess if the
difference is equal to zero. By default Bonferroni correction for
multiple comparison is used. Differences are shown in the upper triangle
and p-values are in the lower triangle.

```{r }
difs <- diff(resamps)
summary(difs)
```

All perform similarly well except the baseline model (differences in the first row
are negative and the p-values in the first column are \<.05 indicating
that the null-hypothesis of a difference of 0 can be rejected).

Most models do similarly well on the data. We choose here the random
forest model and evaluate its generalization performance on the held-out
test set.

```{r }
pr <- predict(randomForestFit, Zoo_test)
pr
```

Calculate the confusion matrix for the held-out test data.

```{r }
confusionMatrix(pr, reference = Zoo_test$type)
```

## Comparing Decision Boundaries of Popular Classification Techniques*

Classifiers create decision boundaries to discriminate between classes.
Different classifiers are able to create different shapes of decision
boundaries (e.g., some are strictly linear) and thus some classifiers
may perform better for certain datasets. This page visualizes the
decision boundaries found by several popular classification methods.

The following plot adds the decision boundary (black lines) and
classification confidence (color intensity) by evaluating the classifier
at evenly spaced grid points. Note that low resolution (to make
evaluation faster) will make the decision boundary look like it has
small steps even if it is a (straight) line.

```{r message=FALSE, warning=FALSE}
library(scales)
library(tidyverse)
library(ggplot2)
library(caret)

decisionplot <- function(model, data, class_var, 
  predict_type = c("class", "prob"), resolution = 3 * 72) {
  # resolution is set to 72 dpi if the image is rendered  3 inches wide. 
  
  y <- data |> pull(class_var)
  x <- data |> dplyr::select(-all_of(class_var))
  
  # resubstitution accuracy
  prediction <- predict(model, x, type = predict_type[1])
  # LDA returns a list
  if(is.list(prediction)) prediction <- prediction$class
  prediction <- factor(prediction, levels = levels(y))
  
  cm <- confusionMatrix(data = prediction, 
                        reference = y)
  acc <- cm$overall["Accuracy"]
  
  # evaluate model on a grid
  r <- sapply(x[, 1:2], range, na.rm = TRUE)
  xs <- seq(r[1,1], r[2,1], length.out = resolution)
  ys <- seq(r[1,2], r[2,2], length.out = resolution)
  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))
  colnames(g) <- colnames(r)
  g <- as_tibble(g)
  
  ### guess how to get class labels from predict
  ### (unfortunately not very consistent between models)
  cl <- predict(model, g, type = predict_type[1])
  
  # LDA returns a list
  prob <- NULL
  if(is.list(cl)) { 
    prob <- cl$posterior
    cl <- cl$class
  } else
    if(!is.na(predict_type[2]))
      try(prob <- predict(model, g, type = predict_type[2]))
  
  # we visualize the difference in probability/score between the 
  # winning class and the second best class.
  # don't use probability if predict for the classifier does not support it.
  max_prob <- 1
  if(!is.null(prob))
    try({
      max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))
      max_prob <- max_prob[,1] - max_prob[,2]
    }, silent = TRUE) 
  
  cl <- factor(cl, levels = levels(y))
  
  g <- g |> add_column(prediction = cl, probability = max_prob)
  
  ggplot(g, mapping = aes(
    x = .data[[colnames(g)[1]]], y = .data[[colnames(g)[2]]])) +
    geom_raster(mapping = aes(fill = prediction, alpha = probability)) +
    geom_contour(mapping = aes(z = as.numeric(prediction)), 
      bins = length(levels(cl)), linewidth = .5, color = "black") +
    geom_point(data = data, mapping =  aes(
      x = .data[[colnames(data)[1]]], 
      y = .data[[colnames(data)[2]]],
      shape = .data[[class_var]]), alpha = .7) + 
    scale_alpha_continuous(range = c(0,1), limits = c(0,1), guide = "none") +  
    labs(subtitle = paste("Training accuracy:", round(acc, 2)))
}
```

### Iris Dataset

For easier visualization, we use two dimensions of the Iris dataset.
Contour lines visualize the density like mountains on a map.

```{r }
set.seed(1000)
data(iris)
iris <- as_tibble(iris)

### Three classes 
### (note: MASS also has a select function which hides dplyr's select)
x <- iris |> dplyr::select(Sepal.Length, Sepal.Width, Species)
x

ggplot(x, aes(x = Sepal.Length, y = Sepal.Width, fill = Species)) +  
  stat_density_2d(geom = "polygon", aes(alpha = after_stat(level))) +
  geom_point()
```

*Note:* There is some overplotting and you could use `geom_jitter()`
instead of `geom_point()`.

#### Nearest Neighbor Classifier

```{r }
model <- x |> caret::knn3(Species ~ ., data = _, k = 1)
decisionplot(model, x, class_var = "Species") + 
  labs(title = "kNN (1 neighbor)")

model <- x |> caret::knn3(Species ~ ., data = _, k = 3)
decisionplot(model, x, class_var = "Species") + 
  labs(title = "kNN (3 neighbor)")

model <- x |> caret::knn3(Species ~ ., data = _, k = 9)
decisionplot(model, x, class_var = "Species") + 
  labs(title = "kNN (9 neighbor)")
```

Increasing $k$ smooths the decision boundary. At $k=1$, we see white
areas around points where flowers of two classes are in the same spot.
Here, the algorithm randomly chooses a class during prediction resulting
in the meandering decision boundary. The predictions in that area are
not stable and every time we ask for a class, we may get a different
class.

#### Naive Bayes Classifier

```{r }
model <- x |> e1071::naiveBayes(Species ~ ., data = _)
decisionplot(model, x, class_var = "Species", 
             predict_type = c("class", "raw")) + 
  labs(title = "Naive Bayes")
```

#### Linear Discriminant Analysis

```{r }
model <- x |> MASS::lda(Species ~ ., data = _)
decisionplot(model, x, class_var = "Species") + 
  labs(title = "LDA")
```

#### Multinomial Logistic Regression (implemented in nnet)

Multinomial logistic regression is an extension of logistic regression
to problems with more than two classes.

```{r }
model <- x |> nnet::multinom(Species ~., data = _)
decisionplot(model, x, class_var = "Species") + 
  labs(titel = "Multinomial Logistic Regression")
```

#### Decision Trees

```{r }
model <- x |> rpart::rpart(Species ~ ., data = _)
decisionplot(model, x, class_var = "Species") + 
  labs(title = "CART")

model <- x |> rpart::rpart(Species ~ ., data = _,
  control = rpart::rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class_var = "Species") + 
  labs(title = "CART (overfitting)")

model <- x |> C50::C5.0(Species ~ ., data = _)
decisionplot(model, x, class_var = "Species") + 
  labs(title = "C5.0")

model <- x |> randomForest::randomForest(Species ~ ., data = _)
decisionplot(model, x, class_var = "Species") + 
  labs(title = "Random Forest")
```

#### SVM

```{r }
model <- x |> e1071::svm(Species ~ ., data = _, kernel = "linear")
decisionplot(model, x, class_var = "Species") + 
  labs(title = "SVM (linear kernel)")

model <- x |> e1071::svm(Species ~ ., data = _, kernel = "radial")
decisionplot(model, x, class_var = "Species") + 
  labs(title = "SVM (radial kernel)")

model <- x |> e1071::svm(Species ~ ., data = _, kernel = "polynomial")
decisionplot(model, x, class_var = "Species") + 
  labs(title = "SVM (polynomial kernel)")

model <- x |> e1071::svm(Species ~ ., data = _, kernel = "sigmoid")
decisionplot(model, x, class_var = "Species") + 
  labs(title = "SVM (sigmoid kernel)")
```

#### Single Layer Feed-forward Neural Networks

```{r }
model <-x |> nnet::nnet(Species ~ ., data = _, size = 1, trace = FALSE)
decisionplot(model, x, class_var  = "Species", 
  predict_type = c("class", "raw")) + labs(title = "NN (1 neuron)")

model <-x |> nnet::nnet(Species ~ ., data = _, size = 2, trace = FALSE)
decisionplot(model, x, class_var  = "Species", 
  predict_type = c("class", "raw")) + labs(title = "NN (2 neurons)")

model <-x |> nnet::nnet(Species ~ ., data = _, size = 4, trace = FALSE)
decisionplot(model, x, class_var  = "Species", 
  predict_type = c("class", "raw")) + labs(title = "NN (4 neurons)")

model <-x |> nnet::nnet(Species ~ ., data = _, size = 10, trace = FALSE)
decisionplot(model, x, class_var  = "Species", 
  predict_type = c("class", "raw")) + labs(title = "NN (10 neurons)")
```

### Circle Dataset

This set is not linearly separable!

```{r }
set.seed(1000)

x <- mlbench::mlbench.circle(500)
###x <- mlbench::mlbench.cassini(500)
###x <- mlbench::mlbench.spirals(500, sd = .1)
###x <- mlbench::mlbench.smiley(500)
x <- cbind(as.data.frame(x$x), factor(x$classes))
colnames(x) <- c("x", "y", "class")
x <- as_tibble(x)
x

ggplot(x, aes(x = x, y = y, color = class)) + 
  geom_point()
```

#### Nearest Neighbor Classifier

```{r }
model <- x |> caret::knn3(class ~ ., data = _, k = 1)
decisionplot(model, x, class_var = "class") + 
  labs(title = "kNN (1 neighbor)")

model <- x |> caret::knn3(class ~ ., data = _, k = 10)
decisionplot(model, x, class_var = "class") + 
  labs(title = "kNN (10 neighbor)")
```

#### Naive Bayes Classifier

```{r }
model <- x |> e1071::naiveBayes(class ~ ., data = _)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class", "raw")) + 
  labs(title = "naive Bayes")
```

#### Linear Discriminant Analysis

LDA cannot find a good model since the true decision boundary is not
linear.

```{r }
model <- x |> MASS::lda(class ~ ., data = _)
decisionplot(model, x, class_var = "class") + labs(title = "LDA")
```

#### Logistic Regression (implemented in nnet)

Multinomial logistic regression is an extension of logistic regression
to problems with more than two classes. It also tries to find a linear
decision boundary.

```{r }
model <- x |> nnet::multinom(class ~., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(titel = "Multinomial Logistic Regression")
```

#### Decision Trees

```{r }
model <- x |> rpart::rpart(class ~ ., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(title = "CART")

model <- x |> rpart::rpart(class ~ ., data = _,
  control = rpart::rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class_var = "class") + 
  labs(title = "CART (overfitting)")

model <- x |> C50::C5.0(class ~ ., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(title = "C5.0")

library(randomForest)
model <- x |> randomForest(class ~ ., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(title = "Random Forest")
```

#### SVM

Linear SVM does not work for this data.

```{r }
model <- x |> e1071::svm(class ~ ., data = _, kernel = "linear")
decisionplot(model, x, class_var = "class") + 
  labs(title = "SVM (linear kernel)")

model <- x |> e1071::svm(class ~ ., data = _, kernel = "radial")
decisionplot(model, x, class_var = "class") + 
  labs(title = "SVM (radial kernel)")

model <- x |> e1071::svm(class ~ ., data = _, kernel = "polynomial")
decisionplot(model, x, class_var = "class") + 
  labs(title = "SVM (polynomial kernel)")

model <- x |> e1071::svm(class ~ ., data = _, kernel = "sigmoid")
decisionplot(model, x, class_var = "class") + 
  labs(title = "SVM (sigmoid kernel)")
```

#### Single Layer Feed-forward Neural Networks

```{r }
model <-x |> nnet::nnet(class ~ ., data = _, size = 1, trace = FALSE)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class")) + labs(title = "NN (1 neuron)")

model <-x |> nnet::nnet(class ~ ., data = _, size = 2, trace = FALSE)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class")) + labs(title = "NN (2 neurons)")

model <-x |> nnet::nnet(class ~ ., data = _, size = 4, trace = FALSE)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class")) + labs(title = "NN (4 neurons)")

model <-x |> nnet::nnet(class ~ ., data = _, size = 10, trace = FALSE)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class")) + labs(title = "NN (10 neurons)")
```

## More Information on Classification with R

-   Package caret: <http://topepo.github.io/caret/index.html>
-   Tidymodels (machine learning with tidyverse):
    <https://www.tidymodels.org/>
-   R taskview on machine learning:
    <http://cran.r-project.org/web/views/MachineLearning.html>

## Exercises*

We will again use the Palmer penguin data for the exercises.

```{r }
library(palmerpenguins)
head(penguins)
```

Create a R markdown file with the code and do the following below.

1. Apply at least 3 different classification models to the data.
2. Compare the models and a simple baseline model. Which model 
  performs the best? Does it perform significantly better than the other 
  models?


