---
editor_options: 
  markdown: 
    wrap: 72
---

# Classification: Basic Concepts

This chapter introduces decision trees for classification and discusses
how models are built and evaluated. 

The corresponding chapter of 
the data mining textbook is available online: 
[Chapter 3: Classification: Basic Concepts and Techniques.](https://www-users.cs.umn.edu/~kumar001/dmbook/ch3_classification.pdf)

### Packages Used in this Chapter {.unnumbered}

```{r setup_03}
pkgs <- c("caret", "FSelector", "lattice", "mlbench", 
          "palmerpenguins", "party", "pROC", "rpart", 
          "rpart.plot", "tidyverse")

pkgs_install <- pkgs[!(pkgs %in% installed.packages()[,"Package"])]
if(length(pkgs_install)) install.packages(pkgs_install)
```

The packages used for this chapter are: 

`r format_pkgs(pkgs)`

```{r setup_03-2, include=FALSE}
source("_common.R")
source("format_pkgs.R")
all_pkgs <- union(all_pkgs, pkgs)
```

In the examples in this book, we use the popular machine learning R package [`caret`](https://topepo.github.io/caret/). It makes preparing
training sets, building classification (and regression) models and
evaluation easier. A great cheat sheet can be found
[here](https://ugoproto.github.io/ugo_r_doc/pdf/caret.pdf).

A newer R framework for machine learning
is [`tidymodels`](https://www.tidymodels.org/), a set of packages that 
integrate more naturally with tidyverse. Using tidymodels, or any other 
framework (e.g., Python's [scikit-learn](https://scikit-learn.org/)) should 
be relatively easy after 
learning the concepts using caret. 

## Basic Concepts

Classification is a machine learning task with the goal to learn a predictive 
function of the form 

$$y = f(\mathbf{x}),$$ 

where $\mathbf{x}$ is called the attribute set and $y$ the class label. The attribute set
consists of feature which describe an object. These features can be measured using any scale 
(i.e., nominal, interval, ...). The class label is a nominal attribute. It it is a binary 
attribute, then the problem is called a binary classification problem.

Classification learns the classification model from training data where both the features and 
the correct class label are available. This is why it is called a [supervised learning problem](https://en.wikipedia.org/wiki/Supervised_learning).

A related supervised learning problem is [regression](https://en.wikipedia.org/wiki/Linear_regression), 
where $y$ is a number instead of a label.
Linear regression is a very popular supervised learning model
which is taught in almost any introductory statistics course.
Code examples for regression are available in the extra Chapter
[Regression*].

This chapter will introduce decision trees, model evaluation and comparison, feature selection,
and then explore methods to handle the class imbalance problem.

You can read the free sample chapter from the textbook [@Tan2005]:
[Chapter 3. Classification: Basic Concepts and
Techniques](https://www-users.cs.umn.edu/~kumar001/dmbook/ch3_classification.pdf)

## General Framework for Classification

Supervised learning has two steps:

1. Induction: Training a model on **training data** with known class labels.
2. Deduction: Predicting class labels for new data.

We often test model by predicting the class for data where we know the 
correct label. We test the model on **test data** with known labels
and can then calculate the error by comparing the prediction with the 
known correct label.
It is tempting to measure how well the model has learned the 
training data, by testing it on the training data. The error on the 
training data is called **resubstitution error.** It does not 
help us to find out if the model generalizes well to new data that 
was not part of the training. 

We typically want to 
evaluate how well the model generalizes new data, so it is important that
the test data and the training data do not overlap. We call the 
error on proper test data the **generalization error.**

This chapter builds up the needed concepts.
A complete example of how to perform model selection and estimate 
the generalization error is in the section [Hyperparameter Tuning]. 

### The Zoo Dataset

To demonstrate classification, we will use the Zoo dataset which is included in the R package
**mlbench** (you may have to install it). The Zoo dataset containing 17
(mostly logical) variables for 101 animals as a data frame with
17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator,
toothed, backbone, breathes, venomous, fins, legs, tail, domestic,
catsize, type). 
The first 16 columns represent the feature vector $\mathbf{x}$ and the last column
called type is the class label $y$.
We convert the data frame into a tidyverse tibble
(optional).

```{r }
data(Zoo, package="mlbench")
head(Zoo)
```

*Note:* data.frames in R can have row names. The Zoo data set uses the
animal name as the row names. tibbles from `tidyverse` do not support
row names. To keep the animal name you can add a column with the animal
name.

```{r }
library(tidyverse)
as_tibble(Zoo, rownames = "animal")
```

You will have to remove the animal column before learning a model! In
the following I use the data.frame.

I translate all the TRUE/FALSE values into factors (nominal). This is
often needed for building models. Always check `summary()` to make sure
the data is ready for model learning.

```{r }
Zoo <- Zoo |>
  mutate(across(where(is.logical), 
         function (x) factor(x, levels = c(TRUE, FALSE)))) |>
  mutate(across(where(is.character), factor))

summary(Zoo)
```

## Decision Tree Classifiers

We use here the recursive partitioning implementation (rpart) which follows largely 
CART and uses the Gini index to make
splitting decisions and then it uses early stopping (also called pre-pruning).

```{r }
library(rpart)
```

### Create Tree 

We create first a tree with the default settings (see `? rpart.control`).

```{r }
tree_default <- Zoo |> 
  rpart(type ~ ., data = _)
tree_default
```

**Notes:** 

- `|>` supplies the data for `rpart`. Since `data` is not
  the first argument of `rpart`, the syntax `data = _` is used to specify
  where the data in `Zoo` goes. The call is equivalent to
  `tree_default <- rpart(type ~ ., data = Zoo)`. 
- The formula models the `type` variable by all other features represented by 
  a single period (`.`).
- The class variable needs to be a factor to be recognized as nominal 
  or rpart will create a regression tree instead of a decision tree. 
  Use `as.factor()` on the column with the class label first, if necessary.

We can plot the resulting decision tree.

```{r }
library(rpart.plot)
rpart.plot(tree_default, extra = 2)
```

*Note:* `extra=2` prints for each leaf node the number of correctly
classified objects from data and the total number of objects from the
training data falling into that node (correct/total).

### Make Predictions for New Data

I will make up my own animal: A lion with feathered wings.

```{r }
my_animal <- tibble(hair = TRUE, feathers = TRUE, eggs = FALSE,
  milk = TRUE, airborne = TRUE, aquatic = FALSE, predator = TRUE,
  toothed = TRUE, backbone = TRUE, breathes = TRUE, 
  venomous = FALSE, fins = FALSE, legs = 4, tail = TRUE, 
  domestic = FALSE, catsize = FALSE, type = NA)
```

The data types need to match the original data so we
change the columns to be factors like in the training set.

```{r }
my_animal <- my_animal |> 
  mutate(across(where(is.logical), 
                function(x) factor(x, levels = c(TRUE, FALSE))))
my_animal
```

Next, we make a prediction using the default tree

```{r }
predict(tree_default , my_animal, type = "class")
```

### Manual Calculation of the Resubstitution Error

We will calculate error of the model on the training data manually first,
so we see how it is calculated.

```{r }
predict(tree_default, Zoo) |> head ()

pred <- predict(tree_default, Zoo, type="class")
head(pred)
```

We can easily tabulate the true and predicted labels to create a 
confusion matrix.

```{r}
confusion_table <- with(Zoo, table(type, pred))
confusion_table
```

The counts in the diagonal are correct predictions. Off-diagonal 
counts represent errors (i.e., confusions).

We can summarize the confusion matrix using the accuracy measure.

```{r}
correct <- confusion_table |> diag() |> sum()
correct
error <- confusion_table |> sum() - correct
error

accuracy <- correct / (correct + error)
accuracy
```

Here is the accuracy calculation as a simple function.

```{r }
accuracy <- function(truth, prediction) {
    tbl <- table(truth, prediction)
    sum(diag(tbl))/sum(tbl)
}

accuracy(Zoo |> pull(type), pred)
```

### Confusion Matrix for the Resubstitution Error using Caret 

The caret package provides a convenient way to create and 
analyze a confusion table including many useful statistics.

```{r }
library(caret)
confusionMatrix(data = pred, 
                reference = Zoo |> pull(type))
```
**Important note:** Calculating accuracy on the training data is not a good idea. 
A complete example with code for holding out a test set and performing hyperparameter
selection using cross-validation can be found in section [Hyperparameter Tuning].

## Model Overfitting

We are tempted to create the largest possible tree
to get the most accurate model. This can be achieved by 
changing the algorithms hyperparameter (parameters that 
change how the algorithm works). We
set the complexity parameter `cp` to 0 (split
even if it does not improve the fit) and we set the minimum number of
observations in a node needed to split to the smallest value of 2 (see:
`?rpart.control`). *Note:* This is not a good idea! 
As we will see later, full trees overfit the training data!

```{r }
tree_full <- Zoo |> 
  rpart(type ~ . , data = _, 
        control = rpart.control(minsplit = 2, cp = 0))
rpart.plot(tree_full, extra = 2, 
           roundint=FALSE,
           box.palette = list("Gy", "Gn", "Bu", "Bn", 
                              "Or", "Rd", "Pu")) 
tree_full
```

Error on the training set of the full tree

```{r }
pred_full <- predict(tree_full, Zoo, type = "class")

accuracy(Zoo |> pull(type), pred_full)
```

We see that the error is smaller then for the pruned tree. This,
however, does not mean that the model is better. It actually is overfitting 
the training data (it just memorizes it) and it likely has worse generalization 
performance on new data. This effect
is called overfitting the training data and needs to be avoided.

## Model Selection

We often can create many different models for a classification problem.
Above, we have created a decision tree using the default settings and also
a full tree. The question is: Which one should we use. This problem is called 
model selection.

In order to select the model we need to split the training data into a 
**validation set** and the training set that is actually used to train model. 
The error rate on the validation set can then be used to choose 
between several models.

Caret has model selection build into the `train()` function. We will select
between the default complexity `cp = 0.01` and a full tree `cp = 0` (set via `tuneGrid`).
`trControl` specified how the validation set is obtained. We use 
Leave Group Out Cross-Validation (`LGOCV`) which
picks randomly the proportion `p` of data to train and uses the rest as 
the validation set. To get a better estimate of the 
error, this process is repeated `number` of times and the errors are averaged.


```{r }
fit <- Zoo |>
  train(type ~ .,
    data = _ ,
    method = "rpart",
    control = rpart.control(minsplit = 2), # we have little data
    tuneGrid = data.frame(cp = c(0.01, 0)),
    trControl = trainControl(method = "LGOCV", 
                             p = 0.8, 
                             number = 10),
    tuneLength = 5)

fit
```

We see that in this case, the full tree model performs slightly better.
However, given the small dataset, this may not be a significant difference 
and we will look at a statistical
test for this later.

## Model Evaluation

Models should be evaluated on a test set that has no overlap with the 
training set. We typically split the data using random sampling.
To get reproducible results, 
we set random number generator seed.

```{r }
set.seed(2000)
```

### Holdout Method

Test data is not used in the model building process and set aside purely
for testing the model. Here, we partition data the 80% training and 20%
testing.

```{r }
inTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]
Zoo_train <- Zoo |> slice(inTrain)
Zoo_test <- Zoo |> slice(-inTrain)
```

Now we can train on the test set and get the generalization error on the 
test set.

### Cross-Validation Methods

There are several cross-validation methods that can use the available datsa more efficiently 
then the holdout method.
The most popular method is k-fold cross-validation which splits the data randomly into $k$ folds. It then
holds one fold back for testing and trains on the other $k-1$ folds. This is 
done with each fold and the resulting statistic (e.g., accuracy) is averaged.
This method uses the data more efficiently then the holdout method.

Cross validation can be directly used in `train()` using 
`trControl = trainControl(method = "cv", number = 10)`.
If no model selection is necessary then this will give the 
generalization error.

Cross-validation runs are independent and can be done faster in
parallel. To enable multi-core support, `caret` uses the package
`foreach` and you need to load a `do` backend. For Linux, you can use
`doMC` with 4 cores. Windows needs different backend like `doParallel`
(see `caret` cheat sheet above).

```{r }
## Linux backend
# library(doMC)
# registerDoMC(cores = 4)
# getDoParWorkers()

## Windows backend
# library(doParallel)
# cl <- makeCluster(4, type="SOCK")
# registerDoParallel(cl)
```

## Hyperparameter Tuning

**Note:** This section contains a complete code example of how data should be used.
It first holds out a test set and then performing hyperparameter selection 
using cross-validation.

Hyperparameters are parameters that change how a
training algorithm works. An example is the complexity parameter 
`cp` for rpart decision trees. Tuning the hyperparameter means that 
we want to perform model selection to pick the best setting. 

We typically first use the holdout method to create a test set and then use 
cross validation using the training data for model selection. Let us use
80% for training and hold out 20% for testing.

```{r }
inTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]]
Zoo_train <- Zoo |> slice(inTrain)
Zoo_test <- Zoo |> slice(-inTrain)
```

The package `caret` combines training and validation for hyperparameter
tuning into the `train()` function. It internally splits the
data into training and validation sets and thus will provide you with
error estimates for different hyperparameter settings. `trainControl` is
used to choose how testing is performed.

For rpart, train tries to tune the `cp` parameter (tree complexity)
using accuracy to chose the best model. I set `minsplit` to 2 since we
have not much data. **Note:** Parameters used for tuning (in this case
`cp`) need to be set using a data.frame in the argument `tuneGrid`!
Setting it in control will be ignored.

```{r }
fit <- Zoo_train |>
  train(type ~ .,
    data = _ ,
    method = "rpart",
    control = rpart.control(minsplit = 2), # we have little data
    trControl = trainControl(method = "cv", number = 10),
    tuneLength = 5)

fit
```

**Note:** Train has built 10 trees using the training folds for each
value of `cp` and the reported values for accuracy and Kappa are the
averages on the validation folds.

A model using the best tuning parameters and using all the data supplied
to `train()` is available as `fit$finalModel`.

```{r }
library(rpart.plot)
rpart.plot(fit$finalModel, extra = 2,
  box.palette = list("Gy", "Gn", "Bu", "Bn", "Or", "Rd", "Pu"))
```

caret also computes variable importance. By default it uses competing
splits (splits which would be runners up, but do not get chosen by the
tree) for rpart models (see `? varImp`). Toothed is the runner up for
many splits, but it never gets chosen!

```{r }
varImp(fit)
```

Here is the variable importance without competing splits.

```{r }
imp <- varImp(fit, compete = FALSE)
imp

ggplot(imp)
```

**Note:** Not all models provide a variable importance function. In this
case caret might calculate the variable importance by itself and ignore
the model (see `? varImp`)!


Now, we can estimate the generalization error of the best model on the 
held out test data.

```{r }
pred <- predict(fit, newdata = Zoo_test)
pred
```

Caret's `confusionMatrix()` function calculates accuracy, confidence
intervals, kappa and many more evaluation metrics. You need to use
separate test data to create a confusion matrix based on the
generalization error.

```{r }
confusionMatrix(data = pred, 
                ref = Zoo_test |> pull(type))
```

**Some notes**

-   Many classification algorithms and `train` in caret do not deal well
    with missing values. If your classification model can deal with
    missing values (e.g., `rpart`) then use `na.action = na.pass` when
    you call `train` and `predict`. Otherwise, you need to remove
    observations with missing values with `na.omit` or use imputation to
    replace the missing values before you train the model. Make sure
    that you still have enough observations left.
-   Make sure that nominal variables (this includes logical variables)
    are coded as factors.
-   The class variable for train in caret cannot have level names that
    are keywords in R (e.g., `TRUE` and `FALSE`). Rename them to, for
    example, "yes" and "no."
-   Make sure that nominal variables (factors) have examples for all
    possible values. Some methods might have problems with variable
    values without examples. You can drop empty levels using
    `droplevels` or `factor`.
-   Sampling in train might create a sample that does not contain
    examples for all values in a nominal (factor) variable. You will get
    an error message. This most likely happens for variables which have
    one very rare value. You may have to remove the variable.

## Pitfalls of Model Selection and Evaluation

* Do not measure the error on the training set or
  use the validation error as a generalization error estimate. 
  Always use the generalization error on a test set!
* The training data and the test sets cannot overlap or we will not 
  evaluate the generalization performance. The training set can be come 
  contaminated by things like preprocessing the all the data together.

## Model Comparison

We will compare three models, a majority class baseline classifier, a 
decision trees with a k-nearest neighbors (kNN)
classifier. We will create fixed sampling scheme (10-fold cross-validation) so we
compare the different models using exactly the same folds. It is
specified as `trControl` during training.

```{r }
train_index <- createFolds(Zoo_train$type, 
                           k = 10, 
                           returnTrain = TRUE)
```

### Build models

Caret does not provide a baseline classifier, but lets us specify our own models. 
We define the classifier as a list with the  
structure needed by Caret's `train()` function (see [use your own model](https://topepo.github.io/caret/using-your-own-model-in-train.html#model-components)  in caret's description). The `fit` component calculates
the majority class label and the `predict` component just labels every row
in `newdata` as the majority class.

```{r }
majority_class_baseline_classifier <- list(label = "Majority Class Baseline Classifier",
                  library = character(0),
                  type = "Classification",
                  parameters = data.frame(parameter = "parameter",
                                          class = "character",
                                          label = "parameter"),
                  grid = function(x, y, len = NULL, search = "grid") 
                    data.frame(parameter = "none"),
                  loop = NULL,
                  fit = function(x, y, wts, param, lev, last, classProbs, ...) {          
                    tbl <- table(y)
                    majority <- names(tbl)[which.max(tbl)]
                    list(majority = majority)
                    },
                  predict = function(modelFit, newdata, submodels = NULL) {
                    rep(modelFit$majority, nrow(newdata))
                  },
                  prob = NULL
                  )
```  
  
We train the model and perform cross-validation.
```{r}  
baseline <-  Zoo_train |> 
  train(type ~ .,
        data = _,
        method = majority_class_baseline_classifier,
	      tuneLength = 1,
	      trControl = trainControl(method = "cv", 
	                               index = train_index)
  )
```


The second model is a default decision tree.

```{r }
rpartFit <- Zoo_train |> 
  train(type ~ .,
        data = _,
        method = "rpart",
        tuneLength = 10,
        trControl = trainControl(method = "cv",
                                 index = train_index)
  )
```

The third model is a kNN classifier, this classifier will be discussed 
in the next Chapter. kNN uses the Euclidean distance between objects.
Logicals will be used as 0-1 variables. To make sure the range of all 
variables is compatible, we 
ask `train` to scale the data using
`preProcess = "scale"`. 

```{r }
knnFit <- Zoo_train |> 
  train(type ~ .,
        data = _,
        method = "knn",
        preProcess = "scale",
	      tuneLength = 10,
	      trControl = trainControl(method = "cv", 
	                               index = train_index)
  )
```


Compare accuracy and kappa over all folds.

```{r }
resamps <- resamples(list(
	baseline = baseline,	
  CART = rpartFit,
		kNearestNeighbors = knnFit
		))

summary(resamps)
```

`caret` provides some visualizations. For
example, a boxplot to compare the accuracy and kappa distribution (over
the 10 folds).

```{r }
bwplot(resamps, layout = c(3, 1))
```

We see that the baseline has no predictive power and produces consistently a kappa of 0. KNN performs consistently the best.
To find out if one models is statistically better than the other, we can use a statistical test.

```{r }
difs <- diff(resamps)
difs

summary(difs)
```

p-values gives us the probability of seeing an even more extreme value
(difference between accuracy or kappa) given that the null hypothesis (difference
= 0) is true. For a better classifier, the p-value should be "significant," i.e., less than
.05 or 0.01. `diff` automatically applies Bonferroni correction for
multiple comparisons. In this case, kNN significantly better than the other classifiers.

## Feature Selection*

Decision trees implicitly select features for splitting, but we can also
select features before we apply any learning algorithm. 
Since different features lead to different models, choosing 
the best set of features is also a type of
model selection.


Many feature selection methods are implemented in the FSelector package.
```{r }
library(FSelector)
```

### Univariate Feature Importance Score

These scores measure how related each feature is to the class variable.
For discrete features (as in our case), the chi-square statistic can be
used to derive a score.

```{r }
weights <- Zoo_train |> 
  chi.squared(type ~ ., data = _) |>
  as_tibble(rownames = "feature") |>
  arrange(desc(attr_importance))

weights
```

We can plot the importance in descending order (using `reorder` to order factor
levels used by `ggplot`).

```{r }
ggplot(weights,
  aes(x = attr_importance, 
      y = reorder(feature, attr_importance))) +
  geom_bar(stat = "identity") +
  xlab("Importance score") + 
  ylab("Feature")
```

Picking the best features is called the feature ranking approach.
Here we pick the 5 highest-ranked features.

```{r }
subset <- cutoff.k(weights |> 
                   column_to_rownames("feature"), 
                   5)
subset
```

Use only the selected features to build a model (`Fselector` provides
`as.simple.formula`).

```{r }
f <- as.simple.formula(subset, "type")
f

m <- Zoo_train |> rpart(f, data = _)
rpart.plot(m, extra = 2, roundint = FALSE)
```

There are many alternative ways to calculate univariate importance
scores (see package FSelector). Some of them (also) work for continuous
features. One example is the information gain ratio based on entropy as
used in decision tree induction.

```{r }
Zoo_train |> 
  gain.ratio(type ~ ., data = _) |>
  as_tibble(rownames = "feature") |>
  arrange(desc(attr_importance))
```

### Feature Subset Selection

Often, features are related and calculating importance for each feature
independently is not optimal. We can use greedy search heuristics. For
example `cfs` uses correlation/entropy with best first search.

```{r }
Zoo_train |> 
  cfs(type ~ ., data = _)
```

The disadvantage of this method is that the model we want to train may not use
correlation/entropy. We can use the actual model using
as a black-box defined in an evaluator function
to calculate a score to be maximized. 
This is typically the best method, since it can use the model
for selection.
First, we define an evaluation
function that builds a model given a subset of features and calculates a
quality score. We use here the average for 5 bootstrap samples
(`method = "cv"` can also be used instead), no tuning (to be faster),
and the average accuracy as the score.

```{r }
evaluator <- function(subset) {
  model <- Zoo_train |> 
    train(as.simple.formula(subset, "type"),
          data = _,
          method = "rpart",
          trControl = trainControl(method = "boot", number = 5),
          tuneLength = 0)
  
  results <- model$resample$Accuracy
  
  cat("Trying features:", paste(subset, collapse = " + "), "\n")
  
  m <- mean(results)
  cat("Accuracy:", round(m, 2), "\n\n")
  m
}
```

Start with all features (but not the class variable `type`)

```{r }
features <- Zoo_train |> 
  colnames() |> 
  setdiff("type")
```

There are several (greedy) search strategies available. These run for a
while so they commented out below. Remove the comment for one at a time
to try these types of feature selection.

```{r }
#subset <- backward.search(features, evaluator)
#subset <- forward.search(features, evaluator)
#subset <- best.first.search(features, evaluator)
#subset <- hill.climbing.search(features, evaluator)
#subset
```

### Using Dummy Variables for Factors

Nominal features (factors) are often encoded as a series of 0-1 dummy
variables. For example, let us try to predict if an animal is a predator
given the type. First we use the original encoding of type as a factor
with several values.

```{r }
tree_predator <- Zoo_train |> 
  rpart(predator ~ type, data = _)
rpart.plot(tree_predator, extra = 2, roundint = FALSE)
```

**Note:** Some splits use multiple values. Building the tree will become
extremely slow if a factor has many levels (different values) since the
tree has to check all possible splits into two subsets. This situation
should be avoided.

Convert type into a set of 0-1 dummy variables using `class2ind`. See
also `? dummyVars` in package `caret`.

```{r }
Zoo_train_dummy <- as_tibble(class2ind(Zoo_train$type)) |> 
  mutate(across(everything(), as.factor)) |>
  add_column(predator = Zoo_train$predator)
Zoo_train_dummy

tree_predator <- Zoo_train_dummy |> 
  rpart(predator ~ ., 
        data = _,
        control = rpart.control(minsplit = 2, cp = 0.01))
rpart.plot(tree_predator, roundint = FALSE)
```

Using `caret` on the original factor encoding automatically translates
factors (here type) into 0-1 dummy variables (e.g., `typeinsect = 0`).
The reason is that some models cannot directly use factors and `caret`
tries to consistently work with all of them.

```{r }
fit <- Zoo_train |> 
  train(predator ~ type, 
        data = _, 
        method = "rpart",
        control = rpart.control(minsplit = 2),
        tuneGrid = data.frame(cp = 0.01))
fit

rpart.plot(fit$finalModel, extra = 2)
```

*Note:* To use a fixed value for the tuning parameter `cp`, we have to
create a tuning grid that only contains that value.

## Exercises*

We will use again the Palmer penguin data for the exercises.

```{r }
library(palmerpenguins)
head(penguins)
```

Create a R markdown file with the code and discussion for the following below.
Remember, the complete approach is described in section [Hyperparameter Tuning]. 

1. Split the data into a training and test set.
2. Create an rpart decision tree to predict the species. You will have to deal with 
missing values. 
3. Experiment with setting `minsplit` for rpart and make sure `tuneLength` is 
at least 5.
Discuss the model selection process (hyperparameter tuning) and what final 
model was chosen.
4. Visualize the tree and discuss what the splits mean.
5. Calculate the variable importance from the fitted model. What variables are
the most important? What variables do not matter?
6. Use the test set to evaluate the generalization error and accuracy.